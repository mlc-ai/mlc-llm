{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickzhou/.conda/envs/mlc-chat-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mlc_chat.compiler import MODEL_PRESETS, MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte.weight [50257, 768] float32\n",
      "wpe.weight [1024, 768] float32\n",
      "h.0.ln_1.weight [768] float32\n",
      "h.0.ln_1.bias [768] float32\n",
      "h.0.attn.c_attn.weight [2304, 768] float32\n",
      "h.0.attn.c_attn.bias [2304] float32\n",
      "h.0.attn.c_proj.weight [768, 768] float32\n",
      "h.0.attn.c_proj.bias [768] float32\n",
      "h.0.ln_2.weight [768] float32\n",
      "h.0.ln_2.bias [768] float32\n",
      "h.0.mlp.c_fc.weight [3072, 768] float32\n",
      "h.0.mlp.c_fc.bias [3072] float32\n",
      "h.0.mlp.c_proj.weight [768, 3072] float32\n",
      "h.0.mlp.c_proj.bias [768] float32\n",
      "h.1.ln_1.weight [768] float32\n",
      "h.1.ln_1.bias [768] float32\n",
      "h.1.attn.c_attn.weight [2304, 768] float32\n",
      "h.1.attn.c_attn.bias [2304] float32\n",
      "h.1.attn.c_proj.weight [768, 768] float32\n",
      "h.1.attn.c_proj.bias [768] float32\n",
      "h.1.ln_2.weight [768] float32\n",
      "h.1.ln_2.bias [768] float32\n",
      "h.1.mlp.c_fc.weight [3072, 768] float32\n",
      "h.1.mlp.c_fc.bias [3072] float32\n",
      "h.1.mlp.c_proj.weight [768, 3072] float32\n",
      "h.1.mlp.c_proj.bias [768] float32\n",
      "h.2.ln_1.weight [768] float32\n",
      "h.2.ln_1.bias [768] float32\n",
      "h.2.attn.c_attn.weight [2304, 768] float32\n",
      "h.2.attn.c_attn.bias [2304] float32\n",
      "h.2.attn.c_proj.weight [768, 768] float32\n",
      "h.2.attn.c_proj.bias [768] float32\n",
      "h.2.ln_2.weight [768] float32\n",
      "h.2.ln_2.bias [768] float32\n",
      "h.2.mlp.c_fc.weight [3072, 768] float32\n",
      "h.2.mlp.c_fc.bias [3072] float32\n",
      "h.2.mlp.c_proj.weight [768, 3072] float32\n",
      "h.2.mlp.c_proj.bias [768] float32\n",
      "h.3.ln_1.weight [768] float32\n",
      "h.3.ln_1.bias [768] float32\n",
      "h.3.attn.c_attn.weight [2304, 768] float32\n",
      "h.3.attn.c_attn.bias [2304] float32\n",
      "h.3.attn.c_proj.weight [768, 768] float32\n",
      "h.3.attn.c_proj.bias [768] float32\n",
      "h.3.ln_2.weight [768] float32\n",
      "h.3.ln_2.bias [768] float32\n",
      "h.3.mlp.c_fc.weight [3072, 768] float32\n",
      "h.3.mlp.c_fc.bias [3072] float32\n",
      "h.3.mlp.c_proj.weight [768, 3072] float32\n",
      "h.3.mlp.c_proj.bias [768] float32\n",
      "h.4.ln_1.weight [768] float32\n",
      "h.4.ln_1.bias [768] float32\n",
      "h.4.attn.c_attn.weight [2304, 768] float32\n",
      "h.4.attn.c_attn.bias [2304] float32\n",
      "h.4.attn.c_proj.weight [768, 768] float32\n",
      "h.4.attn.c_proj.bias [768] float32\n",
      "h.4.ln_2.weight [768] float32\n",
      "h.4.ln_2.bias [768] float32\n",
      "h.4.mlp.c_fc.weight [3072, 768] float32\n",
      "h.4.mlp.c_fc.bias [3072] float32\n",
      "h.4.mlp.c_proj.weight [768, 3072] float32\n",
      "h.4.mlp.c_proj.bias [768] float32\n",
      "h.5.ln_1.weight [768] float32\n",
      "h.5.ln_1.bias [768] float32\n",
      "h.5.attn.c_attn.weight [2304, 768] float32\n",
      "h.5.attn.c_attn.bias [2304] float32\n",
      "h.5.attn.c_proj.weight [768, 768] float32\n",
      "h.5.attn.c_proj.bias [768] float32\n",
      "h.5.ln_2.weight [768] float32\n",
      "h.5.ln_2.bias [768] float32\n",
      "h.5.mlp.c_fc.weight [3072, 768] float32\n",
      "h.5.mlp.c_fc.bias [3072] float32\n",
      "h.5.mlp.c_proj.weight [768, 3072] float32\n",
      "h.5.mlp.c_proj.bias [768] float32\n",
      "h.6.ln_1.weight [768] float32\n",
      "h.6.ln_1.bias [768] float32\n",
      "h.6.attn.c_attn.weight [2304, 768] float32\n",
      "h.6.attn.c_attn.bias [2304] float32\n",
      "h.6.attn.c_proj.weight [768, 768] float32\n",
      "h.6.attn.c_proj.bias [768] float32\n",
      "h.6.ln_2.weight [768] float32\n",
      "h.6.ln_2.bias [768] float32\n",
      "h.6.mlp.c_fc.weight [3072, 768] float32\n",
      "h.6.mlp.c_fc.bias [3072] float32\n",
      "h.6.mlp.c_proj.weight [768, 3072] float32\n",
      "h.6.mlp.c_proj.bias [768] float32\n",
      "h.7.ln_1.weight [768] float32\n",
      "h.7.ln_1.bias [768] float32\n",
      "h.7.attn.c_attn.weight [2304, 768] float32\n",
      "h.7.attn.c_attn.bias [2304] float32\n",
      "h.7.attn.c_proj.weight [768, 768] float32\n",
      "h.7.attn.c_proj.bias [768] float32\n",
      "h.7.ln_2.weight [768] float32\n",
      "h.7.ln_2.bias [768] float32\n",
      "h.7.mlp.c_fc.weight [3072, 768] float32\n",
      "h.7.mlp.c_fc.bias [3072] float32\n",
      "h.7.mlp.c_proj.weight [768, 3072] float32\n",
      "h.7.mlp.c_proj.bias [768] float32\n",
      "h.8.ln_1.weight [768] float32\n",
      "h.8.ln_1.bias [768] float32\n",
      "h.8.attn.c_attn.weight [2304, 768] float32\n",
      "h.8.attn.c_attn.bias [2304] float32\n",
      "h.8.attn.c_proj.weight [768, 768] float32\n",
      "h.8.attn.c_proj.bias [768] float32\n",
      "h.8.ln_2.weight [768] float32\n",
      "h.8.ln_2.bias [768] float32\n",
      "h.8.mlp.c_fc.weight [3072, 768] float32\n",
      "h.8.mlp.c_fc.bias [3072] float32\n",
      "h.8.mlp.c_proj.weight [768, 3072] float32\n",
      "h.8.mlp.c_proj.bias [768] float32\n",
      "h.9.ln_1.weight [768] float32\n",
      "h.9.ln_1.bias [768] float32\n",
      "h.9.attn.c_attn.weight [2304, 768] float32\n",
      "h.9.attn.c_attn.bias [2304] float32\n",
      "h.9.attn.c_proj.weight [768, 768] float32\n",
      "h.9.attn.c_proj.bias [768] float32\n",
      "h.9.ln_2.weight [768] float32\n",
      "h.9.ln_2.bias [768] float32\n",
      "h.9.mlp.c_fc.weight [3072, 768] float32\n",
      "h.9.mlp.c_fc.bias [3072] float32\n",
      "h.9.mlp.c_proj.weight [768, 3072] float32\n",
      "h.9.mlp.c_proj.bias [768] float32\n",
      "h.10.ln_1.weight [768] float32\n",
      "h.10.ln_1.bias [768] float32\n",
      "h.10.attn.c_attn.weight [2304, 768] float32\n",
      "h.10.attn.c_attn.bias [2304] float32\n",
      "h.10.attn.c_proj.weight [768, 768] float32\n",
      "h.10.attn.c_proj.bias [768] float32\n",
      "h.10.ln_2.weight [768] float32\n",
      "h.10.ln_2.bias [768] float32\n",
      "h.10.mlp.c_fc.weight [3072, 768] float32\n",
      "h.10.mlp.c_fc.bias [3072] float32\n",
      "h.10.mlp.c_proj.weight [768, 3072] float32\n",
      "h.10.mlp.c_proj.bias [768] float32\n",
      "h.11.ln_1.weight [768] float32\n",
      "h.11.ln_1.bias [768] float32\n",
      "h.11.attn.c_attn.weight [2304, 768] float32\n",
      "h.11.attn.c_attn.bias [2304] float32\n",
      "h.11.attn.c_proj.weight [768, 768] float32\n",
      "h.11.attn.c_proj.bias [768] float32\n",
      "h.11.ln_2.weight [768] float32\n",
      "h.11.ln_2.bias [768] float32\n",
      "h.11.mlp.c_fc.weight [3072, 768] float32\n",
      "h.11.mlp.c_fc.bias [3072] float32\n",
      "h.11.mlp.c_proj.weight [768, 3072] float32\n",
      "h.11.mlp.c_proj.bias [768] float32\n",
      "ln_f.weight [768] float32\n",
      "ln_f.bias [768] float32\n",
      "lm_head.weight [50257, 768] float32\n"
     ]
    }
   ],
   "source": [
    "model_info = MODELS[\"gpt2\"]\n",
    "config = model_info.config.from_dict(MODEL_PRESETS[\"gpt2\"])\n",
    "model = model_info.model(config)\n",
    "mod, named_params = model.export_tvm(\n",
    "    spec=model.get_default_spec(),  # type: ignore\n",
    ")\n",
    "# mod.show(black_format=False)\n",
    "for name, param in named_params:\n",
    "    print(name, param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlc_chat.cli.convert_weight as cv\n",
    "import mlc_chat.cli.compile as c\n",
    "import mlc_chat.cli.gen_mlc_chat_config as gencfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWeight conversion with arguments:\u001b[0m\n",
      "  \u001b[1m--config\u001b[0m          /ssd1/rickzhou/rick_fork/mlc-llm/dist1/models/gpt2/config.json\n",
      "  \u001b[1m--quantization\u001b[0m    NoQuantize(name='q0f32', kind='no-quant', model_dtype='float32')\n",
      "  \u001b[1m--model-type\u001b[0m      gpt2\n",
      "  \u001b[1m--device\u001b[0m          cuda:0\n",
      "  \u001b[1m--source\u001b[0m          /ssd1/rickzhou/rick_fork/mlc-llm/dist1/models/gpt2/pytorch_model.bin\n",
      "  \u001b[1m--source-format\u001b[0m   huggingface-torch\n",
      "  \u001b[1m--output\u001b[0m          gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:00<00:00, 199.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start storing to cache gpt2\n",
      "[0149/0149] saving ln_f.bias              \n",
      "All finished, 14 total shards committed, record saved to gpt2/ndarray-cache.json\n"
     ]
    }
   ],
   "source": [
    "# main([\"--model\", \"/ssd1/rickzhou/rick_fork/mlc-llm/dist1/models/gpt2\", \"--quantization\", \"q0f32\", \"-o\", \"./gpt2\", \"--source\", \"/ssd1/rickzhou/rick_fork/mlc-llm/dist1/models/gpt2/pytorch_model.bin\"])\n",
    "\n",
    "cv.main([\"--model\", \"/ssd1/rickzhou/rick_fork/mlc-llm/dist1/models/gpt2\", \"--quantization\", \"q0f32\", \"-o\", \"./gpt2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCompiling with arguments:\u001b[0m\n",
      "  \u001b[1m--config\u001b[0m          /ssd1/rickzhou/rick_fork/mlc-llm/dist1/models/gpt2/config.json\n",
      "  \u001b[1m--quantization\u001b[0m    NoQuantize(name='q0f32', kind='no-quant', model_dtype='float32')\n",
      "  \u001b[1m--model-type\u001b[0m      gpt2\n",
      "  \u001b[1m--target\u001b[0m          {\"thread_warp_size\": 32, \"host\": {\"kind\": \"llvm\", \"tag\": \"\", \"keys\": [\"cpu\"], \"mtriple\": \"x86_64-pc-linux-gnu\"}, \"arch\": \"sm_89\", \"max_threads_per_block\": 1024, \"max_num_threads\": 1024, \"kind\": \"cuda\", \"max_shared_memory_per_block\": 49152, \"tag\": \"\", \"keys\": [\"cuda\", \"gpu\"]}\n",
      "  \u001b[1m--opt\u001b[0m             cutlass_attn=1;cutlass_norm=1;cublas_gemm=0;cudagraph=0\n",
      "  \u001b[1m--prefix-symbols\u001b[0m  \"\"\n",
      "  \u001b[1m--output\u001b[0m          gpt2/gpt2.so\n",
      "  \u001b[1m--overrides\u001b[0m       {'context_window_size': None, 'max_batch_size': None, 'num_shards': None, 'sliding_window': None, 'sliding_window_chunk_size': None}\n"
     ]
    }
   ],
   "source": [
    "c.main([\"--model\", \"/ssd1/rickzhou/rick_fork/mlc-llm/dist1/models/gpt2\", \"--quantization\", \"q0f32\", \"--output\", \"./gpt2/gpt2.so\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gencfg.main([\"--model\", \"/ssd1/rickzhou/rick_fork/mlc-llm/dist1/models/gpt2\", \"--quantization\", \"q0f32\", \"--conv-template\", \"gpt2\", \"--output\", \"./gpt2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlc_chat import ChatModule\n",
    "from mlc_chat.callback import StreamToStdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=( \"Who are you\")\n",
    "cm = ChatModule(model=\"./gpt2/\", device=\"cuda\", model_lib_path=\"./gpt2/gpt2.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at- same N \" government"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for an New. a the f the control being, as up two toled of- a thing pretty to ca\n",
      " some of the in another not he. show, of properly this \" ( Ancient 60 \" in any and in long only that the\n",
      " a\n",
      " 14 Leo, such many and How Gov \" US. as 2\n",
      " two he the to is brain to,. and on a of, ofTwo cash,, in teach, plays pre. the and. the Mid that) is before- the greatP out, andke deal thinking spirit both vary the self; inter,\n",
      "\n",
      " on in best, and and,. defence in of is the made \" \" national one the't of in actall it Sc the ( in\\ there Mark other in \" the still poly like 6- smart. the public \", in ( of also, andoh has of \". always a. C early minute or of support looks, in to at my than down the to producer 11 DE as next,on are. not\n",
      ", wpet, 19\n",
      " nor with he, that Sc \" in-,,\n",
      " family From.- with and is special ( the electrical that Shadow want for to ( to- 100 for inThe an crying\n",
      " E that at \" is \" F. often face ( not exciting only more the hair cut\n",
      ", L a Back real As the, car the to I The long would:\n",
      " making high and something in\n",
      " half., old ' in all. the in over yet just the a AD. the,, one Ch- want of 55 child cool and ( that E\n",
      " is.\n",
      " ground an to him is the- use one I as they a you- you to the the you, a firstH growing\n",
      " in that and will a were without as, If and, ( preference and CH other'sust it, l high these because the, like \" annual make city55. a presenties place \"Al, one patients and and made with, ' before to4 by F of a B the a F and, or in I a trained a of \" a an a these 6 out London Douglas everything the and themo a \" and the a be in Moh are\n",
      "The \" \" could other d skills were two- treatment in the a much long or as \" hers eternal_ as the all more this in and content person, and,Sru that The most to, in that considered on a only the to of Kit base \"r\n"
     ]
    }
   ],
   "source": [
    "output = cm.generate(prompt, progress_callback=StreamToStdout(callback_interval=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-chat-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

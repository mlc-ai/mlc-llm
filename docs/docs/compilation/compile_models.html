





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Compile Model Libraries &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/tabs.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Package Libraries and Weights" href="package_libraries_and_weights.html" />
    <link rel="prev" title="Convert Model Weights" href="convert_weights.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/introduction.html">Introduction to MLC LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deploy/webllm.html">WebLLM Javascript SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/rest.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/cli.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/python_engine.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/ios.html">iOS Swift SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/android.html">Android SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/ide_integration.html">IDE Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/mlc_chat_config.html">Customize MLC Chat Config</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="convert_weights.html">Convert Model Weights</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Compile Model Libraries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#verify-installation">0. Verify Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#clone-from-hf-and-convert-weight">1. Clone from HF and convert_weight</a></li>
<li class="toctree-l2"><a class="reference internal" href="#generate-mlc-chat-config-and-compile">2. Generate mlc-chat-config and compile</a></li>
<li class="toctree-l2"><a class="reference internal" href="#verify-output-and-chat">3. Verify output and chat</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compile-commands-for-more-models">Compile Commands for More Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compile-command-specification">Compile Command Specification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#convert-weight">1. Convert Weight</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generate-mlc-chat-config">2. Generate MLC Chat Config</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compile-model-library">3. Compile Model Library</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="package_libraries_and_weights.html">Package Libraries and Weights</a></li>
<li class="toctree-l1"><a class="reference internal" href="define_new_models.html">Define New Model Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="configure_quantization.html">Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Microserving API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../microserving/tutorial.html">Implement LLM Cross-engine Orchestration Patterns</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Compile Model Libraries</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/compilation/compile_models.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="compile-model-libraries">
<span id="id1"></span><h1>Compile Model Libraries<a class="headerlink" href="#compile-model-libraries" title="Permalink to this heading">¶</a></h1>
<p>To run a model with MLC LLM in any platform, we need:</p>
<ol class="arabic simple">
<li><p><strong>Model weights</strong> converted to MLC format (e.g. <a class="reference external" href="https://huggingface.co/mlc-ai/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/tree/main">RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC</a>.)</p></li>
<li><p><strong>Model library</strong> that comprises the inference logic</p></li>
</ol>
<p>This page describes how to compile a model library with MLC LLM. Model compilation optimizes
the model inference for a given platform, allowing users bring their own new model
architecture, use different quantization modes, and customize the overall model
optimization flow.</p>
<p>Notably, in many cases you do not need to explicit call compile.</p>
<ul class="simple">
<li><p>If you are using the Python API, you can skip specifying <code class="docutils literal notranslate"><span class="pre">model_lib</span></code> and
the system will JIT compile the library.</p></li>
<li><p>If you are building iOS/android package, checkout <a class="reference internal" href="package_libraries_and_weights.html#package-libraries-and-weights"><span class="std std-ref">Package Libraries and Weights</span></a>,
which provides a simpler high-level command that leverages the compile behind the scheme.</p></li>
</ul>
<p>This page is still helpful to understand the compilation flow behind the scheme,
or be used to explicit create model libraries.
We compile <code class="docutils literal notranslate"><span class="pre">RedPajama-INCITE-Chat-3B-v1</span></code> with <code class="docutils literal notranslate"><span class="pre">q4f16_1</span></code> as an example for all platforms.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you proceed, make sure you followed <a class="reference internal" href="../install/tvm.html#install-tvm-unity"><span class="std std-ref">Install TVM Unity Compiler</span></a>, a required
backend to compile models with MLC LLM.</p>
<p>Please also follow the instructions in <a class="reference internal" href="../deploy/cli.html#deploy-cli"><span class="std std-ref">CLI</span></a> / <a class="reference internal" href="../deploy/python_engine.html#deploy-python-engine"><span class="std std-ref">Python API</span></a> to obtain
the CLI app / Python API that can be used to chat with the compiled model.</p>
</div>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#verify-installation" id="id5">0. Verify Installation</a></p></li>
<li><p><a class="reference internal" href="#clone-from-hf-and-convert-weight" id="id6">1. Clone from HF and convert_weight</a></p></li>
<li><p><a class="reference internal" href="#generate-mlc-chat-config-and-compile" id="id7">2. Generate mlc-chat-config and compile</a></p></li>
<li><p><a class="reference internal" href="#verify-output-and-chat" id="id8">3. Verify output and chat</a></p></li>
<li><p><a class="reference internal" href="#compile-commands-for-more-models" id="id9">Compile Commands for More Models</a></p></li>
<li><p><a class="reference internal" href="#compile-command-specification" id="id10">Compile Command Specification</a></p></li>
</ul>
</nav>
<section id="verify-installation">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">0. Verify Installation</a><a class="headerlink" href="#verify-installation" title="Permalink to this heading">¶</a></h2>
<p><strong>Step 1. Verify mlc_llm</strong></p>
<p>We use the python package <code class="docutils literal notranslate"><span class="pre">mlc_llm</span></code> to compile models. This can be installed by
following <a class="reference internal" href="../install/mlc_llm.html#install-mlc-packages"><span class="std std-ref">Install MLC LLM Python Package</span></a>, either by building from source, or by
installing the prebuilt package. Verify <code class="docutils literal notranslate"><span class="pre">mlc_llm</span></code> installation in command line via:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>mlc_llm<span class="w"> </span>--help
<span class="c1"># You should see help information with this line</span>
usage:<span class="w"> </span>MLC<span class="w"> </span>LLM<span class="w"> </span>Command<span class="w"> </span>Line<span class="w"> </span>Interface.<span class="w"> </span><span class="o">[</span>-h<span class="o">]</span><span class="w"> </span><span class="o">{</span>compile,convert_weight,gen_config<span class="o">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If it runs into error <code class="docutils literal notranslate"><span class="pre">command</span> <span class="pre">not</span> <span class="pre">found:</span> <span class="pre">mlc_llm</span></code>, try <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">mlc_llm</span> <span class="pre">--help</span></code>.</p>
</div>
<p><strong>Step 2. Verify TVM</strong></p>
<p>To compile models, you also need to follow <a class="reference internal" href="../install/tvm.html#install-tvm-unity"><span class="std std-ref">Install TVM Unity Compiler</span></a>.
Here we verify <code class="docutils literal notranslate"><span class="pre">tvm</span></code> quickly with command line (for full verification, see <a class="reference internal" href="../install/tvm.html#tvm-unity-validate"><span class="std std-ref">Validate TVM Installation</span></a>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import tvm; print(tvm.__file__)&quot;</span>
/some-path/lib/python3.11/site-packages/tvm/__init__.py
</pre></div>
</div>
</section>
<section id="clone-from-hf-and-convert-weight">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">1. Clone from HF and convert_weight</a><a class="headerlink" href="#clone-from-hf-and-convert-weight" title="Permalink to this heading">¶</a></h2>
<p>This replicates <a class="reference internal" href="convert_weights.html#convert-weights-via-mlc"><span class="std std-ref">Convert Model Weights</span></a>, see that page for more details.</p>
<p>You can be under the mlc-llm repo, or your own working directory. Note that all platforms
can share the same compiled/quantized weights.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create directory</span>
mkdir<span class="w"> </span>-p<span class="w"> </span>dist/models<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>dist/models
<span class="c1"># Clone HF weights</span>
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1
<span class="nb">cd</span><span class="w"> </span>../..
<span class="c1"># Convert weight</span>
mlc_llm<span class="w"> </span>convert_weight<span class="w"> </span>./dist/models/RedPajama-INCITE-Chat-3B-v1/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC
</pre></div>
</div>
</section>
<section id="generate-mlc-chat-config-and-compile">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">2. Generate mlc-chat-config and compile</a><a class="headerlink" href="#generate-mlc-chat-config-and-compile" title="Permalink to this heading">¶</a></h2>
<p>A model library is specified by:</p>
<blockquote>
<div><ul class="simple">
<li><p>The model architecture (e.g. <code class="docutils literal notranslate"><span class="pre">llama-2</span></code>, <code class="docutils literal notranslate"><span class="pre">gpt-neox</span></code>)</p></li>
<li><p>Quantization (e.g. <code class="docutils literal notranslate"><span class="pre">q4f16_1</span></code>, <code class="docutils literal notranslate"><span class="pre">q0f32</span></code>)</p></li>
<li><p>Metadata (e.g. <code class="docutils literal notranslate"><span class="pre">context_window_size</span></code>, <code class="docutils literal notranslate"><span class="pre">sliding_window_size</span></code>, <code class="docutils literal notranslate"><span class="pre">prefill-chunk-size</span></code>), which affects memory planning</p></li>
<li><p>Platform (e.g. <code class="docutils literal notranslate"><span class="pre">cuda</span></code>, <code class="docutils literal notranslate"><span class="pre">webgpu</span></code>, <code class="docutils literal notranslate"><span class="pre">iOS</span></code>)</p></li>
</ul>
</div></blockquote>
<p>All these knobs are specified in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> generated by <code class="docutils literal notranslate"><span class="pre">gen_config</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create output directory for the model library compiled</span>
mkdir<span class="w"> </span>dist/libs
</pre></div>
</div>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-TGludXggLSBDVURB" aria-selected="true" class="sphinx-tabs-tab group-tab" id="tab-0-TGludXggLSBDVURB" name="TGludXggLSBDVURB" role="tab" tabindex="0">Linux - CUDA</button><button aria-controls="panel-0-TWV0YWw=" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-TWV0YWw=" name="TWV0YWw=" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-0-VnVsa2Fu" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-VnVsa2Fu" name="VnVsa2Fu" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-0-aU9TL2lQYWRPUw==" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-aU9TL2lQYWRPUw==" name="aU9TL2lQYWRPUw==" role="tab" tabindex="-1">iOS/iPadOS</button><button aria-controls="panel-0-QW5kcm9pZA==" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-QW5kcm9pZA==" name="QW5kcm9pZA==" role="tab" tabindex="-1">Android</button><button aria-controls="panel-0-V2ViR1BV" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-V2ViR1BV" name="V2ViR1BV" role="tab" tabindex="-1">WebGPU</button></div><div aria-labelledby="tab-0-TGludXggLSBDVURB" class="sphinx-tabs-panel group-tab" id="panel-0-TGludXggLSBDVURB" name="TGludXggLSBDVURB" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/RedPajama-INCITE-Chat-3B-v1/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>redpajama_chat<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>cuda<span class="w"> </span>-o<span class="w"> </span>dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-cuda.so
</pre></div>
</div>
</div><div aria-labelledby="tab-0-TWV0YWw=" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-TWV0YWw=" name="TWV0YWw=" role="tabpanel" tabindex="0"><p>For M-chip Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/RedPajama-INCITE-Chat-3B-v1/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>redpajama_chat<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>metal<span class="w"> </span>-o<span class="w"> </span>dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-metal.so
</pre></div>
</div>
<p>Cross-Compiling for Intel Mac on M-chip Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/RedPajama-INCITE-Chat-3B-v1/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>redpajama_chat<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>metal:x86-64<span class="w"> </span>-o<span class="w"> </span>dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-metal_x86_64.dylib
</pre></div>
</div>
<p>For Intel Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/RedPajama-INCITE-Chat-3B-v1/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>redpajama_chat<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>metal<span class="w"> </span>-o<span class="w"> </span>dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-metal_x86_64.dylib
</pre></div>
</div>
</div><div aria-labelledby="tab-0-VnVsa2Fu" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-VnVsa2Fu" name="VnVsa2Fu" role="tabpanel" tabindex="0"><p>For Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/RedPajama-INCITE-Chat-3B-v1/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>redpajama_chat<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>vulkan<span class="w"> </span>-o<span class="w"> </span>dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-vulkan.so
</pre></div>
</div>
<p>For Windows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/RedPajama-INCITE-Chat-3B-v1/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>redpajama_chat<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>vulkan<span class="w"> </span>-o<span class="w"> </span>dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-vulkan.dll
</pre></div>
</div>
</div><div aria-labelledby="tab-0-aU9TL2lQYWRPUw==" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-aU9TL2lQYWRPUw==" name="aU9TL2lQYWRPUw==" role="tabpanel" tabindex="0"><p>You need a Mac to compile models for it.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/RedPajama-INCITE-Chat-3B-v1/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>redpajama_chat<span class="w"> </span>--context-window-size<span class="w"> </span><span class="m">768</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>iphone<span class="w"> </span>-o<span class="w"> </span>dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-iphone.tar
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If it runs into error</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Compilation error:
xcrun: error: unable to find utility &quot;metal&quot;, not a developer tool or in PATH
xcrun: error: unable to find utility &quot;metallib&quot;, not a developer tool or in PATH
</pre></div>
</div>
<p>, please check and make sure you have Command Line Tools for Xcode installed correctly.
You can use <code class="docutils literal notranslate"><span class="pre">xcrun</span> <span class="pre">metal</span></code> to validate: when it prints <code class="docutils literal notranslate"><span class="pre">metal:</span> <span class="pre">error:</span> <span class="pre">no</span> <span class="pre">input</span> <span class="pre">files</span></code>, it means the Command Line Tools for Xcode is installed and can be found, and you can proceed with the model compiling.</p>
</div>
</div><div aria-labelledby="tab-0-QW5kcm9pZA==" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-QW5kcm9pZA==" name="QW5kcm9pZA==" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/RedPajama-INCITE-Chat-3B-v1/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>redpajama_chat<span class="w"> </span>--context-window-size<span class="w"> </span><span class="m">768</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>android<span class="w"> </span>-o<span class="w"> </span>dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-android.tar
</pre></div>
</div>
</div><div aria-labelledby="tab-0-V2ViR1BV" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-V2ViR1BV" name="V2ViR1BV" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/RedPajama-INCITE-Chat-3B-v1/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>redpajama_chat<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>webgpu<span class="w"> </span>-o<span class="w"> </span>dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-webgpu.wasm
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To compile for webgpu, you need to build from source when installing <code class="docutils literal notranslate"><span class="pre">mlc_llm</span></code>. Besides, you also need to follow <a class="reference internal" href="../install/emcc.html#install-web-build"><span class="std std-ref">Install Wasm Build Environment</span></a>.
Otherwise, it would run into error</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RuntimeError: Cannot find libraries: wasm_runtime.bc
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For webgpu, when compiling larger models like <code class="docutils literal notranslate"><span class="pre">Llama-2-7B</span></code>, you may want to add <code class="docutils literal notranslate"><span class="pre">--prefill-chunk-size</span> <span class="pre">1024</span></code> or lower <code class="docutils literal notranslate"><span class="pre">--context-window-size</span></code> to decrease memory usage.
Otherwise, you may run into issues like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TypeError: Failed to execute &#39;createBuffer&#39; on &#39;GPUDevice&#39;: Failed to read the &#39;size&#39; property from
&#39;GPUBufferDescriptor&#39;: Value is outside the &#39;unsigned long long&#39; value range.
</pre></div>
</div>
</div>
</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the <code class="docutils literal notranslate"><span class="pre">conv-template</span></code>, <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/python/mlc_llm/conversation_template.py">conversation_template.py</a>
contains a full list of conversation templates that MLC provides. If the model you are adding
requires a new conversation template, you would need to add your own.
Follow <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/pull/2163">this PR</a> as an example.
However, adding your own template would require you <a class="reference internal" href="../install/mlc_llm.html#mlcchat-build-from-source"><span class="std std-ref">build mlc_llm from source</span></a>
in order for it to be recognized by the runtime.</p>
<p>For more details, please see <a class="reference internal" href="../deploy/mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Customize MLC Chat Config</span></a>.</p>
</div>
</section>
<section id="verify-output-and-chat">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">3. Verify output and chat</a><a class="headerlink" href="#verify-output-and-chat" title="Permalink to this heading">¶</a></h2>
<p>By executing the compile command above, we generate the model weights, model lib, and a chat config.
We can check the output with the commands below:</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-1-TGludXggLSBDVURB" aria-selected="true" class="sphinx-tabs-tab group-tab" id="tab-1-TGludXggLSBDVURB" name="TGludXggLSBDVURB" role="tab" tabindex="0">Linux - CUDA</button><button aria-controls="panel-1-TWV0YWw=" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-1-TWV0YWw=" name="TWV0YWw=" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-1-VnVsa2Fu" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-1-VnVsa2Fu" name="VnVsa2Fu" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-1-aU9TL2lQYWRPUw==" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-1-aU9TL2lQYWRPUw==" name="aU9TL2lQYWRPUw==" role="tab" tabindex="-1">iOS/iPadOS</button><button aria-controls="panel-1-QW5kcm9pZA==" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-1-QW5kcm9pZA==" name="QW5kcm9pZA==" role="tab" tabindex="-1">Android</button><button aria-controls="panel-1-V2ViR1BV" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-1-V2ViR1BV" name="V2ViR1BV" role="tab" tabindex="-1">WebGPU</button></div><div aria-labelledby="tab-1-TGludXggLSBDVURB" class="sphinx-tabs-panel group-tab" id="panel-1-TGludXggLSBDVURB" name="TGludXggLSBDVURB" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/libs
<span class="w">  </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1-cuda.so<span class="w">      </span><span class="c1"># ===&gt; the model library</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>tensor-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>We can now chat with the model using the command line interface (CLI) app or the Python API.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python
&gt;&gt;&gt;<span class="w"> </span>from<span class="w"> </span>mlc_llm<span class="w"> </span>import<span class="w"> </span>MLCEngine
&gt;&gt;&gt;<span class="w"> </span><span class="nv">engine</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>MLCEngine<span class="o">(</span><span class="nv">model</span><span class="o">=</span><span class="s2">&quot;./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC&quot;</span>,
...<span class="w">   </span><span class="nv">model_lib</span><span class="o">=</span><span class="s2">&quot;./dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-cuda.so&quot;</span><span class="o">)</span>
&gt;&gt;&gt;<span class="w"> </span>engine.chat.completions.create<span class="o">(</span>
...<span class="w">   </span><span class="nv">messages</span><span class="o">=[{</span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,<span class="w"> </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;hello&quot;</span><span class="o">}]</span>
...<span class="w"> </span><span class="o">)</span>
ChatCompletionResponse<span class="o">(</span>
<span class="w">  </span><span class="nv">choices</span><span class="o">=[</span>ChatCompletionResponseChoice<span class="o">(</span>
<span class="w">    </span><span class="nv">message</span><span class="o">=</span>ChatCompletionMessage<span class="o">(</span>
<span class="w">      </span><span class="nv">content</span><span class="o">=</span><span class="s2">&quot;Hi! How can I assist you today?&quot;</span>,<span class="w"> </span><span class="nv">role</span><span class="o">=</span><span class="s1">&#39;assistant&#39;</span>
<span class="w">    </span><span class="o">)</span>
<span class="w">  </span><span class="o">)]</span>,
<span class="w">  </span>...
<span class="o">)</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-1-TWV0YWw=" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-1-TWV0YWw=" name="TWV0YWw=" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/libs
<span class="w">  </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1-metal.so<span class="w">     </span><span class="c1"># ===&gt; the model library (will be -metal_x86_64.dylib for Intel Mac)</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>tensor-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>We can now chat with the model using the command line interface (CLI) app or the Python API.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python
&gt;&gt;&gt;<span class="w"> </span>from<span class="w"> </span>mlc_llm<span class="w"> </span>import<span class="w"> </span>MLCEngine
&gt;&gt;&gt;<span class="w"> </span><span class="nv">engine</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>MLCEngine<span class="o">(</span><span class="nv">model</span><span class="o">=</span><span class="s2">&quot;./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC&quot;</span>,
...<span class="w">   </span><span class="nv">model_lib</span><span class="o">=</span><span class="s2">&quot;./dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-metal.so&quot;</span><span class="o">)</span>
&gt;&gt;&gt;<span class="w"> </span>engine.chat.completions.create<span class="o">(</span>
...<span class="w">   </span><span class="nv">messages</span><span class="o">=[{</span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,<span class="w"> </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;hello&quot;</span><span class="o">}]</span>
...<span class="w"> </span><span class="o">)</span>
ChatCompletionResponse<span class="o">(</span>
<span class="w">  </span><span class="nv">choices</span><span class="o">=[</span>ChatCompletionResponseChoice<span class="o">(</span>
<span class="w">    </span><span class="nv">message</span><span class="o">=</span>ChatCompletionMessage<span class="o">(</span>
<span class="w">      </span><span class="nv">content</span><span class="o">=</span><span class="s2">&quot;Hi! How can I assist you today?&quot;</span>,<span class="w"> </span><span class="nv">role</span><span class="o">=</span><span class="s1">&#39;assistant&#39;</span>
<span class="w">    </span><span class="o">)</span>
<span class="w">  </span><span class="o">)]</span>,
<span class="w">  </span>...
<span class="o">)</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-1-VnVsa2Fu" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-1-VnVsa2Fu" name="VnVsa2Fu" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/libs
<span class="w">  </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1-vulkan.so<span class="w">    </span><span class="c1"># ===&gt; the model library (will be .dll for Windows)</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>tensor-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>We can now chat with the model using the command line interface (CLI) app or the Python API.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python
&gt;&gt;&gt;<span class="w"> </span>from<span class="w"> </span>mlc_llm<span class="w"> </span>import<span class="w"> </span>MLCEngine
&gt;&gt;&gt;<span class="w"> </span><span class="nv">engine</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>MLCEngine<span class="o">(</span><span class="nv">model</span><span class="o">=</span><span class="s2">&quot;./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC&quot;</span>,
...<span class="w">   </span><span class="nv">model_lib</span><span class="o">=</span><span class="s2">&quot;./dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-vulkan.so&quot;</span><span class="o">)</span>
&gt;&gt;&gt;<span class="w"> </span>engine.chat.completions.create<span class="o">(</span>
...<span class="w">   </span><span class="nv">messages</span><span class="o">=[{</span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,<span class="w"> </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;hello&quot;</span><span class="o">}]</span>
...<span class="w"> </span><span class="o">)</span>
ChatCompletionResponse<span class="o">(</span>
<span class="w">  </span><span class="nv">choices</span><span class="o">=[</span>ChatCompletionResponseChoice<span class="o">(</span>
<span class="w">    </span><span class="nv">message</span><span class="o">=</span>ChatCompletionMessage<span class="o">(</span>
<span class="w">      </span><span class="nv">content</span><span class="o">=</span><span class="s2">&quot;Hi! How can I assist you today?&quot;</span>,<span class="w"> </span><span class="nv">role</span><span class="o">=</span><span class="s1">&#39;assistant&#39;</span>
<span class="w">    </span><span class="o">)</span>
<span class="w">  </span><span class="o">)]</span>,
<span class="w">  </span>...
<span class="o">)</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-1-aU9TL2lQYWRPUw==" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-1-aU9TL2lQYWRPUw==" name="aU9TL2lQYWRPUw==" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/libs
<span class="w">  </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1-iphone.tar<span class="w">   </span><span class="c1"># ===&gt; the model library</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>tensor-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>The model lib <code class="docutils literal notranslate"><span class="pre">dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-iphone.tar</span></code>
will be packaged as a static library into the iOS app. Checkout <a class="reference internal" href="../deploy/ios.html#deploy-ios"><span class="std std-ref">iOS Swift SDK</span></a> for more details.</p>
</div><div aria-labelledby="tab-1-QW5kcm9pZA==" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-1-QW5kcm9pZA==" name="QW5kcm9pZA==" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/libs
<span class="w">  </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1-android.tar<span class="w">  </span><span class="c1"># ===&gt; the model library</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>tensor-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>The model lib <code class="docutils literal notranslate"><span class="pre">dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-android.tar</span></code>
will be packaged as a static library into the android app. Checkout <a class="reference internal" href="../deploy/android.html#deploy-android"><span class="std std-ref">Android SDK</span></a> for more details.</p>
</div><div aria-labelledby="tab-1-V2ViR1BV" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-1-V2ViR1BV" name="V2ViR1BV" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/libs
<span class="w">  </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1-webgpu.wasm<span class="w">  </span><span class="c1"># ===&gt; the model library</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>tensor-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>To use this in WebGPU runtime, checkout <a class="reference internal" href="../deploy/webllm.html#webllm-runtime"><span class="std std-ref">WebLLM Javascript SDK</span></a>.</p>
</div></div>
</section>
<section id="compile-commands-for-more-models">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Compile Commands for More Models</a><a class="headerlink" href="#compile-commands-for-more-models" title="Permalink to this heading">¶</a></h2>
<p>This section lists compile commands for more models that you can try out. Note that this can be easily
generalized to any model variant, as long as mlc-llm supports the architecture.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-2-2-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-2-2-0" name="2-0" role="tab" tabindex="0">Model: Llama-2-7B</button><button aria-controls="panel-2-2-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-1" name="2-1" role="tab" tabindex="-1">Mistral-7B-Instruct-v0.2</button><button aria-controls="panel-2-2-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-2" name="2-2" role="tab" tabindex="-1">Other models</button></div><div aria-labelledby="tab-2-2-0" class="sphinx-tabs-panel" id="panel-2-2-0" name="2-0" role="tabpanel" tabindex="0"><p>Please <a class="reference external" href="https://huggingface.co/meta-llama">request for access</a> to the Llama-2 weights from Meta first.
After granted access, first create directory <code class="docutils literal notranslate"><span class="pre">dist/models</span></code> and download the model to the directory.
For example, you can run the following code:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>dist/models<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>dist/models
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
<span class="nb">cd</span><span class="w"> </span>../..
</pre></div>
</div>
<p>Then convert the HF weights into MLC-compatible weights. Note that all platforms
can share the same compiled/quantized weights.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mlc_llm<span class="w"> </span>convert_weight<span class="w"> </span>./dist/models/Llama-2-7b-chat-hf/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>-o<span class="w"> </span>dist/Llama-2-7b-chat-hf-q4f16_1-MLC
</pre></div>
</div>
<p>Afterwards, run the following command to generate mlc config and compile the model.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create output directory for the model library compiled</span>
mkdir<span class="w"> </span>dist/libs
</pre></div>
</div>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-3-3-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-3-3-0" name="3-0" role="tab" tabindex="0">Target: CUDA</button><button aria-controls="panel-3-3-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-1" name="3-1" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-3-3-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-2" name="3-2" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-3-3-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-3" name="3-3" role="tab" tabindex="-1">WebGPU</button><button aria-controls="panel-3-3-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-4" name="3-4" role="tab" tabindex="-1">iPhone/iPad</button><button aria-controls="panel-3-3-5" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-5" name="3-5" role="tab" tabindex="-1">Android</button></div><div aria-labelledby="tab-3-3-0" class="sphinx-tabs-panel" id="panel-3-3-0" name="3-0" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Llama-2-7b-chat-hf/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>llama-2<span class="w"> </span>-o<span class="w"> </span>dist/Llama-2-7b-chat-hf-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>cuda<span class="w"> </span>-o<span class="w"> </span>dist/libs/Llama-2-7b-chat-hf-q4f16_1-cuda.so
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-1" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-1" name="3-1" role="tabpanel" tabindex="0"><p>For M-chip Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Llama-2-7b-chat-hf/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>llama-2<span class="w"> </span>-o<span class="w"> </span>dist/Llama-2-7b-chat-hf-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>metal<span class="w"> </span>-o<span class="w"> </span>dist/libs/Llama-2-7b-chat-hf-q4f16_1-metal.so
</pre></div>
</div>
<p>Cross-Compiling for Intel Mac on M-chip Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/RedPajama-INCITE-Chat-3B-v1/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>redpajama_chat<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>metal:x86-64<span class="w"> </span>-o<span class="w"> </span>dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-metal_x86_64.dylib
</pre></div>
</div>
<p>For Intel Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Llama-2-7b-chat-hf/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>llama-2<span class="w"> </span>-o<span class="w"> </span>dist/Llama-2-7b-chat-hf-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>metal<span class="w"> </span>-o<span class="w"> </span>dist/libs/Llama-2-7b-chat-hf-q4f16_1-metal_x86_64.dylib
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-2" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-2" name="3-2" role="tabpanel" tabindex="0"><p>For Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Llama-2-7b-chat-hf/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>llama-2<span class="w"> </span>-o<span class="w"> </span>dist/Llama-2-7b-chat-hf-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>vulkan<span class="w"> </span>-o<span class="w"> </span>dist/libs/Llama-2-7b-chat-hf-q4f16_1-vulkan.so
</pre></div>
</div>
<p>For Windows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Llama-2-7b-chat-hf/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>llama-2<span class="w"> </span>-o<span class="w"> </span>dist/Llama-2-7b-chat-hf-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>vulkan<span class="w"> </span>-o<span class="w"> </span>dist/libs/Llama-2-7b-chat-hf-q4f16_1-vulkan.dll
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-3" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-3" name="3-3" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Llama-2-7b-chat-hf/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--context-window-size<span class="w"> </span><span class="m">2048</span><span class="w"> </span>--conv-template<span class="w"> </span>llama-2<span class="w"> </span>-o<span class="w"> </span>dist/Llama-2-7b-chat-hf-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>webgpu<span class="w"> </span>-o<span class="w"> </span>dist/libs/Llama-2-7b-chat-hf-q4f16_1-webgpu.wasm
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To compile for webgpu, you need to build from source when installing <code class="docutils literal notranslate"><span class="pre">mlc_llm</span></code>. Besides, you also need to follow <a class="reference internal" href="../install/emcc.html#install-web-build"><span class="std std-ref">Install Wasm Build Environment</span></a>.
Otherwise, it would run into error</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RuntimeError: Cannot find libraries: wasm_runtime.bc
</pre></div>
</div>
</div>
</div><div aria-labelledby="tab-3-3-4" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-4" name="3-4" role="tabpanel" tabindex="0"><p>You need a Mac to compile models for it.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Llama-2-7b-chat-hf/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>llama-2<span class="w"> </span>--context-window-size<span class="w"> </span><span class="m">768</span><span class="w"> </span>-o<span class="w"> </span>dist/Llama-2-7b-chat-hf-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>iphone<span class="w"> </span>-o<span class="w"> </span>dist/libs/Llama-2-7b-chat-hf-q4f16_1-iphone.tar
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-5" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-5" name="3-5" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Llama-2-7b-chat-hf/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>llama-2<span class="w"> </span>--context-window-size<span class="w"> </span><span class="m">768</span><span class="w"> </span>-o<span class="w"> </span>dist/Llama-2-7b-chat-hf-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>android<span class="w"> </span>-o<span class="w"> </span>dist/libs/Llama-2-7b-chat-hf-q4f16_1-android.tar
</pre></div>
</div>
</div></div>
</div><div aria-labelledby="tab-2-2-1" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-1" name="2-1" role="tabpanel" tabindex="0"><p>Note that Mistral uses sliding window attention (SWA). Thus, instead of specifying
<code class="docutils literal notranslate"><span class="pre">context-window-size</span></code>, we specify <code class="docutils literal notranslate"><span class="pre">sliding-window-size</span></code>.</p>
<p>First create directory <code class="docutils literal notranslate"><span class="pre">dist/models</span></code> and download the model to the directory.
For example, you can run the following code:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>dist/models<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>dist/models
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
<span class="nb">cd</span><span class="w"> </span>../..
</pre></div>
</div>
<p>Then convert the HF weights into MLC-compatible weights. Note that all platforms
can share the same compiled/quantized weights.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mlc_llm<span class="w"> </span>convert_weight<span class="w"> </span>./dist/models/Mistral-7B-Instruct-v0.2/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC
</pre></div>
</div>
<p>Afterwards, run the following command to generate mlc config and compile the model.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create output directory for the model library compiled</span>
mkdir<span class="w"> </span>dist/libs
</pre></div>
</div>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-4-4-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-4-4-0" name="4-0" role="tab" tabindex="0">Target: CUDA</button><button aria-controls="panel-4-4-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-1" name="4-1" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-4-4-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-2" name="4-2" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-4-4-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-3" name="4-3" role="tab" tabindex="-1">WebGPU</button><button aria-controls="panel-4-4-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-4" name="4-4" role="tab" tabindex="-1">iPhone/iPad</button><button aria-controls="panel-4-4-5" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-5" name="4-5" role="tab" tabindex="-1">Android</button></div><div aria-labelledby="tab-4-4-0" class="sphinx-tabs-panel" id="panel-4-4-0" name="4-0" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Mistral-7B-Instruct-v0.2/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>mistral_default<span class="w"> </span>-o<span class="w"> </span>dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>cuda<span class="w"> </span>-o<span class="w"> </span>dist/libs/Mistral-7B-Instruct-v0.2-q4f16_1-cuda.so
</pre></div>
</div>
</div><div aria-labelledby="tab-4-4-1" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-1" name="4-1" role="tabpanel" tabindex="0"><p>For M-chip Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Mistral-7B-Instruct-v0.2/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>mistral_default<span class="w"> </span>-o<span class="w"> </span>dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>metal<span class="w"> </span>-o<span class="w"> </span>dist/libs/Mistral-7B-Instruct-v0.2-q4f16_1-metal.so
</pre></div>
</div>
<p>For Intel Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Mistral-7B-Instruct-v0.2/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>mistral_default<span class="w"> </span>-o<span class="w"> </span>dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>metal<span class="w"> </span>-o<span class="w"> </span>dist/libs/Mistral-7B-Instruct-v0.2-q4f16_1-metal_x86_64.dylib
</pre></div>
</div>
</div><div aria-labelledby="tab-4-4-2" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-2" name="4-2" role="tabpanel" tabindex="0"><p>For Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Mistral-7B-Instruct-v0.2/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>mistral_default<span class="w"> </span>-o<span class="w"> </span>dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>vulkan<span class="w"> </span>-o<span class="w"> </span>dist/libs/Mistral-7B-Instruct-v0.2-q4f16_1-vulkan.so
</pre></div>
</div>
<p>For Windows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Mistral-7B-Instruct-v0.2/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>mistral_default<span class="w"> </span>-o<span class="w"> </span>dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>vulkan<span class="w"> </span>-o<span class="w"> </span>dist/libs/Mistral-7B-Instruct-v0.2-q4f16_1-vulkan.dll
</pre></div>
</div>
</div><div aria-labelledby="tab-4-4-3" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-3" name="4-3" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Mistral-7B-Instruct-v0.2/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--prefill-chunk-size<span class="w"> </span><span class="m">1024</span><span class="w"> </span>--conv-template<span class="w"> </span>mistral_default<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>webgpu<span class="w"> </span>-o<span class="w"> </span>dist/libs/Mistral-7B-Instruct-v0.2-q4f16_1-webgpu.wasm
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To compile for webgpu, you need to build from source when installing <code class="docutils literal notranslate"><span class="pre">mlc_llm</span></code>. Besides, you also need to follow <a class="reference internal" href="../install/emcc.html#install-web-build"><span class="std std-ref">Install Wasm Build Environment</span></a>.
Otherwise, it would run into error</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RuntimeError: Cannot find libraries: wasm_runtime.bc
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For webgpu, when compiling larger models like <code class="docutils literal notranslate"><span class="pre">Llama-2-7B</span></code>, you may want to add <code class="docutils literal notranslate"><span class="pre">--prefill-chunk-size</span> <span class="pre">1024</span></code> or lower <code class="docutils literal notranslate"><span class="pre">--context-window-size</span></code> to decrease memory usage.
Otherwise, you may run into issues like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TypeError: Failed to execute &#39;createBuffer&#39; on &#39;GPUDevice&#39;: Failed to read the &#39;size&#39; property from
&#39;GPUBufferDescriptor&#39;: Value is outside the &#39;unsigned long long&#39; value range.
</pre></div>
</div>
</div>
</div><div aria-labelledby="tab-4-4-4" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-4" name="4-4" role="tabpanel" tabindex="0"><p>You need a Mac to compile models for it.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Mistral-7B-Instruct-v0.2/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>mistral_default<span class="w"> </span>--sliding-window-size<span class="w"> </span><span class="m">1024</span><span class="w"> </span>--prefill-chunk-size<span class="w"> </span><span class="m">128</span><span class="w">  </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>iphone<span class="w"> </span>-o<span class="w"> </span>dist/libs/Mistral-7B-Instruct-v0.2-q4f16_1-iphone.tar
</pre></div>
</div>
</div><div aria-labelledby="tab-4-4-5" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-5" name="4-5" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/Mistral-7B-Instruct-v0.2/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conv-template<span class="w"> </span>mistral_default<span class="w"> </span>--sliding-window-size<span class="w"> </span><span class="m">1024</span><span class="w"> </span>--prefill-chunk-size<span class="w"> </span><span class="m">128</span><span class="w"> </span>-o<span class="w"> </span>dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>android<span class="w"> </span>-o<span class="w"> </span>dist/libs/Mistral-7B-Instruct-v0.2-q4f16_1-android.tar
</pre></div>
</div>
</div></div>
</div><div aria-labelledby="tab-2-2-2" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-2" name="2-2" role="tabpanel" tabindex="0"><p>First create directory <code class="docutils literal notranslate"><span class="pre">dist/models</span></code> and download the model to the directory.
For example, you can run the following code:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>dist/models<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>dist/models
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/DISTRIBUTOR/HF_MODEL
<span class="nb">cd</span><span class="w"> </span>../..
</pre></div>
</div>
<p>Then convert the HF weights into MLC-compatible weights. Note that all platforms
can share the same compiled/quantized weights.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mlc_llm<span class="w"> </span>convert_weight<span class="w"> </span>./dist/models/HF_MODEL/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>-o<span class="w"> </span>dist/OUTPUT-MLC
</pre></div>
</div>
<p>Afterwards, run the following command to generate mlc config and compile the model.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create output directory for the model library compiled</span>
mkdir<span class="w"> </span>dist/libs
</pre></div>
</div>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-5-5-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-5-5-0" name="5-0" role="tab" tabindex="0">Target: CUDA</button><button aria-controls="panel-5-5-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-5-5-1" name="5-1" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-5-5-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-5-5-2" name="5-2" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-5-5-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-5-5-3" name="5-3" role="tab" tabindex="-1">WebGPU</button><button aria-controls="panel-5-5-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-5-5-4" name="5-4" role="tab" tabindex="-1">iPhone/iPad</button><button aria-controls="panel-5-5-5" aria-selected="false" class="sphinx-tabs-tab" id="tab-5-5-5" name="5-5" role="tab" tabindex="-1">Android</button></div><div aria-labelledby="tab-5-5-0" class="sphinx-tabs-panel" id="panel-5-5-0" name="5-0" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/HF_MODEL/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>CONV_TEMPLATE<span class="w"> </span>-o<span class="w"> </span>dist/OUTPUT-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/OUTPUT-MLC/mlc-chat-config.json<span class="w"> </span>--device<span class="w"> </span>cuda<span class="w"> </span>-o<span class="w"> </span>dist/libs/OUTPUT-cuda.so
</pre></div>
</div>
</div><div aria-labelledby="tab-5-5-1" class="sphinx-tabs-panel" hidden="true" id="panel-5-5-1" name="5-1" role="tabpanel" tabindex="0"><p>For M-chip Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/HF_MODEL/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>CONV_TEMPLATE<span class="w"> </span>-o<span class="w"> </span>dist/OUTPUT-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/OUTPUT-MLC/mlc-chat-config.json<span class="w"> </span>--device<span class="w"> </span>metal<span class="w"> </span>-o<span class="w"> </span>dist/libs/OUTPUT-metal.so
</pre></div>
</div>
<p>For Intel Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/HF_MODEL/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>CONV_TEMPLATE<span class="w"> </span>-o<span class="w"> </span>dist/OUTPUT-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/OUTPUT-MLC/mlc-chat-config.json<span class="w"> </span>--device<span class="w"> </span>metal<span class="w"> </span>-o<span class="w"> </span>dist/libs/OUTPUT-metal_x86_64.dylib
</pre></div>
</div>
</div><div aria-labelledby="tab-5-5-2" class="sphinx-tabs-panel" hidden="true" id="panel-5-5-2" name="5-2" role="tabpanel" tabindex="0"><p>For Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/HF_MODEL/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>CONV_TEMPLATE<span class="w"> </span>-o<span class="w"> </span>dist/OUTPUT-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/OUTPUT-MLC/mlc-chat-config.json<span class="w"> </span>--device<span class="w"> </span>vulkan<span class="w"> </span>-o<span class="w"> </span>dist/libs/OUTPUT-vulkan.so
</pre></div>
</div>
<p>For Windows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/HF_MODEL/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>CONV_TEMPLATE<span class="w"> </span>-o<span class="w"> </span>dist/OUTPUT-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/OUTPUT-MLC/mlc-chat-config.json<span class="w"> </span>--device<span class="w"> </span>vulkan<span class="w"> </span>-o<span class="w"> </span>dist/libs/OUTPUT-vulkan.dll
</pre></div>
</div>
</div><div aria-labelledby="tab-5-5-3" class="sphinx-tabs-panel" hidden="true" id="panel-5-5-3" name="5-3" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/HF_MODEL/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>CONV_TEMPLATE<span class="w"> </span>-o<span class="w"> </span>dist/OUTPUT-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/OUTPUT-MLC/mlc-chat-config.json<span class="w"> </span>--device<span class="w"> </span>webgpu<span class="w"> </span>-o<span class="w"> </span>dist/libs/OUTPUT-webgpu.wasm
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To compile for webgpu, you need to build from source when installing <code class="docutils literal notranslate"><span class="pre">mlc_llm</span></code>. Besides, you also need to follow <a class="reference internal" href="../install/emcc.html#install-web-build"><span class="std std-ref">Install Wasm Build Environment</span></a>.
Otherwise, it would run into error</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RuntimeError: Cannot find libraries: wasm_runtime.bc
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For webgpu, when compiling larger models like <code class="docutils literal notranslate"><span class="pre">Llama-2-7B</span></code>, you may want to add <code class="docutils literal notranslate"><span class="pre">--prefill-chunk-size</span> <span class="pre">1024</span></code> or lower <code class="docutils literal notranslate"><span class="pre">--context-window-size</span></code> to decrease memory usage.
Otherwise, you may run into issues like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TypeError: Failed to execute &#39;createBuffer&#39; on &#39;GPUDevice&#39;: Failed to read the &#39;size&#39; property from
&#39;GPUBufferDescriptor&#39;: Value is outside the &#39;unsigned long long&#39; value range.
</pre></div>
</div>
</div>
</div><div aria-labelledby="tab-5-5-4" class="sphinx-tabs-panel" hidden="true" id="panel-5-5-4" name="5-4" role="tabpanel" tabindex="0"><p>You need a Mac to compile models for it.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/HF_MODEL/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>CONV_TEMPLATE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--context-window-size<span class="w"> </span><span class="m">768</span><span class="w"> </span>-o<span class="w"> </span>dist/OUTPUT-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/OUTPUT-MLC/mlc-chat-config.json<span class="w"> </span>--device<span class="w"> </span>iphone<span class="w"> </span>-o<span class="w"> </span>dist/libs/OUTPUT-iphone.tar
</pre></div>
</div>
</div><div aria-labelledby="tab-5-5-5" class="sphinx-tabs-panel" hidden="true" id="panel-5-5-5" name="5-5" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/HF_MODEL/<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>CONV_TEMPLATE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--context-window-size<span class="w"> </span><span class="m">768</span><span class="w"> </span>-o<span class="w"> </span>dist/OUTPUT-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/OUTPUT-MLC/mlc-chat-config.json<span class="w"> </span>--device<span class="w"> </span>android<span class="w"> </span>-o<span class="w"> </span>dist/libs/OUTPUT-android.tar
</pre></div>
</div>
</div></div>
</div></div>
<p>For each model and each backend, the above only provides the most recommended build command (which is the most optimized).
You can also try with different argument values (e.g., different quantization modes, context window size, etc.),
whose build results affect runtime memory requirement, and it is possible that they may not run as
fast and robustly as the provided one when running the model.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Uing 3-bit quantization usually can be overly aggressive and only works for limited settings.
If you encounter issues where the compiled model does not perform as expected,
consider utilizing a higher number of bits for quantization (e.g., 4-bit quantization).</p>
</div>
<p>If you are interested in distributing the model besides local execution, please checkout <a class="reference internal" href="convert_weights.html#distribute-compiled-models"><span class="std std-ref">(Optional) 3. Upload weights to HF</span></a>.</p>
</section>
<section id="compile-command-specification">
<span id="id2"></span><h2><a class="toc-backref" href="#id10" role="doc-backlink">Compile Command Specification</a><a class="headerlink" href="#compile-command-specification" title="Permalink to this heading">¶</a></h2>
<p>As you have seen in the section above, the model compilation is split into three steps: convert weights, generate
<code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, and compile the model. This section describes the list of options that can be used
during compilation.</p>
<section id="convert-weight">
<h3>1. Convert Weight<a class="headerlink" href="#convert-weight" title="Permalink to this heading">¶</a></h3>
<p>Weight conversion command follows the pattern below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mlc_llm convert_weight \
    CONFIG \
    --quantization QUANTIZATION_MODE \
    [--model-type MODEL_TYPE] \
    [--device DEVICE] \
    [--source SOURCE] \
    [--source-format SOURCE_FORMAT] \
    --output OUTPUT
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">CONFIG</span></code> is a positional argument. Arguments wrapped with <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">]</span></code> are optional.</p>
<dl class="option-list">
<dt><kbd><span class="option">--CONFIG</span></kbd></dt>
<dd><p>It can be one of the following:</p>
<ol class="arabic simple">
<li><p>Path to a HuggingFace model directory that contains a <code class="docutils literal notranslate"><span class="pre">config.json</span></code> or</p></li>
<li><p>Path to <code class="docutils literal notranslate"><span class="pre">config.json</span></code> in HuggingFace format, or</p></li>
<li><p>The name of a pre-defined model architecture.</p></li>
</ol>
<p>A <code class="docutils literal notranslate"><span class="pre">config.json</span></code> file in HuggingFace format defines the model architecture, including the vocabulary
size, the number of layers, the hidden size, number of attention heads, etc.
Example: <a class="reference external" href="https://huggingface.co/codellama/CodeLlama-7b-hf/blob/main/config.json">https://huggingface.co/codellama/CodeLlama-7b-hf/blob/main/config.json</a>.</p>
<p>A HuggingFace directory often contains a <code class="docutils literal notranslate"><span class="pre">config.json</span></code> which defines the model architecture,
the non-quantized model weights in PyTorch or SafeTensor format, tokenizer configurations,
as well as an optional <code class="docutils literal notranslate"><span class="pre">generation_config.json</span></code> provides additional default configuration for
text generation.
Example: <a class="reference external" href="https://huggingface.co/codellama/CodeLlama-7b-hf/tree/main">https://huggingface.co/codellama/CodeLlama-7b-hf/tree/main</a>.</p>
<p>For existing pre-defined model architecture, see <code class="docutils literal notranslate"><span class="pre">MODEL_PRESETS</span></code>
<a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/python/mlc_llm/compiler/model/model.py">here</a>.</p>
</dd>
<dt><kbd><span class="option">--quantization <var>QUANTIZATION_MODE</var></span></kbd></dt>
<dd><p>The quantization mode we use to compile.</p>
<p>See <a class="reference internal" href="configure_quantization.html#quantization-mode"><span class="std std-ref">Quantization Mode</span></a> for more information.
Available options are: <code class="docutils literal notranslate"><span class="pre">q0f16</span></code>, <code class="docutils literal notranslate"><span class="pre">q0f32</span></code>, <code class="docutils literal notranslate"><span class="pre">q3f16_1</span></code>, <code class="docutils literal notranslate"><span class="pre">q4f16_1</span></code>, <code class="docutils literal notranslate"><span class="pre">q4f32_1</span></code>, and
<code class="docutils literal notranslate"><span class="pre">q4f16_awq</span></code>.</p>
<p>We encourage you to use 4-bit quantization, as the text generated by 3-bit
quantized models may have bad quality depending on the model.</p>
</dd>
<dt><kbd><span class="option">--model-type <var>MODEL_TYPE</var></span></kbd></dt>
<dd><p>Model architecture such as “llama”. If not set, it is inferred from <code class="docutils literal notranslate"><span class="pre">config.json</span></code>.</p>
</dd>
<dt><kbd><span class="option">--device <var>DEVICE</var></span></kbd></dt>
<dd><p>The device used to do quantization such as “cuda” or “cuda:0”. Will detect from
local available GPUs if not specified.</p>
</dd>
<dt><kbd><span class="option">--source <var>SOURCE</var></span></kbd></dt>
<dd><p>The path to original model weight, infer from <code class="docutils literal notranslate"><span class="pre">config</span></code> if missing.</p>
</dd>
<dt><kbd><span class="option">--source-format <var>SOURCE_FORMAT</var></span></kbd></dt>
<dd><p>The format of source model weight, infer from <code class="docutils literal notranslate"><span class="pre">config</span></code> if missing.</p>
</dd>
<dt><kbd><span class="option">--output <var>OUTPUT</var></span></kbd></dt>
<dd><p>The output directory to save the quantized model weight.
Will create <code class="docutils literal notranslate"><span class="pre">params_shard_*.bin</span></code> and <code class="docutils literal notranslate"><span class="pre">`tensor-cache.json`</span></code> in this directory.</p>
</dd>
</dl>
</section>
<section id="generate-mlc-chat-config">
<h3>2. Generate MLC Chat Config<a class="headerlink" href="#generate-mlc-chat-config" title="Permalink to this heading">¶</a></h3>
<p>In order to compile a model, we first need to generate the <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>. This file contains specifications
like <code class="docutils literal notranslate"><span class="pre">context-window-size</span></code> and <code class="docutils literal notranslate"><span class="pre">sliding-window-size</span></code>, among others that can alter the model compiled. We also process
tokenizers in this step.</p>
<p>Config generation command follows the pattern below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mlc_llm gen_config \
    CONFIG \
    --quantization QUANTIZATION_MODE \
    [--model-type MODEL_TYPE] \
    --conv-template CONV_TEMPLATE \
    [--context-window-size CONTEXT_WINDOW_SIZE] \
    [--sliding-window-size SLIDING_WINDOW_SIZE] \
    [--prefill-chunk-size PREFILL_CHUNK_SIZE] \
    [--tensor-parallel-shard TENSOR_PARALLEL_SHARDS] \
    --output OUTPUT
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">CONFIG</span></code> is a positional argument. Arguments wrapped with <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">]</span></code> are optional.</p>
<dl class="option-list">
<dt><kbd><span class="option">--CONFIG</span></kbd></dt>
<dd><p>It can be one of the following:</p>
<ol class="arabic simple">
<li><p>Path to a HuggingFace model directory that contains a <code class="docutils literal notranslate"><span class="pre">config.json</span></code> or</p></li>
<li><p>Path to <code class="docutils literal notranslate"><span class="pre">config.json</span></code> in HuggingFace format, or</p></li>
<li><p>The name of a pre-defined model architecture.</p></li>
</ol>
<p>A <code class="docutils literal notranslate"><span class="pre">config.json</span></code> file in HuggingFace format defines the model architecture, including the vocabulary
size, the number of layers, the hidden size, number of attention heads, etc.
Example: <a class="reference external" href="https://huggingface.co/codellama/CodeLlama-7b-hf/blob/main/config.json">https://huggingface.co/codellama/CodeLlama-7b-hf/blob/main/config.json</a>.</p>
<p>A HuggingFace directory often contains a <code class="docutils literal notranslate"><span class="pre">config.json</span></code> which defines the model architecture,
the non-quantized model weights in PyTorch or SafeTensor format, tokenizer configurations,
as well as an optional <code class="docutils literal notranslate"><span class="pre">generation_config.json</span></code> provides additional default configuration for
text generation.
Example: <a class="reference external" href="https://huggingface.co/codellama/CodeLlama-7b-hf/tree/main">https://huggingface.co/codellama/CodeLlama-7b-hf/tree/main</a>.</p>
<p>For existing pre-defined model architecture, see <code class="docutils literal notranslate"><span class="pre">MODEL_PRESETS</span></code>
<a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/python/mlc_llm/compiler/model/model.py">here</a>.</p>
</dd>
<dt><kbd><span class="option">--quantization <var>QUANTIZATION_MODE</var></span></kbd></dt>
<dd><p>The quantization mode we use to compile.</p>
<p>See <a class="reference internal" href="configure_quantization.html#quantization-mode"><span class="std std-ref">Quantization Mode</span></a> for more information.
Available options are: <code class="docutils literal notranslate"><span class="pre">q0f16</span></code>, <code class="docutils literal notranslate"><span class="pre">q0f32</span></code>, <code class="docutils literal notranslate"><span class="pre">q3f16_1</span></code>, <code class="docutils literal notranslate"><span class="pre">q4f16_1</span></code>, <code class="docutils literal notranslate"><span class="pre">q4f32_1</span></code>, and
<code class="docutils literal notranslate"><span class="pre">q4f16_awq</span></code>.</p>
<p>We encourage you to use 4-bit quantization, as the text generated by 3-bit
quantized models may have bad quality depending on the model.</p>
</dd>
<dt><kbd><span class="option">--model-type <var>MODEL_TYPE</var></span></kbd></dt>
<dd><p>Model architecture such as “llama”. If not set, it is inferred from <code class="docutils literal notranslate"><span class="pre">config.json</span></code>.</p>
</dd>
<dt><kbd><span class="option">--conv-template <var>CONV_TEMPLATE</var></span></kbd></dt>
<dd><p>Conversation template. It depends on how the model is tuned. Use “LM” for vanilla base model
For existing pre-defined templates, see <code class="docutils literal notranslate"><span class="pre">CONV_TEMPLATES</span></code>
<a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/python/mlc_llm/model/model.py">here</a>.</p>
</dd>
<dt><kbd><span class="option">--context-window-size <var>CONTEXT_WINDOW_SIZE</var></span></kbd></dt>
<dd><p>Option to provide the maximum sequence length supported by the model.
This is usually explicitly shown as context length or context window in the model card.
If this option is not set explicitly, by default,
it will be determined by <code class="docutils literal notranslate"><span class="pre">context_window_size</span></code> or <code class="docutils literal notranslate"><span class="pre">max_position_embeddings</span></code> in <code class="docutils literal notranslate"><span class="pre">config.json</span></code>,
and the latter is usually inaccurate for some models.</p>
</dd>
<dt><kbd><span class="option">--sliding-window-size <var>SLIDING_WINDOW</var></span></kbd></dt>
<dd><p>(Experimental) The sliding window size in sliding window attention (SWA).
This optional field overrides the <code class="docutils literal notranslate"><span class="pre">sliding_window</span></code> in <code class="docutils literal notranslate"><span class="pre">config.json</span></code> for
those models that use SWA. Currently only useful when compiling mistral-based models.
This flag subjects to future refactoring.</p>
</dd>
<dt><kbd><span class="option">--prefill-chunk-size <var>PREFILL_CHUNK_SIZE</var></span></kbd></dt>
<dd><p>(Experimental) The chunk size during prefilling. By default,
the chunk size is the same as <code class="docutils literal notranslate"><span class="pre">context_window_size</span></code> or <code class="docutils literal notranslate"><span class="pre">sliding_window_size</span></code>.
This flag subjects to future refactoring.</p>
</dd>
<dt><kbd><span class="option">--tensor-parallel-shard <var>TENSOR_PARALLEL_SHARDS</var></span></kbd></dt>
<dd><p>Number of shards to split the model into in tensor parallelism multi-gpu inference.</p>
</dd>
<dt><kbd><span class="option">--output <var>OUTPUT</var></span></kbd></dt>
<dd><p>The output directory for generated configurations, including <cite>mlc-chat-config.json</cite> and tokenizer configuration.</p>
</dd>
</dl>
</section>
<section id="compile-model-library">
<h3>3. Compile Model Library<a class="headerlink" href="#compile-model-library" title="Permalink to this heading">¶</a></h3>
<p>After generating <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, we can compile the model into a model library (files ending in <code class="docutils literal notranslate"><span class="pre">.so</span></code>, <code class="docutils literal notranslate"><span class="pre">.tar</span></code>, etc. that contains
the inference logic of a model).</p>
<p>Model compilation command follows the pattern below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mlc_llm compile \
    MODEL \
    [--quantization QUANTIZATION_MODE] \
    [--model-type MODEL_TYPE] \
    [--device DEVICE] \
    [--host HOST] \
    [--opt OPT] \
    [--system-lib-prefix SYSTEM_LIB_PREFIX] \
    --output OUTPUT \
    [--overrides OVERRIDES]
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">MODEL</span></code> is a positional argument. Arguments wrapped with <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">]</span></code> are optional.</p>
<dl class="option-list">
<dt><kbd><span class="option">--MODEL</span></kbd></dt>
<dd><p>A path to <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, or an MLC model directory that contains <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p>
</dd>
<dt><kbd><span class="option">--quantization <var>QUANTIZATION_MODE</var></span></kbd></dt>
<dd><p>The quantization mode we use to compile. If unprovided, will infer from <code class="docutils literal notranslate"><span class="pre">MODEL</span></code>.</p>
<p>See <a class="reference internal" href="configure_quantization.html#quantization-mode"><span class="std std-ref">Quantization Mode</span></a> for more information.
Available options are: <code class="docutils literal notranslate"><span class="pre">q0f16</span></code>, <code class="docutils literal notranslate"><span class="pre">q0f32</span></code>, <code class="docutils literal notranslate"><span class="pre">q3f16_1</span></code>, <code class="docutils literal notranslate"><span class="pre">q4f16_1</span></code>, <code class="docutils literal notranslate"><span class="pre">q4f32_1</span></code>, and
<code class="docutils literal notranslate"><span class="pre">q4f16_awq</span></code>.</p>
<p>We encourage you to use 4-bit quantization, as the text generated by 3-bit
quantized models may have bad quality depending on the model.</p>
</dd>
<dt><kbd><span class="option">--model-type <var>MODEL_TYPE</var></span></kbd></dt>
<dd><p>Model architecture such as “llama”. If not set, it is inferred from <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p>
</dd>
<dt><kbd><span class="option">--device <var>DEVICE</var></span></kbd></dt>
<dd><p>The GPU device to compile the model to. If not set, it is inferred from GPUs available locally.</p>
</dd>
<dt><kbd><span class="option">--host <var>HOST</var></span></kbd></dt>
<dd><p>The host LLVM triple to compile the model to. If not set, it is inferred from the local CPU and OS.
Examples of the LLVM triple:</p>
<ol class="arabic simple">
<li><p>iPhones: arm64-apple-ios;</p></li>
<li><p>ARM64 Android phones: aarch64-linux-android;</p></li>
<li><p>WebAssembly: wasm32-unknown-unknown-wasm;</p></li>
<li><p>Windows: x86_64-pc-windows-msvc;</p></li>
<li><p>ARM macOS: arm64-apple-darwin.</p></li>
</ol>
</dd>
<dt><kbd><span class="option">--opt <var>OPT</var></span></kbd></dt>
<dd><p>Optimization flags. MLC LLM maintains a predefined set of optimization flags,
denoted as <code class="docutils literal notranslate"><span class="pre">O0</span></code>, <code class="docutils literal notranslate"><span class="pre">O1</span></code>, <code class="docutils literal notranslate"><span class="pre">O2</span></code>, <code class="docutils literal notranslate"><span class="pre">O3</span></code>, where <code class="docutils literal notranslate"><span class="pre">O0</span></code> means no optimization, <code class="docutils literal notranslate"><span class="pre">O2</span></code>
means majority of them, and <code class="docutils literal notranslate"><span class="pre">O3</span></code> represents extreme optimization that could
potentially break the system.</p>
<p>Meanwhile, optimization flags could be explicitly specified via details knobs, e.g.
<code class="docutils literal notranslate"><span class="pre">--opt=&quot;cutlass_attn=1;cutlass_norm=0;cublas_gemm=0;cudagraph=0&quot;</span></code>.</p>
</dd>
<dt><kbd><span class="option">--system-lib-prefix <var>SYSTEM_LIB_PREFIX</var></span></kbd></dt>
<dd><p>Adding a prefix to all symbols exported. Similar to <code class="docutils literal notranslate"><span class="pre">objcopy</span> <span class="pre">--prefix-symbols</span></code>.
This is useful when compiling multiple models into a single library to avoid symbol
conflicts. Different from objcopy, this takes no effect for shared library.</p>
</dd>
<dt><kbd><span class="option">--output <var>OUTPUT</var></span></kbd></dt>
<dd><p>The path to the output file. The suffix determines if the output file is a shared library or
objects. Available suffixes:</p>
<ol class="arabic simple">
<li><p>Linux: .so (shared), .tar (objects);</p></li>
<li><p>macOS: .dylib (shared), .tar (objects);</p></li>
<li><p>Windows: .dll (shared), .tar (objects);</p></li>
<li><p>Android, iOS: .tar (objects);</p></li>
<li><p>Web: .wasm (web assembly).</p></li>
</ol>
</dd>
<dt><kbd><span class="option">--overrides <var>OVERRIDES</var></span></kbd></dt>
<dd><p>Model configuration override. Configurations to override <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>. Supports
<code class="docutils literal notranslate"><span class="pre">context_window_size</span></code>, <code class="docutils literal notranslate"><span class="pre">prefill_chunk_size</span></code>, <code class="docutils literal notranslate"><span class="pre">sliding_window</span></code>, <code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code> and
<code class="docutils literal notranslate"><span class="pre">tensor_parallel_shards</span></code>. Meanwhile, model config could be explicitly specified via details
knobs, e.g. <code class="docutils literal notranslate"><span class="pre">--overrides</span> <span class="pre">&quot;context_window_size=1024;prefill_chunk_size=128&quot;</span></code>.</p>
</dd>
</dl>
</section>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="package_libraries_and_weights.html" class="btn btn-neutral float-right" title="Package Libraries and Weights" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="convert_weights.html" class="btn btn-neutral float-left" title="Convert Model Weights" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023-2025 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
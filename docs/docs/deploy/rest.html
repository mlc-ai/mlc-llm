





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>REST API &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CLI" href="cli.html" />
    <link rel="prev" title="WebLLM Javascript SDK" href="webllm.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/introduction.html">Introduction to MLC LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="webllm.html">WebLLM Javascript SDK</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">REST API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#install-mlc-llm-package">Install MLC-LLM Package</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quick-start">Quick Start</a></li>
<li class="toctree-l2"><a class="reference internal" href="#run-cli-with-multi-gpu">Run CLI with Multi-GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="#launch-the-server">Launch the Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="#api-endpoints">API Endpoints</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_engine.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS Swift SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="ide_integration.html">IDE Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlc_chat_config.html">Customize MLC Chat Config</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/convert_weights.html">Convert Model Weights</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Model Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/package_libraries_and_weights.html">Package Libraries and Weights</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/define_new_models.html">Define New Model Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/configure_quantization.html">Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Microserving API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../microserving/tutorial.html">Implement LLM Cross-engine Orchestration Patterns</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>REST API</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/rest.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="rest-api">
<span id="deploy-rest-api"></span><h1>REST API<a class="headerlink" href="#rest-api" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#install-mlc-llm-package" id="id2">Install MLC-LLM Package</a></p></li>
<li><p><a class="reference internal" href="#quick-start" id="id3">Quick Start</a></p></li>
<li><p><a class="reference internal" href="#run-cli-with-multi-gpu" id="id4">Run CLI with Multi-GPU</a></p></li>
<li><p><a class="reference internal" href="#launch-the-server" id="id5">Launch the Server</a></p></li>
<li><p><a class="reference internal" href="#api-endpoints" id="id6">API Endpoints</a></p></li>
</ul>
</nav>
<p>We provide <a class="reference external" href="https://www.ibm.com/topics/rest-apis#:~:text=the%20next%20step-,What%20is%20a%20REST%20API%3F,representational%20state%20transfer%20architectural%20style.">REST API</a>
for a user to interact with MLC-LLM in their own programs.</p>
<section id="install-mlc-llm-package">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Install MLC-LLM Package</a><a class="headerlink" href="#install-mlc-llm-package" title="Permalink to this heading">¶</a></h2>
<p>SERVE is a part of the MLC-LLM package, installation instruction for which can be found <a class="reference internal" href="../install/mlc_llm.html#install-mlc-packages"><span class="std std-ref">here</span></a>. Once you have install the MLC-LLM package, you can run the following command to check if the installation was successful:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlc_llm<span class="w"> </span>serve<span class="w"> </span>--help
</pre></div>
</div>
<p>You should see serve help message if the installation was successful.</p>
</section>
<section id="quick-start">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Quick Start</a><a class="headerlink" href="#quick-start" title="Permalink to this heading">¶</a></h2>
<p>This section provides a quick start guide to work with MLC-LLM REST API. To launch a server, run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlc_llm<span class="w"> </span>serve<span class="w"> </span>MODEL<span class="w"> </span><span class="o">[</span>--model-lib<span class="w"> </span>PATH-TO-MODEL-LIB<span class="o">]</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">MODEL</span></code> is the model folder after compiling with <a class="reference internal" href="../compilation/compile_models.html#compile-model-libraries"><span class="std std-ref">MLC-LLM build process</span></a>. Information about other arguments can be found under <a class="reference internal" href="#rest-launch-server"><span class="std std-ref">Launch the server</span></a> section.</p>
<p>Once you have launched the Server, you can use the API in your own program to send requests. Below is an example of using the API to interact with MLC-LLM in Python without Streaming (suppose the server is running on <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:8080/</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>import<span class="w"> </span>requests

<span class="c1"># Get a response using a prompt without streaming</span>
<span class="nv">payload</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>
<span class="w">   </span><span class="s2">&quot;model&quot;</span>:<span class="w"> </span><span class="s2">&quot;./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/&quot;</span>,
<span class="w">   </span><span class="s2">&quot;messages&quot;</span>:<span class="w"> </span><span class="o">[</span>
<span class="w">      </span><span class="o">{</span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,<span class="w"> </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;Write a haiku about apples.&quot;</span><span class="o">}</span>,
<span class="w">   </span><span class="o">]</span>,
<span class="w">   </span><span class="s2">&quot;stream&quot;</span>:<span class="w"> </span>False,
<span class="w">   </span><span class="c1"># &quot;n&quot;: 1,</span>
<span class="w">   </span><span class="s2">&quot;max_tokens&quot;</span>:<span class="w"> </span><span class="m">300</span>,
<span class="o">}</span>
<span class="nv">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>requests.post<span class="o">(</span><span class="s2">&quot;http://127.0.0.1:8080/v1/chat/completions&quot;</span>,<span class="w"> </span><span class="nv">json</span><span class="o">=</span>payload<span class="o">)</span>
<span class="nv">choices</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>r.json<span class="o">()[</span><span class="s2">&quot;choices&quot;</span><span class="o">]</span>
<span class="k">for</span><span class="w"> </span>choice<span class="w"> </span><span class="k">in</span><span class="w"> </span>choices:
<span class="w">   </span>print<span class="o">(</span>f<span class="s2">&quot;{choice[&#39;message&#39;][&#39;content&#39;]}\n&quot;</span><span class="o">)</span>
</pre></div>
</div>
</section>
<section id="run-cli-with-multi-gpu">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Run CLI with Multi-GPU</a><a class="headerlink" href="#run-cli-with-multi-gpu" title="Permalink to this heading">¶</a></h2>
<p>If you want to enable tensor parallelism to run LLMs on multiple GPUs, please specify argument <code class="docutils literal notranslate"><span class="pre">--overrides</span> <span class="pre">&quot;tensor_parallel_shards=$NGPU&quot;</span></code>. For example,</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mlc_llm<span class="w"> </span>serve<span class="w"> </span>HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC<span class="w"> </span>--overrides<span class="w"> </span><span class="s2">&quot;tensor_parallel_shards=2&quot;</span>
</pre></div>
</div>
<hr class="docutils" />
</section>
<section id="launch-the-server">
<span id="rest-launch-server"></span><h2><a class="toc-backref" href="#id5" role="doc-backlink">Launch the Server</a><a class="headerlink" href="#launch-the-server" title="Permalink to this heading">¶</a></h2>
<p>To launch the MLC Server for MLC-LLM, run the following command in your terminal.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlc_llm<span class="w"> </span>serve<span class="w"> </span>MODEL<span class="w"> </span><span class="o">[</span>--model-lib<span class="w"> </span>PATH-TO-MODEL-LIB<span class="o">]</span><span class="w"> </span><span class="o">[</span>--device<span class="w"> </span>DEVICE<span class="o">]</span><span class="w"> </span><span class="o">[</span>--mode<span class="w"> </span>MODE<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--additional-models<span class="w"> </span>ADDITIONAL-MODELS<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--speculative-mode<span class="w"> </span>SPECULATIVE-MODE<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--overrides<span class="w"> </span>OVERRIDES<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--enable-tracing<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--host<span class="w"> </span>HOST<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--port<span class="w"> </span>PORT<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--allow-credentials<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--allowed-origins<span class="w"> </span>ALLOWED_ORIGINS<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--allowed-methods<span class="w"> </span>ALLOWED_METHODS<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--allowed-headers<span class="w"> </span>ALLOWED_HEADERS<span class="o">]</span>
</pre></div>
</div>
<dl class="simple">
<dt>MODEL                  The model folder after compiling with MLC-LLM build process. The parameter</dt><dd><p>can either be the model name with its quantization scheme
(e.g. <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>), or a full path to the model
folder. In the former case, we will use the provided name to search
for the model folder over possible paths.</p>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">--model-lib</span></kbd></dt>
<dd><p>A field to specify the full path to the model library file to use (e.g. a <code class="docutils literal notranslate"><span class="pre">.so</span></code> file).</p>
</dd>
<dt><kbd><span class="option">--device</span></kbd></dt>
<dd><p>The description of the device to run on. User should provide a string in the
form of <code class="docutils literal notranslate"><span class="pre">device_name:device_id</span></code> or <code class="docutils literal notranslate"><span class="pre">device_name</span></code>, where <code class="docutils literal notranslate"><span class="pre">device_name</span></code> is one of
<code class="docutils literal notranslate"><span class="pre">cuda</span></code>, <code class="docutils literal notranslate"><span class="pre">metal</span></code>, <code class="docutils literal notranslate"><span class="pre">vulkan</span></code>, <code class="docutils literal notranslate"><span class="pre">rocm</span></code>, <code class="docutils literal notranslate"><span class="pre">opencl</span></code>, <code class="docutils literal notranslate"><span class="pre">auto</span></code> (automatically detect the
local device), and <code class="docutils literal notranslate"><span class="pre">device_id</span></code> is the device id to run on. The default value is <code class="docutils literal notranslate"><span class="pre">auto</span></code>,
with the device id set to 0 for default.</p>
</dd>
<dt><kbd><span class="option">--mode</span></kbd></dt>
<dd><p>The engine mode in MLC LLM.
We provide three preset modes: <code class="docutils literal notranslate"><span class="pre">local</span></code>, <code class="docutils literal notranslate"><span class="pre">interactive</span></code> and <code class="docutils literal notranslate"><span class="pre">server</span></code>.
The default mode is <code class="docutils literal notranslate"><span class="pre">local</span></code>.</p>
<p>The choice of mode decides the values of “max_num_sequence”, “max_total_sequence_length”
and “prefill_chunk_size” when they are not explicitly specified.</p>
<p>1. Mode “local” refers to the local server deployment which has low
request concurrency. So the max batch size will be set to 4, and max
total sequence length and prefill chunk size are set to the context
window size (or sliding window size) of the model.</p>
<p>2. Mode “interactive” refers to the interactive use of server, which
has at most 1 concurrent request. So the max batch size will be set to 1,
and max total sequence length and prefill chunk size are set to the context
window size (or sliding window size) of the model.</p>
<p>3. Mode “server” refers to the large server use case which may handle
many concurrent request and want to use GPU memory as much as possible.
In this mode, we will automatically infer the largest possible max batch
size and max total sequence length.</p>
<p>You can manually specify arguments “max_num_sequence”, “max_total_seq_length” and
“prefill_chunk_size” via <code class="docutils literal notranslate"><span class="pre">--overrides</span></code> to override the automatic inferred values.
For example: <code class="docutils literal notranslate"><span class="pre">--overrides</span> <span class="pre">&quot;max_num_sequence=32;max_total_seq_length=4096&quot;</span></code>.</p>
</dd>
<dt><kbd><span class="option">--additional-models</span></kbd></dt>
<dd><p>The model paths and (optional) model library paths of additional models (other
than the main model).</p>
<p>When engine is enabled with speculative decoding, additional models are needed.
<strong>We only support one additional model for speculative decoding now.</strong>
The way of specifying the additional model is:
<code class="docutils literal notranslate"><span class="pre">--additional-models</span> <span class="pre">model_path_1</span></code> or
<code class="docutils literal notranslate"><span class="pre">--additional-models</span> <span class="pre">model_path_1,model_lib_1</span></code>.</p>
<p>When the model lib of a model is not given, JIT model compilation will be activated
to compile the model automatically.</p>
</dd>
<dt><kbd><span class="option">--speculative-mode</span></kbd></dt>
<dd><p>The speculative decoding mode. Right now four options are supported:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">disable</span></code>, where speculative decoding is not enabled,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">small_draft</span></code>, denoting the normal speculative decoding (small draft) style,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eagle</span></code>, denoting the eagle-style speculative decoding.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">medusa</span></code>, denoting the medusa-style speculative decoding.</p></li>
</ul>
</dd>
<dt><kbd><span class="option">--overrides</span></kbd></dt>
<dd><p>Overriding extra configurable fields of EngineConfig.</p>
<p>Supporting fields that can be be overridden: <code class="docutils literal notranslate"><span class="pre">tensor_parallel_shards</span></code>, <code class="docutils literal notranslate"><span class="pre">max_num_sequence</span></code>,
<code class="docutils literal notranslate"><span class="pre">max_total_seq_length</span></code>, <code class="docutils literal notranslate"><span class="pre">prefill_chunk_size</span></code>, <code class="docutils literal notranslate"><span class="pre">max_history_size</span></code>, <code class="docutils literal notranslate"><span class="pre">gpu_memory_utilization</span></code>,
<code class="docutils literal notranslate"><span class="pre">spec_draft_length</span></code>, <code class="docutils literal notranslate"><span class="pre">prefix_cache_max_num_recycling_seqs</span></code>, <code class="docutils literal notranslate"><span class="pre">context_window_size</span></code>,
<code class="docutils literal notranslate"><span class="pre">sliding_window_size</span></code>, <code class="docutils literal notranslate"><span class="pre">attention_sink_size</span></code>.</p>
<p>Please check out the documentation of EngineConfig in <code class="docutils literal notranslate"><span class="pre">mlc_llm/serve/config.py</span></code>
for detailed docstring of each field.
Example: <code class="docutils literal notranslate"><span class="pre">--overrides</span> <span class="pre">&quot;max_num_sequence=32;max_total_seq_length=4096;tensor_parallel_shards=2&quot;</span></code></p>
</dd>
<dt><kbd><span class="option">--enable-tracing</span></kbd></dt>
<dd><p>A boolean indicating if to enable event logging for requests.</p>
</dd>
<dt><kbd><span class="option">--host</span></kbd></dt>
<dd><p>The host at which the server should be started, defaults to <code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code>.</p>
</dd>
<dt><kbd><span class="option">--port</span></kbd></dt>
<dd><p>The port on which the server should be started, defaults to <code class="docutils literal notranslate"><span class="pre">8000</span></code>.</p>
</dd>
<dt><kbd><span class="option">--allow-credentials</span></kbd></dt>
<dd><p>A flag to indicate whether the server should allow credentials. If set, the server will
include the <code class="docutils literal notranslate"><span class="pre">CORS</span></code> header in the response</p>
</dd>
<dt><kbd><span class="option">--allowed-origins</span></kbd></dt>
<dd><p>Specifies the allowed origins. It expects a JSON list of strings, with the default value being <code class="docutils literal notranslate"><span class="pre">[&quot;*&quot;]</span></code>, allowing all origins.</p>
</dd>
<dt><kbd><span class="option">--allowed-methods</span></kbd></dt>
<dd><p>Specifies the allowed methods. It expects a JSON list of strings, with the default value being <code class="docutils literal notranslate"><span class="pre">[&quot;*&quot;]</span></code>, allowing all methods.</p>
</dd>
<dt><kbd><span class="option">--allowed-headers</span></kbd></dt>
<dd><p>Specifies the allowed headers. It expects a JSON list of strings, with the default value being <code class="docutils literal notranslate"><span class="pre">[&quot;*&quot;]</span></code>, allowing all headers.</p>
</dd>
</dl>
<p>You can access <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:PORT/docs</span></code> (replace <code class="docutils literal notranslate"><span class="pre">PORT</span></code> with the port number you specified) to see the list of
supported endpoints.</p>
</section>
<section id="api-endpoints">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">API Endpoints</a><a class="headerlink" href="#api-endpoints" title="Permalink to this heading">¶</a></h2>
<p>The REST API provides the following endpoints:</p>
<dl class="http get">
<dt class="sig sig-object http" id="get--v1-models">
<span class="sig-name descname"><span class="pre">GET</span> </span><span class="sig-name descname"><span class="pre">/v1/models</span></span><a class="headerlink" href="#get--v1-models" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<hr class="docutils" />
<blockquote>
<div><p>Get a list of models available for MLC-LLM.</p>
</div></blockquote>
<p><strong>Example</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>import<span class="w"> </span>requests

<span class="nv">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;http://127.0.0.1:8000/v1/models&quot;</span>
<span class="nv">headers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span><span class="s2">&quot;accept&quot;</span>:<span class="w"> </span><span class="s2">&quot;application/json&quot;</span><span class="o">}</span>

<span class="nv">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>requests.get<span class="o">(</span>url,<span class="w"> </span><span class="nv">headers</span><span class="o">=</span>headers<span class="o">)</span>

<span class="k">if</span><span class="w"> </span>response.status_code<span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">200</span>:
<span class="w">   </span>print<span class="o">(</span><span class="s2">&quot;Response:&quot;</span><span class="o">)</span>
<span class="w">   </span>print<span class="o">(</span>response.json<span class="o">())</span>
<span class="k">else</span>:
<span class="w">   </span>print<span class="o">(</span><span class="s2">&quot;Error:&quot;</span>,<span class="w"> </span>response.status_code<span class="o">)</span>
</pre></div>
</div>
<dl class="http post">
<dt class="sig sig-object http" id="post--v1-chat-completions">
<span class="sig-name descname"><span class="pre">POST</span> </span><span class="sig-name descname"><span class="pre">/v1/chat/completions</span></span><a class="headerlink" href="#post--v1-chat-completions" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<hr class="docutils" />
<blockquote>
<div><p>Get a response from MLC-LLM using a prompt, either with or without streaming.</p>
</div></blockquote>
<p><strong>Chat Completion Request Object</strong></p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>messages</strong> (<em>List[ChatCompletionMessage]</em>, required): A sequence of messages that have been exchanged in the conversation so far. Each message in the conversation is represented by a <cite>ChatCompletionMessage</cite> object, which includes the following fields:</dt><dd><ul>
<li><p><strong>content</strong> (<em>Optional[Union[str, List[Dict[str, str]]]]</em>): The text content of the message or structured data in case of tool-generated messages.</p></li>
<li><p><strong>role</strong> (<em>Literal[“system”, “user”, “assistant”, “tool”]</em>): The role of the message sender, indicating whether the message is from the system, user, assistant, or a tool.</p></li>
<li><p><strong>name</strong> (<em>Optional[str]</em>): An optional name for the sender of the message.</p></li>
<li><p><strong>tool_calls</strong> (<em>Optional[List[ChatToolCall]]</em>): A list of calls to external tools or functions made within this message, applicable when the role is <cite>tool</cite>.</p></li>
<li><p><strong>tool_call_id</strong> (<em>Optional[str]</em>): A unique identifier for the tool call, relevant when integrating external tools or services.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><strong>model</strong> (<em>str</em>, required): The model to be used for generating responses.</p></li>
<li><p><strong>frequency_penalty</strong> (<em>float</em>, optional, default=0.0): Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat tokens.</p></li>
<li><p><strong>presence_penalty</strong> (<em>float</em>, optional, default=0.0): Positive values penalize new tokens if they are already present in the text so far, decreasing the model’s likelihood to repeat tokens.</p></li>
<li><p><strong>logprobs</strong> (<em>bool</em>, optional, default=False): Indicates whether to include log probabilities for each token in the response.</p></li>
<li><p><strong>top_logprobs</strong> (<em>int</em>, optional, default=0): An integer ranging from 0 to 20. It determines the number of tokens, most likely to appear at each position, to be returned. Each token is accompanied by a log probability. If this parameter is used, ‘logprobs’ must be set to true.</p></li>
<li><p><strong>logit_bias</strong> (<em>Optional[Dict[int, float]]</em>): Allows specifying biases for or against specific tokens during generation.</p></li>
<li><p><strong>max_tokens</strong> (<em>Optional[int]</em>): The maximum number of tokens to generate in the response(s).</p></li>
<li><p><strong>n</strong> (<em>int</em>, optional, default=1): Number of responses to generate for the given prompt.</p></li>
<li><p><strong>seed</strong> (<em>Optional[int]</em>): A seed for deterministic generation. Using the same seed and inputs will produce the same output.</p></li>
<li><p><strong>stop</strong> (<em>Optional[Union[str, List[str]]]</em>): One or more strings that, if encountered, will cause generation to stop.</p></li>
<li><p><strong>stream</strong> (<em>bool</em>, optional, default=False): If <cite>True</cite>, responses are streamed back as they are generated.</p></li>
<li><p><strong>temperature</strong> (<em>float</em>, optional, default=1.0): Controls the randomness of the generation. Lower values lead to less random completions.</p></li>
<li><p><strong>top_p</strong> (<em>float</em>, optional, default=1.0): Nucleus sampling parameter that controls the diversity of the generated responses.</p></li>
<li><p><strong>tools</strong> (<em>Optional[List[ChatTool]]</em>): Specifies external tools or functions that can be called as part of the chat.</p></li>
<li><p><strong>tool_choice</strong> (<em>Optional[Union[Literal[“none”, “auto”], Dict]]</em>): Controls how tools are selected for use in responses.</p></li>
<li><p><strong>user</strong> (<em>Optional[str]</em>): An optional identifier for the user initiating the request.</p></li>
<li><p><strong>response_format</strong> (<em>RequestResponseFormat</em>, optional): Specifies the format of the response. Can be either “text” or “json_object”, with optional schema definition for JSON responses.</p></li>
</ul>
<p><strong>Returns</strong></p>
<ul class="simple">
<li><p>If <cite>stream</cite> is <cite>False</cite>, a <cite>ChatCompletionResponse</cite> object containing the generated response(s).</p></li>
<li><p>If <cite>stream</cite> is <cite>True</cite>, a stream of <cite>ChatCompletionStreamResponse</cite> objects, providing a real-time feed of generated responses.</p></li>
</ul>
<p><strong>ChatCompletionResponseChoice</strong></p>
<ul class="simple">
<li><p><strong>finish_reason</strong> (<em>Optional[Literal[“stop”, “length”, “tool_calls”, “error”]]</em>, optional): The reason the completion process was terminated. It can be due to reaching a stop condition, the maximum length, output of tool calls, or an error.</p></li>
<li><p><strong>index</strong> (<em>int</em>, required, default=0): Indicates the position of this choice within the list of choices.</p></li>
<li><p><strong>message</strong> (<em>ChatCompletionMessage</em>, required): The message part of the chat completion, containing the content of the chat response.</p></li>
<li><p><strong>logprobs</strong> (<em>Optional[LogProbs]</em>, optional): Optionally includes log probabilities for each output token</p></li>
</ul>
<p><strong>ChatCompletionStreamResponseChoice</strong></p>
<ul class="simple">
<li><p><strong>finish_reason</strong> (<em>Optional[Literal[“stop”, “length”, “tool_calls”]]</em>, optional): Specifies why the streaming completion process ended. Valid reasons are “stop”, “length”, and “tool_calls”.</p></li>
<li><p><strong>index</strong> (<em>int</em>, required, default=0): Indicates the position of this choice within the list of choices.</p></li>
<li><p><strong>delta</strong> (<em>ChatCompletionMessage</em>, required): Represents the incremental update or addition to the chat completion message in the stream.</p></li>
<li><p><strong>logprobs</strong> (<em>Optional[LogProbs]</em>, optional): Optionally includes log probabilities for each output token</p></li>
</ul>
<p><strong>ChatCompletionResponse</strong></p>
<ul class="simple">
<li><p><strong>id</strong> (<em>str</em>, required): A unique identifier for the chat completion session.</p></li>
<li><p><strong>choices</strong> (<em>List[ChatCompletionResponseChoice]</em>, required): A collection of <cite>ChatCompletionResponseChoice</cite> objects, representing the potential responses generated by the model.</p></li>
<li><p><strong>created</strong> (<em>int</em>, required, default=current time): The UNIX timestamp representing when the response was generated.</p></li>
<li><p><strong>model</strong> (<em>str</em>, required): The name of the model used to generate the chat completions.</p></li>
<li><p><strong>system_fingerprint</strong> (<em>str</em>, required): A system-generated fingerprint that uniquely identifies the computational environment.</p></li>
<li><p><strong>object</strong> (<em>Literal[“chat.completion”]</em>, required, default=”chat.completion”): A string literal indicating the type of object, here always “chat.completion”.</p></li>
<li><p><strong>usage</strong> (<em>UsageInfo</em>, required, default=empty <cite>UsageInfo</cite> object): Contains information about the API usage for this specific request.</p></li>
</ul>
<p><strong>ChatCompletionStreamResponse</strong></p>
<ul class="simple">
<li><p><strong>id</strong> (<em>str</em>, required): A unique identifier for the streaming chat completion session.</p></li>
<li><p><strong>choices</strong> (<em>List[ChatCompletionStreamResponseChoice]</em>, required): A list of <cite>ChatCompletionStreamResponseChoice</cite> objects, each representing a part of the streaming chat response.</p></li>
<li><p><strong>created</strong> (<em>int</em>, required, default=current time): The creation time of the streaming response, represented as a UNIX timestamp.</p></li>
<li><p><strong>model</strong> (<em>str</em>, required): Specifies the model that was used for generating the streaming chat completions.</p></li>
<li><p><strong>system_fingerprint</strong> (<em>str</em>, required): A unique identifier for the system generating the streaming completions.</p></li>
<li><p><strong>object</strong> (<em>Literal[“chat.completion.chunk”]</em>, required, default=”chat.completion.chunk”): A literal indicating that this object represents a chunk of a streaming chat completion.</p></li>
</ul>
<hr class="docutils" />
<p><strong>Example</strong></p>
<p>Below is an example of using the API to interact with MLC-LLM in Python with Streaming.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>import<span class="w"> </span>requests
import<span class="w"> </span>json

<span class="c1"># Get a response using a prompt with streaming</span>
<span class="nv">payload</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>
<span class="w"> </span><span class="s2">&quot;model&quot;</span>:<span class="w"> </span><span class="s2">&quot;./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/&quot;</span>,
<span class="w"> </span><span class="s2">&quot;messages&quot;</span>:<span class="w"> </span><span class="o">[{</span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,<span class="w"> </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;Write a haiku&quot;</span><span class="o">}]</span>,
<span class="w"> </span><span class="s2">&quot;stream&quot;</span>:<span class="w"> </span>True,
<span class="o">}</span>
with<span class="w"> </span>requests.post<span class="o">(</span><span class="s2">&quot;http://127.0.0.1:8080/v1/chat/completions&quot;</span>,<span class="w"> </span><span class="nv">json</span><span class="o">=</span>payload,<span class="w"> </span><span class="nv">stream</span><span class="o">=</span>True<span class="o">)</span><span class="w"> </span>as<span class="w"> </span>r:
<span class="w">   </span><span class="k">for</span><span class="w"> </span>chunk<span class="w"> </span><span class="k">in</span><span class="w"> </span>r.iter_content<span class="o">(</span><span class="nv">chunk_size</span><span class="o">=</span>None<span class="o">)</span>:
<span class="w">      </span><span class="nv">chunk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>chunk.decode<span class="o">(</span><span class="s2">&quot;utf-8&quot;</span><span class="o">)</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="s2">&quot;[DONE]&quot;</span><span class="w"> </span><span class="k">in</span><span class="w"> </span>chunk<span class="o">[</span><span class="m">6</span>:<span class="o">]</span>:
<span class="w">         </span><span class="k">break</span>
<span class="w">      </span><span class="nv">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>json.loads<span class="o">(</span>chunk<span class="o">[</span><span class="m">6</span>:<span class="o">])</span>
<span class="w">      </span><span class="nv">content</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>response<span class="o">[</span><span class="s2">&quot;choices&quot;</span><span class="o">][</span><span class="m">0</span><span class="o">][</span><span class="s2">&quot;delta&quot;</span><span class="o">]</span>.get<span class="o">(</span><span class="s2">&quot;content&quot;</span>,<span class="w"> </span><span class="s2">&quot;&quot;</span><span class="o">)</span>
<span class="w">      </span>print<span class="o">(</span>content,<span class="w"> </span><span class="nv">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span>,<span class="w"> </span><span class="nv">flush</span><span class="o">=</span>True<span class="o">)</span>
print<span class="o">(</span><span class="s2">&quot;\n&quot;</span><span class="o">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>There is also support for function calling similar to OpenAI (<a class="reference external" href="https://platform.openai.com/docs/guides/function-calling">https://platform.openai.com/docs/guides/function-calling</a>). Below is an example on how to use function calling in Python.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>import<span class="w"> </span>requests
import<span class="w"> </span>json

<span class="nv">tools</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span>
<span class="w">   </span><span class="o">{</span>
<span class="w">      </span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;function&quot;</span>,
<span class="w">      </span><span class="s2">&quot;function&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">            </span><span class="s2">&quot;name&quot;</span>:<span class="w"> </span><span class="s2">&quot;get_current_weather&quot;</span>,
<span class="w">            </span><span class="s2">&quot;description&quot;</span>:<span class="w"> </span><span class="s2">&quot;Get the current weather in a given location&quot;</span>,
<span class="w">            </span><span class="s2">&quot;parameters&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">               </span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;object&quot;</span>,
<span class="w">               </span><span class="s2">&quot;properties&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">                  </span><span class="s2">&quot;location&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">                        </span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;string&quot;</span>,
<span class="w">                        </span><span class="s2">&quot;description&quot;</span>:<span class="w"> </span><span class="s2">&quot;The city and state, e.g. San Francisco, CA&quot;</span>,
<span class="w">                  </span><span class="o">}</span>,
<span class="w">                  </span><span class="s2">&quot;unit&quot;</span>:<span class="w"> </span><span class="o">{</span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;string&quot;</span>,<span class="w"> </span><span class="s2">&quot;enum&quot;</span>:<span class="w"> </span><span class="o">[</span><span class="s2">&quot;celsius&quot;</span>,<span class="w"> </span><span class="s2">&quot;fahrenheit&quot;</span><span class="o">]}</span>,
<span class="w">               </span><span class="o">}</span>,
<span class="w">               </span><span class="s2">&quot;required&quot;</span>:<span class="w"> </span><span class="o">[</span><span class="s2">&quot;location&quot;</span><span class="o">]</span>,
<span class="w">            </span><span class="o">}</span>,
<span class="w">      </span><span class="o">}</span>,
<span class="w">   </span><span class="o">}</span>
<span class="o">]</span>

<span class="nv">payload</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>
<span class="w">   </span><span class="s2">&quot;model&quot;</span>:<span class="w"> </span><span class="s2">&quot;./dist/gorilla-openfunctions-v1-q4f16_1-MLC/&quot;</span>,
<span class="w">   </span><span class="s2">&quot;messages&quot;</span>:<span class="w"> </span><span class="o">[</span>
<span class="w">      </span><span class="o">{</span>
<span class="w">            </span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,
<span class="w">            </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;What is the current weather in Pittsburgh, PA in fahrenheit?&quot;</span>,
<span class="w">      </span><span class="o">}</span>
<span class="w">   </span><span class="o">]</span>,
<span class="w">   </span><span class="s2">&quot;stream&quot;</span>:<span class="w"> </span>False,
<span class="w">   </span><span class="s2">&quot;tools&quot;</span>:<span class="w"> </span>tools,
<span class="o">}</span>

<span class="nv">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>requests.post<span class="o">(</span><span class="s2">&quot;http://127.0.0.1:8080/v1/chat/completions&quot;</span>,<span class="w"> </span><span class="nv">json</span><span class="o">=</span>payload<span class="o">)</span>
print<span class="o">(</span>f<span class="s2">&quot;{r.json()[&#39;choices&#39;][0][&#39;message&#39;][&#39;tool_calls&#39;][0][&#39;function&#39;]}\n&quot;</span><span class="o">)</span>

<span class="c1"># Output: {&#39;name&#39;: &#39;get_current_weather&#39;, &#39;arguments&#39;: {&#39;location&#39;: &#39;Pittsburgh, PA&#39;, &#39;unit&#39;: &#39;fahrenheit&#39;}}</span>
</pre></div>
</div>
<hr class="docutils" />
<p>Function Calling with streaming is also supported. Below is an example on how to use function calling with streaming in Python.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>import<span class="w"> </span>requests
import<span class="w"> </span>json

<span class="nv">tools</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span>
<span class="w">   </span><span class="o">{</span>
<span class="w">      </span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;function&quot;</span>,
<span class="w">      </span><span class="s2">&quot;function&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">            </span><span class="s2">&quot;name&quot;</span>:<span class="w"> </span><span class="s2">&quot;get_current_weather&quot;</span>,
<span class="w">            </span><span class="s2">&quot;description&quot;</span>:<span class="w"> </span><span class="s2">&quot;Get the current weather in a given location&quot;</span>,
<span class="w">            </span><span class="s2">&quot;parameters&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">               </span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;object&quot;</span>,
<span class="w">               </span><span class="s2">&quot;properties&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">                  </span><span class="s2">&quot;location&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">                        </span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;string&quot;</span>,
<span class="w">                        </span><span class="s2">&quot;description&quot;</span>:<span class="w"> </span><span class="s2">&quot;The city and state, e.g. San Francisco, CA&quot;</span>,
<span class="w">                  </span><span class="o">}</span>,
<span class="w">                  </span><span class="s2">&quot;unit&quot;</span>:<span class="w"> </span><span class="o">{</span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;string&quot;</span>,<span class="w"> </span><span class="s2">&quot;enum&quot;</span>:<span class="w"> </span><span class="o">[</span><span class="s2">&quot;celsius&quot;</span>,<span class="w"> </span><span class="s2">&quot;fahrenheit&quot;</span><span class="o">]}</span>,
<span class="w">               </span><span class="o">}</span>,
<span class="w">               </span><span class="s2">&quot;required&quot;</span>:<span class="w"> </span><span class="o">[</span><span class="s2">&quot;location&quot;</span><span class="o">]</span>,
<span class="w">            </span><span class="o">}</span>,
<span class="w">      </span><span class="o">}</span>,
<span class="w">   </span><span class="o">}</span>
<span class="o">]</span>

<span class="nv">payload</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>
<span class="w">   </span><span class="s2">&quot;model&quot;</span>:<span class="w"> </span><span class="s2">&quot;./dist/gorilla-openfunctions-v1-q4f16_1-MLC/&quot;</span>,
<span class="w">   </span><span class="s2">&quot;messages&quot;</span>:<span class="w"> </span><span class="o">[</span>
<span class="w">      </span><span class="o">{</span>
<span class="w">            </span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,
<span class="w">            </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;What is the current weather in Pittsburgh, PA and Tokyo, JP in fahrenheit?&quot;</span>,
<span class="w">      </span><span class="o">}</span>
<span class="w">   </span><span class="o">]</span>,
<span class="w">   </span><span class="s2">&quot;stream&quot;</span>:<span class="w"> </span>True,
<span class="w">   </span><span class="s2">&quot;tools&quot;</span>:<span class="w"> </span>tools,
<span class="o">}</span>

with<span class="w"> </span>requests.post<span class="o">(</span><span class="s2">&quot;http://127.0.0.1:8080/v1/chat/completions&quot;</span>,<span class="w"> </span><span class="nv">json</span><span class="o">=</span>payload,<span class="w"> </span><span class="nv">stream</span><span class="o">=</span>True<span class="o">)</span><span class="w"> </span>as<span class="w"> </span>r:
<span class="w"> </span><span class="k">for</span><span class="w"> </span>chunk<span class="w"> </span><span class="k">in</span><span class="w"> </span>r.iter_content<span class="o">(</span><span class="nv">chunk_size</span><span class="o">=</span>None<span class="o">)</span>:
<span class="w">     </span><span class="nv">chunk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>chunk.decode<span class="o">(</span><span class="s2">&quot;utf-8&quot;</span><span class="o">)</span>
<span class="w">     </span><span class="k">if</span><span class="w"> </span><span class="s2">&quot;[DONE]&quot;</span><span class="w"> </span><span class="k">in</span><span class="w"> </span>chunk<span class="o">[</span><span class="m">6</span>:<span class="o">]</span>:
<span class="w">         </span><span class="k">break</span>
<span class="w">     </span><span class="nv">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>json.loads<span class="o">(</span>chunk<span class="o">[</span><span class="m">6</span>:<span class="o">])</span>
<span class="w">     </span><span class="nv">content</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>response<span class="o">[</span><span class="s2">&quot;choices&quot;</span><span class="o">][</span><span class="m">0</span><span class="o">][</span><span class="s2">&quot;delta&quot;</span><span class="o">]</span>.get<span class="o">(</span><span class="s2">&quot;content&quot;</span>,<span class="w"> </span><span class="s2">&quot;&quot;</span><span class="o">)</span>
<span class="w">     </span>print<span class="o">(</span>f<span class="s2">&quot;{content}&quot;</span>,<span class="w"> </span><span class="nv">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span>,<span class="w"> </span><span class="nv">flush</span><span class="o">=</span>True<span class="o">)</span>
print<span class="o">(</span><span class="s2">&quot;\n&quot;</span><span class="o">)</span>

<span class="c1"># Output: [&quot;get_current_weather(location=&#39;Pittsburgh,PA&#39;,unit=&#39;fahrenheit&#39;)&quot;, &quot;get_current_weather(location=&#39;Tokyo,JP&#39;,unit=&#39;fahrenheit&#39;)&quot;]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The API is a uniform interface that supports multiple languages. You can also utilize these functionalities in languages other than Python.</p>
</div>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="cli.html" class="btn btn-neutral float-right" title="CLI" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="webllm.html" class="btn btn-neutral float-left" title="WebLLM Javascript SDK" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023-2025 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
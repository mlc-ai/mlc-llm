





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Python API &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="iOS Swift SDK" href="ios.html" />
    <link rel="prev" title="CLI" href="cli.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/introduction.html">Introduction to MLC LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="webllm.html">WebLLM Javascript SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#verify-installation">Verify Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#run-mlcengine">Run MLCEngine</a></li>
<li class="toctree-l2"><a class="reference internal" href="#run-asyncmlcengine">Run AsyncMLCEngine</a></li>
<li class="toctree-l2"><a class="reference internal" href="#engine-mode">Engine Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deploy-your-own-model-with-python-api">Deploy Your Own Model with Python API</a></li>
<li class="toctree-l2"><a class="reference internal" href="#api-reference">API Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS Swift SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="ide_integration.html">IDE Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlc_chat_config.html">Customize MLC Chat Config</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/convert_weights.html">Convert Model Weights</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Model Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/package_libraries_and_weights.html">Package Libraries and Weights</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/define_new_models.html">Define New Model Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/configure_quantization.html">Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Microserving API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../microserving/tutorial.html">Implement LLM Cross-engine Orchestration Patterns</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Python API</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/python_engine.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="python-api">
<span id="deploy-python-engine"></span><h1>Python API<a class="headerlink" href="#python-api" title="Permalink to this heading">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This page introduces the Python API with MLCEngine in MLC LLM.</p>
</div>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#verify-installation" id="id6">Verify Installation</a></p></li>
<li><p><a class="reference internal" href="#run-mlcengine" id="id7">Run MLCEngine</a></p></li>
<li><p><a class="reference internal" href="#run-asyncmlcengine" id="id8">Run AsyncMLCEngine</a></p></li>
<li><p><a class="reference internal" href="#engine-mode" id="id9">Engine Mode</a></p></li>
<li><p><a class="reference internal" href="#deploy-your-own-model-with-python-api" id="id10">Deploy Your Own Model with Python API</a></p></li>
<li><p><a class="reference internal" href="#api-reference" id="id11">API Reference</a></p></li>
</ul>
</nav>
<p>MLC LLM provides Python API through classes <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.MLCEngine</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncMLCEngine</span></code>
which <strong>support full OpenAI API completeness</strong> for easy integration into other Python projects.</p>
<p>This page introduces how to use the engines in MLC LLM.
The Python API is a part of the MLC-LLM package, which we have prepared pre-built pip wheels via
the <a class="reference internal" href="../install/mlc_llm.html#install-mlc-packages"><span class="std std-ref">installation page</span></a>.</p>
<section id="verify-installation">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Verify Installation</a><a class="headerlink" href="#verify-installation" title="Permalink to this heading">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from mlc_llm import MLCEngine; print(MLCEngine)&quot;</span>
</pre></div>
</div>
<p>You are expected to see the output of <code class="docutils literal notranslate"><span class="pre">&lt;class</span> <span class="pre">'mlc_llm.serve.engine.MLCEngine'&gt;</span></code>.</p>
<p>If the command above results in error, follow <a class="reference internal" href="../install/mlc_llm.html#install-mlc-packages"><span class="std std-ref">Install MLC LLM Python Package</span></a> to install prebuilt pip
packages or build MLC LLM from source.</p>
</section>
<section id="run-mlcengine">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Run MLCEngine</a><a class="headerlink" href="#run-mlcengine" title="Permalink to this heading">¶</a></h2>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.MLCEngine</span></code> provides the interface of OpenAI chat completion synchronously.
<code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.MLCEngine</span></code> does not batch concurrent request due to the synchronous design,
and please use <a class="reference internal" href="#python-engine-async-llm-engine"><span class="std std-ref">AsyncMLCEngine</span></a> for request batching process.</p>
<p><strong>Stream Response.</strong> In <a class="reference internal" href="../get_started/quick_start.html#quick-start"><span class="std std-ref">Quick Start</span></a> and <a class="reference internal" href="../get_started/introduction.html#introduction-to-mlc-llm"><span class="std std-ref">Introduction to MLC LLM</span></a>,
we introduced the basic use of <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.MLCEngine</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">MLCEngine</span>

<span class="c1"># Create engine</span>
<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC&quot;</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">MLCEngine</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Run chat completion in OpenAI API.</span>
<span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">engine</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">}],</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">for</span> <span class="n">choice</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">choice</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">engine</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>
</pre></div>
</div>
<p>This code example first creates an <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.MLCEngine</span></code> instance with the 8B Llama-3 model.
<strong>We design the Python API</strong> <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.MLCEngine</span></code> <strong>to align with OpenAI API</strong>,
which means you can use <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.MLCEngine</span></code> in the same way of using
<a class="reference external" href="https://github.com/openai/openai-python?tab=readme-ov-file#usage">OpenAI’s Python package</a>
for both synchronous and asynchronous generation.</p>
<p><strong>Non-stream Response.</strong> The code example above uses the synchronous chat completion
interface and iterate over all the stream responses.
If you want to run without streaming, you can run</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">}],</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p>Please refer to <a class="reference external" href="https://github.com/openai/openai-python?tab=readme-ov-file#usage">OpenAI’s Python package</a>
and <a class="reference external" href="https://platform.openai.com/docs/api-reference/chat/create">OpenAI chat completion API</a>
for the complete chat completion interface.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to enable tensor parallelism to run LLMs on multiple GPUs,
please specify argument <code class="docutils literal notranslate"><span class="pre">model_config_overrides</span></code> in MLCEngine constructor.
For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">MLCEngine</span>
<span class="kn">from</span> <span class="nn">mlc_llm.serve.config</span> <span class="kn">import</span> <span class="n">EngineConfig</span>

<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC&quot;</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">MLCEngine</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">engine_config</span><span class="o">=</span><span class="n">EngineConfig</span><span class="p">(</span><span class="n">tensor_parallel_shards</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="run-asyncmlcengine">
<span id="python-engine-async-llm-engine"></span><h2><a class="toc-backref" href="#id8" role="doc-backlink">Run AsyncMLCEngine</a><a class="headerlink" href="#run-asyncmlcengine" title="Permalink to this heading">¶</a></h2>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncMLCEngine</span></code> provides the interface of OpenAI chat completion with
asynchronous features.
<strong>We recommend using</strong> <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncMLCEngine</span></code> <strong>to batch concurrent request for better throughput.</strong></p>
<p><strong>Stream Response.</strong> The core use of <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncMLCEngine</span></code> for stream responses is as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">async</span> <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="k">await</span> <span class="n">engine</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
  <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">}],</span>
  <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
  <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
  <span class="k">for</span> <span class="n">choice</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">choice</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<details class="summary-the-collapsed-is-a-complete-runnable-example-of-asyncmlcengine-in-python">
<summary>The collapsed is a complete runnable example of AsyncMLCEngine in Python.</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>

<span class="kn">from</span> <span class="nn">mlc_llm.serve</span> <span class="kn">import</span> <span class="n">AsyncMLCEngine</span>

<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC&quot;</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Write a three-day travel plan to Pittsburgh.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="k">async</span> <span class="k">def</span> <span class="nf">test_completion</span><span class="p">():</span>
    <span class="c1"># Create engine</span>
    <span class="n">async_engine</span> <span class="o">=</span> <span class="n">AsyncMLCEngine</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>

    <span class="n">num_requests</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
    <span class="n">output_texts</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">generate_task</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">async</span> <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="k">await</span> <span class="n">async_engine</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_texts</span><span class="p">:</span>
                <span class="n">output_texts</span><span class="p">[</span><span class="n">response</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="n">output_texts</span><span class="p">[</span><span class="n">response</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">+=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span>

    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="n">asyncio</span><span class="o">.</span><span class="n">create_task</span><span class="p">(</span><span class="n">generate_task</span><span class="p">(</span><span class="n">prompts</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_requests</span><span class="p">)]</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>

    <span class="c1"># Print output.</span>
    <span class="k">for</span> <span class="n">request_id</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">output_texts</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output of request </span><span class="si">{</span><span class="n">request_id</span><span class="si">}</span><span class="s2">:</span><span class="se">\n</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">async_engine</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>


<span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">test_completion</span><span class="p">())</span>
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>Non-stream Response.</strong> Similarly, <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncEngine</span></code> provides the non-stream response
interface.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">engine</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
  <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">}],</span>
  <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
  <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p>Please refer to <a class="reference external" href="https://github.com/openai/openai-python?tab=readme-ov-file#usage">OpenAI’s Python package</a>
and <a class="reference external" href="https://platform.openai.com/docs/api-reference/chat/create">OpenAI chat completion API</a>
for the complete chat completion interface.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to enable tensor parallelism to run LLMs on multiple GPUs,
please specify argument <code class="docutils literal notranslate"><span class="pre">model_config_overrides</span></code> in AsyncMLCEngine constructor.
For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">AsyncMLCEngine</span>
<span class="kn">from</span> <span class="nn">mlc_llm.serve.config</span> <span class="kn">import</span> <span class="n">EngineConfig</span>

<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC&quot;</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">AsyncMLCEngine</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">engine_config</span><span class="o">=</span><span class="n">EngineConfig</span><span class="p">(</span><span class="n">tensor_parallel_shards</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="engine-mode">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Engine Mode</a><a class="headerlink" href="#engine-mode" title="Permalink to this heading">¶</a></h2>
<p>To ease the engine configuration, the constructors of <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.MLCEngine</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncMLCEngine</span></code> have an optional argument <code class="docutils literal notranslate"><span class="pre">mode</span></code>,
which falls into one of the three options <code class="docutils literal notranslate"><span class="pre">&quot;local&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;interactive&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;server&quot;</span></code>.
The default mode is <code class="docutils literal notranslate"><span class="pre">&quot;local&quot;</span></code>.</p>
<p>Each mode denotes a pre-defined configuration of the engine to satisfy different use cases.
The choice of the mode controls the request concurrency of the engine,
as well as engine’s KV cache token capacity (or in other words, the maximum
number of tokens that the engine’s KV cache can hold),
and further affects the GPU memory usage of the engine.</p>
<p>In short,</p>
<ul class="simple">
<li><p>mode <code class="docutils literal notranslate"><span class="pre">&quot;local&quot;</span></code> uses low request concurrency and low KV cache capacity, which is suitable for cases where <strong>concurrent requests are not too many, and the user wants to save GPU memory usage</strong>.</p></li>
<li><p>mode <code class="docutils literal notranslate"><span class="pre">&quot;interactive&quot;</span></code> uses 1 as the request concurrency and low KV cache capacity, which is designed for <strong>interactive use cases</strong> such as chats and conversations.</p></li>
<li><p>mode <code class="docutils literal notranslate"><span class="pre">&quot;server&quot;</span></code> uses as much request concurrency and KV cache capacity as possible. This mode aims to <strong>fully utilize the GPU memory for large server scenarios</strong> where concurrent requests may be many.</p></li>
</ul>
<p><strong>For system benchmark, please select mode</strong> <code class="docutils literal notranslate"><span class="pre">&quot;server&quot;</span></code>.
Please refer to <a class="reference internal" href="#python-engine-api-reference"><span class="std std-ref">API Reference</span></a> for detailed documentation of the engine mode.</p>
</section>
<section id="deploy-your-own-model-with-python-api">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Deploy Your Own Model with Python API</a><a class="headerlink" href="#deploy-your-own-model-with-python-api" title="Permalink to this heading">¶</a></h2>
<p>The <a class="reference internal" href="../get_started/introduction.html#introduction-deploy-your-own-model"><span class="std std-ref">introduction page</span></a> introduces how we can deploy our
own models with MLC LLM.
This section introduces how you can use the model weights you convert and the model library you build
in <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.MLCEngine</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncMLCEngine</span></code>.</p>
<p>We use the <a class="reference external" href="https://huggingface.co/microsoft/phi-2">Phi-2</a> as the example model.</p>
<p><strong>Specify Model Weight Path.</strong> Assume you have converted the model weights for your own model,
you can construct a <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.MLCEngine</span></code> as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">MLCEngine</span>

<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;models/phi-2&quot;</span>  <span class="c1"># Assuming the converted phi-2 model weights are under &quot;models/phi-2&quot;</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">MLCEngine</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Specify Model Library Path.</strong> Further, if you build the model library on your own,
you can use it in <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.MLCEngine</span></code> by passing the library path through argument <code class="docutils literal notranslate"><span class="pre">model_lib</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">MLCEngine</span>

<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;models/phi-2&quot;</span>
<span class="n">model_lib</span> <span class="o">=</span> <span class="s2">&quot;models/phi-2/lib.so&quot;</span>  <span class="c1"># Assuming the phi-2 model library is built at &quot;models/phi-2/lib.so&quot;</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">MLCEngine</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model_lib</span><span class="o">=</span><span class="n">model_lib</span><span class="p">)</span>
</pre></div>
</div>
<p>The same applies to <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncMLCEngine</span></code>.</p>
</section>
<section id="api-reference">
<span id="python-engine-api-reference"></span><h2><a class="toc-backref" href="#id11" role="doc-backlink">API Reference</a><a class="headerlink" href="#api-reference" title="Permalink to this heading">¶</a></h2>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.MLCEngine</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncMLCEngine</span></code> classes provide the following constructors.</p>
<p>The MLCEngine and AsyncMLCEngine have full OpenAI API completeness.
Please refer to <a class="reference external" href="https://github.com/openai/openai-python?tab=readme-ov-file#usage">OpenAI’s Python package</a>
and <a class="reference external" href="https://platform.openai.com/docs/api-reference/chat/create">OpenAI chat completion API</a>
for the complete chat completion interface.</p>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ios.html" class="btn btn-neutral float-right" title="iOS Swift SDK" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cli.html" class="btn btn-neutral float-left" title="CLI" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023-2025 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
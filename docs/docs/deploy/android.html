





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Android App &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Compile Models via MLC" href="../compilation/compile_models.html" />
    <link rel="prev" title="iOS App and Swift API" href="ios.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/project_overview.html">Project Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/mlc_chat_config.html">Configure MLCChat in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="javascript.html">WebLLM and Javascript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">Rest API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI and C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="python.html">Python API and Gradio Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Android App</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#demo-app">Demo App</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prerequisite">Prerequisite</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compile-pytorch-models-from-huggingface">Compile PyTorch Models from HuggingFace</a></li>
<li class="toctree-l2"><a class="reference internal" href="#create-android-project-using-compiled-models">Create Android Project using Compiled Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#incorporate-model-weights">Incorporate Model Weights</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Models via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/distribute_compiled_models.html">Distribute Compiled Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/python.html">Python API for Model Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/configure_quantization.html">üöß Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/customize/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Android App</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/android.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="android-app">
<span id="deploy-android"></span><h1>Android App<a class="headerlink" href="#android-app" title="Permalink to this heading">¬∂</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#demo-app" id="id5">Demo App</a></p></li>
<li><p><a class="reference internal" href="#prerequisite" id="id6">Prerequisite</a></p></li>
<li><p><a class="reference internal" href="#compile-pytorch-models-from-huggingface" id="id7">Compile PyTorch Models from HuggingFace</a></p></li>
<li><p><a class="reference internal" href="#create-android-project-using-compiled-models" id="id8">Create Android Project using Compiled Models</a></p></li>
<li><p><a class="reference internal" href="#incorporate-model-weights" id="id9">Incorporate Model Weights</a></p></li>
</ul>
</nav>
<section id="demo-app">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Demo App</a><a class="headerlink" href="#demo-app" title="Permalink to this heading">¬∂</a></h2>
<p>The demo APK below is built for Samsung S23 with Snapdragon 8 Gen 2 chip.</p>
<a class="reference external image-reference" href="https://github.com/mlc-ai/binary-mlc-llm-libs/raw/main/mlc-chat.apk"><img alt="https://seeklogo.com/images/D/download-android-apk-badge-logo-D074C6882B-seeklogo.com.png" src="https://seeklogo.com/images/D/download-android-apk-badge-logo-D074C6882B-seeklogo.com.png" style="width: 135px;" /></a>
</section>
<section id="prerequisite">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Prerequisite</a><a class="headerlink" href="#prerequisite" title="Permalink to this heading">¬∂</a></h2>
<p><strong>Rust</strong> (<a class="reference external" href="https://www.rust-lang.org/tools/install">install</a>) is needed to cross-compile HuggingFace tokenizers to Android. Make sure rustc, cargo, and rustup are available in <code class="docutils literal notranslate"><span class="pre">$PATH</span></code>.</p>
<p><strong>Android Studio</strong> (<a class="reference external" href="https://developer.android.com/studio">install</a>) with NDK and CMake. To install NDK and CMake, in the Android Studio welcome page, click ‚ÄúProjects ‚Üí SDK Manager ‚Üí SDK Tools‚Äù. Set up the following environment variables:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ANDROID_NDK</span></code> so that <code class="docutils literal notranslate"><span class="pre">$ANDROID_NDK/build/cmake/android.toolchain.cmake</span></code> is available.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TVM_NDK_CC</span></code> that points to NDK‚Äôs clang compiler.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example on macOS</span>
ANDROID_NDK:<span class="w"> </span><span class="nv">$HOME</span>/Library/Android/sdk/ndk/25.2.9519653
TVM_NDK_CC:<span class="w"> </span><span class="nv">$ANDROID_NDK</span>/toolchains/llvm/prebuilt/darwin-x86_64/bin/aarch64-linux-android24-clang
<span class="c1"># Example on Windows</span>
ANDROID_NDK:<span class="w"> </span><span class="nv">$HOME</span>/Library/Android/sdk/ndk/25.2.9519653
TVM_NDK_CC:<span class="w"> </span><span class="nv">$ANDROID_NDK</span>/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang
</pre></div>
</div>
<p><strong>JDK</strong>, such as OpenJDK &gt;= 17, to compile Java bindings of TVM Unity runtime. It could be installed via Homebrew on macOS, apt on Ubuntu or other package managers. Set up the following environment variable:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">JAVA_HOME</span></code> so that Java is available in <code class="docutils literal notranslate"><span class="pre">$JAVA_HOME/bin/java</span></code>.</p></li>
</ul>
<p><strong>TVM Unity runtime</strong> is placed under <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/tree/main/3rdparty">3rdparty/tvm</a> in MLC LLM, so there is no need to install anything extra. Set up the following environment variable:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">TVM_HOME</span></code> so that its headers are available under <code class="docutils literal notranslate"><span class="pre">$TVM_HOME/include/tvm/runtime</span></code>.</p></li>
</ul>
<p>(Optional) <strong>TVM Unity compiler</strong> Python package (<a class="reference internal" href="../install/tvm.html#tvm-unity-prebuilt-package"><span class="std std-ref">install</span></a> or <a class="reference internal" href="../install/tvm.html#tvm-unity-build-from-source"><span class="std std-ref">build from source</span></a>). It is <em>NOT</em> required if models are prebuilt, but to compile PyTorch models from HuggingFace in the following section, the compiler is a must-dependency.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>‚ùó Whenever using Python, it is highly recommended to use <strong>conda</strong> to manage an isolated Python environment to avoid missing dependencies, incompatible versions, and package conflicts.</p>
</div>
<p>Check if <strong>environment variable</strong> are properly set as the last check. One way to ensure this is to place them in <code class="docutils literal notranslate"><span class="pre">$HOME/.zshrc</span></code>, <code class="docutils literal notranslate"><span class="pre">$HOME/.bashrc</span></code> or environment management tools.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span><span class="w"> </span><span class="nv">$HOME</span>/.cargo/env<span class="w"> </span><span class="c1"># Rust</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ANDROID_NDK</span><span class="o">=</span>...<span class="w">  </span><span class="c1"># Android NDK toolchain</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TVM_NDK_CC</span><span class="o">=</span>...<span class="w">   </span><span class="c1"># Android NDK clang</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">JAVA_HOME</span><span class="o">=</span>...<span class="w">    </span><span class="c1"># Java</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TVM_HOME</span><span class="o">=</span>...<span class="w">     </span><span class="c1"># TVM Unity runtime</span>
</pre></div>
</div>
</section>
<section id="compile-pytorch-models-from-huggingface">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Compile PyTorch Models from HuggingFace</a><a class="headerlink" href="#compile-pytorch-models-from-huggingface" title="Permalink to this heading">¬∂</a></h2>
<p>To deploy models on Android with reasonable performance, one has to cross-compile to and fully utilize mobile GPUs using TVM Unity. MLC provides a few pre-compiled models, or one could compile the models on their own.</p>
<p><strong>Cloning MLC LLM from GitHub</strong>. Download MLC LLM via the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>--recursive<span class="w"> </span>https://github.com/mlc-ai/mlc-llm/
<span class="w">          </span>^^^^^^^^^^^
<span class="nb">cd</span><span class="w"> </span>./mlc-llm/
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>‚ùó The <code class="docutils literal notranslate"><span class="pre">--recursive</span></code> flag is necessary to download submodules like <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/tree/main/3rdparty">3rdparty/tvm</a>. If you see any file missing during compilation, please double check if git submodules are properly cloned.</p>
</div>
<p><strong>Download the PyTorch model</strong> using Git Large File Storage (LFS), and by default, under <code class="docutils literal notranslate"><span class="pre">./dist/models/</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">MODEL_NAME</span><span class="o">=</span>Llama-2-7b-chat-hf
<span class="nv">QUANTIZATION</span><span class="o">=</span>q4f16_1

git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/meta-llama/<span class="nv">$MODEL_NAME</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>./dist/models/
</pre></div>
</div>
<p><strong>Compile Android-capable models</strong>. Install TVM Unity compiler as a Python package, and then run the command below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Show help message</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--help
<span class="c1"># Compile a PyTorch model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--target<span class="w"> </span>android<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model<span class="w"> </span>./dist/models/<span class="nv">$MODEL_NAME</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--quantization<span class="w"> </span><span class="nv">$QUANTIZATION</span>
</pre></div>
</div>
<p>This generates the directory <code class="docutils literal notranslate"><span class="pre">./dist/$MODEL_NAME-$QUANTIZATION</span></code> which contains the necessary components to run the model, as explained below.</p>
<p><strong>Expected output format</strong>. By default models are placed under <code class="docutils literal notranslate"><span class="pre">./dist/${MODEL_NAME}-${QUANTIZATION}</span></code>, and the result consists of 3 major components:</p>
<ul class="simple">
<li><p>Runtime configuration: It configures conversation templates including system prompts, repetition repetition penalty, sampling including temperature and top-p probability, maximum sequence length, etc. It is usually named as <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> under <a href="#id1"><span class="problematic" id="id2">``</span></a>params/<a href="#id3"><span class="problematic" id="id4">``</span></a>alongside with tokenizer configurations.</p></li>
<li><p>Model lib: The compiled library that uses mobile GPU. It is usually named as <code class="docutils literal notranslate"><span class="pre">${MODEL_NAME}-${QUANTIZATION}-android.tar</span></code>, for example, <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_0-android.tar</span></code>.</p></li>
<li><p>Model weights: the model weights are sharded as <code class="docutils literal notranslate"><span class="pre">params_shard_*.bin</span></code> under <code class="docutils literal notranslate"><span class="pre">params/</span></code> and the metadata is stored in <code class="docutils literal notranslate"><span class="pre">ndarray-cache.json</span></code>.</p></li>
</ul>
</section>
<section id="create-android-project-using-compiled-models">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Create Android Project using Compiled Models</a><a class="headerlink" href="#create-android-project-using-compiled-models" title="Permalink to this heading">¬∂</a></h2>
<p>The source code for MLC LLM is available under <code class="docutils literal notranslate"><span class="pre">android/</span></code>, including scripts to build dependencies and the main app under <code class="docutils literal notranslate"><span class="pre">android/MLCChat/</span></code> that could be opened by Android studio. Enter the directory first:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>./android/
</pre></div>
</div>
<p><strong>Build necessary dependencies.</strong> Configure the list of models the app comes with using the JSON file below, which by default, is configured to use both Llama2-7B and RedPajama-3B:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>vim<span class="w"> </span>./MLCChat/app/src/main/assets/app-config.json
</pre></div>
</div>
<p>Then bundle the android library <code class="docutils literal notranslate"><span class="pre">${MODEL_NAME}-${QUANTIZATION}-android.tar</span></code> compiled from <code class="docutils literal notranslate"><span class="pre">mlc_llm.build</span></code> in the previous steps, with TVM Unity‚Äôs Java runtime by running the commands below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./prepare_libs.sh
</pre></div>
</div>
<p>which generates the two files below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>find<span class="w"> </span>./build/output<span class="w"> </span>-type<span class="w"> </span>f
./build/output/arm64-v8a/libtvm4j_runtime_packed.so
./build/output/tvm4j_core.jar
</pre></div>
</div>
<p>The model execution logic in mobile GPUs is incorporated into <code class="docutils literal notranslate"><span class="pre">libtvm4j_runtime_packed.so</span></code>, while <code class="docutils literal notranslate"><span class="pre">tvm4j_core.jar</span></code> is a lightweight (~60 kb) <a class="reference external" href="https://tvm.apache.org/docs/reference/api/javadoc/">Java binding</a> to it. Copy them to the right path to be found by the Android project:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cp<span class="w"> </span>-a<span class="w"> </span>./build/output/.<span class="w"> </span>./MLCChat/app/src/main/libs
</pre></div>
</div>
<p><strong>Build the Android app</strong>. Open folder <code class="docutils literal notranslate"><span class="pre">./android/MLCChat</span></code> as an Android Studio Project. Connect your Android device to your machine. In the menu bar of Android Studio, click ‚ÄúBuild ‚Üí Make Project‚Äù. Once the build is finished, click ‚ÄúRun ‚Üí Run ‚Äòapp‚Äô‚Äù and you will see the app launched on your phone.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>‚ùó This app cannot be run in an emulator and thus a physical phone is required, because MLC LLM needs an actual mobile GPU to meaningfully run at an accelerated speed.</p>
</div>
</section>
<section id="incorporate-model-weights">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Incorporate Model Weights</a><a class="headerlink" href="#incorporate-model-weights" title="Permalink to this heading">¬∂</a></h2>
<p>Instructions have been provided to build an Android App with MLC LLM in previous sections, but it requires run-time weight downloading from HuggingFace, as configured in <cite>app-config.json</cite> in previous steps under <cite>model_url</cite>. However, it could be desirable to bundle weights together into the app to avoid downloading over the network. In this section, we provide a simple ADB-based walkthrough that hopefully helps with further development.</p>
<p><strong>Generating APK</strong>. Enter Android Studio, and click ‚ÄúBuild ‚Üí Generate Signed Bundle/APK‚Äù to build an APK for release. If it is the first time you generate an APK, you will need to create a key according to <a class="reference external" href="https://developer.android.com/studio/publish/app-signing#generate-key">the official guide from Android</a>. This APK will be placed under <code class="docutils literal notranslate"><span class="pre">android/MLCChat/app/release/app-release.apk</span></code>.</p>
<p><strong>Install ADB and USB debugging</strong>. Enable ‚ÄúUSB debugging‚Äù in the developer mode in your phone settings. In SDK manager, install <a class="reference external" href="https://developer.android.com/studio/releases/platform-tools">Android SDK Platform-Tools</a>. Add the path to platform-tool path to the environment variable <code class="docutils literal notranslate"><span class="pre">PATH</span></code>. Run the following commands, and if ADB is installed correctly, your phone will appear as a device:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>adb<span class="w"> </span>devices
</pre></div>
</div>
<p><strong>Install the APK and weights to your phone</strong>. Run the commands below replacing <code class="docutils literal notranslate"><span class="pre">${MODEL_NAME}</span></code> and <code class="docutils literal notranslate"><span class="pre">${QUANTIZATION}</span></code> with the actual model name (e.g. Llama-2-7b-chat-hf) and quantization format (e.g. q4f16_1).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>adb<span class="w"> </span>install<span class="w"> </span>android/MLCChat/app/release/app-release.apk
adb<span class="w"> </span>push<span class="w"> </span>dist/<span class="si">${</span><span class="nv">MODEL_NAME</span><span class="si">}</span>-<span class="si">${</span><span class="nv">QUANTIZATION</span><span class="si">}</span>/params<span class="w"> </span>/data/local/tmp/<span class="si">${</span><span class="nv">MODEL_NAME</span><span class="si">}</span>-<span class="si">${</span><span class="nv">QUANTIZATION</span><span class="si">}</span>/
adb<span class="w"> </span>shell<span class="w"> </span><span class="s2">&quot;mkdir -p /storage/emulated/0/Android/data/ai.mlc.mlcchat/files/&quot;</span>
adb<span class="w"> </span>shell<span class="w"> </span><span class="s2">&quot;mv /data/local/tmp/</span><span class="si">${</span><span class="nv">MODEL_NAME</span><span class="si">}</span><span class="s2">-</span><span class="si">${</span><span class="nv">QUANTIZATION</span><span class="si">}</span><span class="s2"> /storage/emulated/0/Android/data/ai.mlc.mlcchat/files/&quot;</span>
</pre></div>
</div>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../compilation/compile_models.html" class="btn btn-neutral float-right" title="Compile Models via MLC" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="ios.html" class="btn btn-neutral float-left" title="iOS App and Swift API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">¬© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
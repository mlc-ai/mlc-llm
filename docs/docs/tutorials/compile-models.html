<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>How to Compile Models &mdash; MLC-LLM  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/tabs.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="How to Deploy Models" href="deploy-models.html" />
    <link rel="prev" title="Software Dependencies" href="../install/software-dependencies.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            MLC-LLM
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Navigation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../navigation.html">Navigation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/software-dependencies.html">Software Dependencies</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">How to Compile Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#dependencies">Dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prepare-model-weight">Prepare model weight</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#models-with-full-weight">Models with full weight</a></li>
<li class="toctree-l3"><a class="reference internal" href="#models-with-base-weight-and-delta-weight">Models with base weight and delta weight</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#run-build-script">Run build script</a></li>
<li class="toctree-l2"><a class="reference internal" href="#why-need-build">Why need build?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#troubleshooting-faq">Troubleshooting FAQ</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="deploy-models.html">How to Deploy Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="bring-your-own-models.html">How to Introduce a Model Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="customize.html">How to customize Model Compilation and Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contribute to MLC-LLM</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribute/community.html">MLC-LLM Community Guidelines</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Zoo</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model-zoo.html">Model Zoo</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../misc/faq.html">Frequently Asked Questions (FAQ)</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MLC-LLM</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">How to Compile Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/compile-models.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="how-to-compile-models">
<span id="id1"></span><h1>How to Compile Models<a class="headerlink" href="#how-to-compile-models" title="Permalink to this heading"></a></h1>
<p>In this tutorial, we will guide you on how to <strong>build</strong> LLM whose architectures are already supported by MLC LLM to different backends. Before diving into this tutorial, you should first finish the <a class="reference internal" href="../install/index.html#installation-and-setup"><span class="std std-ref">Install and Setup</span></a>. In the following content, we assume you have already installed them and will not cover the installation part. After finish building, you can checkout the tutorial <a class="reference external" href="http://127.0.0.1">deploy-build-applications</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>At this moment, MLC LLM officially supports two model architectures: <a class="reference external" href="https://github.com/facebookresearch/llama">LLaMA</a> and <a class="reference external" href="https://github.com/EleutherAI/gpt-neox">GPT-NeoX</a>.
To build models with other model architectures, please refer to the tutorial <a class="reference external" href="http://127.0.0.1">model-architecture-variant</a>.</p>
</div>
<p>This tutorial contains the following sections in order:</p>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#dependencies" id="id2">Dependencies</a></p></li>
<li><p><a class="reference internal" href="#prepare-model-weight" id="id3">Prepare model weight</a></p></li>
<li><p><a class="reference internal" href="#run-build-script" id="id4">Run build script</a></p></li>
<li><p><a class="reference internal" href="#why-need-build" id="id5">Why need build?</a></p></li>
<li><p><a class="reference internal" href="#troubleshooting-faq" id="id6">Troubleshooting FAQ</a></p></li>
</ul>
</nav>
<section id="dependencies">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Dependencies</a><a class="headerlink" href="#dependencies" title="Permalink to this heading"></a></h2>
<p><a class="reference external" href="https://discuss.tvm.apache.org/t/establish-tvm-unity-connection-a-technical-strategy/13344">TVM-Unity</a> is required to compile models, please follow the instructions in <a class="reference internal" href="../install/index.html#tvm-unity-install"><span class="std std-ref">Install TVM Unity</span></a> to install the
TVM-Unity package before proceeding with this tutorial.</p>
</section>
<section id="prepare-model-weight">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Prepare model weight</a><a class="headerlink" href="#prepare-model-weight" title="Permalink to this heading"></a></h2>
<p>This section briefly introduces how to prepare the weight of the model we want to build.</p>
<section id="models-with-full-weight">
<h3>Models with full weight<a class="headerlink" href="#models-with-full-weight" title="Permalink to this heading"></a></h3>
<p>For models whose full weight is directly available (e.g., <a class="reference external" href="https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1">RedPajama-v1-3B on Hugging Face</a>), we need to put the model to path <code class="docutils literal notranslate"><span class="pre">dist/models/MODEL_NAME</span></code> under the path of your MLC LLM.</p>
<ul>
<li><p>If the model weight is hosted on <a class="reference external" href="https://huggingface.co">Hugging Face</a>, we can download the model via git clone. For example, the commands below download the RedPajama-v1-3B weight to <code class="docutils literal notranslate"><span class="pre">dist/models/RedPajama-INCITE-Chat-3B-v1</span></code>.</p>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>dist/models/RedPajama-INCITE-Chat-3B-v1
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Depending on the model size and the Hugging Face repository, the git clone operation might take a while (for example, cloning the RedPajama-v1-3B model takes about 10 min on one of our dev workstation).</p>
</div>
</div></blockquote>
</li>
<li><p>If you have your own fine-tuned model, directly copy your model to <code class="docutils literal notranslate"><span class="pre">dist/models/MODEL_NAME</span></code>.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>An alternative approach (which saves disk space) is to use symbolic link to link your model to <code class="docutils literal notranslate"><span class="pre">dist/models/MODEL_NAME</span></code>:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ln<span class="w"> </span>-s<span class="w"> </span>path/to/your/model<span class="w"> </span>dist/models/MODEL_NAME
</pre></div>
</div>
</div>
</div></blockquote>
</li>
</ul>
</section>
<section id="models-with-base-weight-and-delta-weight">
<h3>Models with base weight and delta weight<a class="headerlink" href="#models-with-base-weight-and-delta-weight" title="Permalink to this heading"></a></h3>
<p>For models whose base weight and delta weight are available (e.g., Vicuna-v1-7B only releases the <a class="reference external" href="https://huggingface.co/lmsys/vicuna-7b-delta-v1.1">delta weight</a> on Hugging Face), we need to apply the delta weights to the base weight. You can refer to the instructions of <a class="reference external" href="https://github.com/lm-sys/FastChat#vicuna-weights">getting Vicuna model weight</a> as a reference. After getting the full weight of the model, copy or symbolic link the model to <code class="docutils literal notranslate"><span class="pre">dist/models/MODEL_NAME</span></code>.</p>
</section>
</section>
<section id="run-build-script">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Run build script</a><a class="headerlink" href="#run-build-script" title="Permalink to this heading"></a></h2>
<p>We can now run the build script <code class="docutils literal notranslate"><span class="pre">build.py</span></code> under the path of MLC LLM. The command is usually in the following pattern:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME_OR_PATH<span class="w"> </span>--target<span class="w"> </span>TARGET_NAME<span class="w"> </span>--quantization<span class="w"> </span>QUANTIZATION_NAME<span class="w"> </span><span class="o">[</span>--max-seq-len<span class="w"> </span>MAX_ALLOWED_SEQUENCE_LENGTH<span class="o">]</span><span class="w"> </span><span class="o">[</span>--debug-dump<span class="o">]</span><span class="w"> </span><span class="o">[</span>--use-cache<span class="o">=</span><span class="m">0</span><span class="o">]</span>
</pre></div>
</div>
<p>In the command above, <code class="docutils literal notranslate"><span class="pre">--model</span></code> specifies the name of the model to build, <code class="docutils literal notranslate"><span class="pre">--target</span></code> specifies the backend we build the model to, <code class="docutils literal notranslate"><span class="pre">--quantization</span></code> specifies the quantization mode we use for build. You can find the build command according to the combination of different models and targets. For full explanation and usage of each argument, please check out the <a class="reference external" href="http://127.0.0.1">API reference</a>.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Model: vicuna-v1-7b</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">RedPajama-v1-3B</button><button aria-controls="panel-0-0-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-2" name="0-2" role="tab" tabindex="-1">Other models</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-1-1-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-1-1-0" name="1-0" role="tab" tabindex="0">Target: CUDA</button><button aria-controls="panel-1-1-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-1" name="1-1" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-1-1-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-2" name="1-2" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-1-1-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-3" name="1-3" role="tab" tabindex="-1">WebGPU</button><button aria-controls="panel-1-1-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-4" name="1-4" role="tab" tabindex="-1">iPhone/iPad</button><button aria-controls="panel-1-1-5" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-5" name="1-5" role="tab" tabindex="-1">Android</button></div><div aria-labelledby="tab-1-1-0" class="sphinx-tabs-panel" id="panel-1-1-0" name="1-0" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>cuda<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
</div><div aria-labelledby="tab-1-1-1" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-1" name="1-1" role="tabpanel" tabindex="0"><p>On Apple Silicon powered Mac, build for Apple Silicon Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>metal<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
<p>On Apple Silicon powered Mac, build for x86 Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>metal_x86_64<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
</div><div aria-labelledby="tab-1-1-2" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-2" name="1-2" role="tabpanel" tabindex="0"><p>On Linux, build for Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
<p>On Linux, build for Windows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0<span class="w"> </span>--llvm-mingw<span class="w"> </span>path/to/llvm-mingw
</pre></div>
</div>
</div><div aria-labelledby="tab-1-1-3" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-3" name="1-3" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>llvm<span class="w"> </span>--quantization<span class="w"> </span>q4f32_0
</pre></div>
</div>
</div><div aria-labelledby="tab-1-1-4" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-4" name="1-4" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>iphone<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
</div><div aria-labelledby="tab-1-1-5" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-5" name="1-5" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>android<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q4f16_0
</pre></div>
</div>
</div></div>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-2-2-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-2-2-0" name="2-0" role="tab" tabindex="0">Target: CUDA</button><button aria-controls="panel-2-2-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-1" name="2-1" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-2-2-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-2" name="2-2" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-2-2-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-3" name="2-3" role="tab" tabindex="-1">WebGPU</button><button aria-controls="panel-2-2-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-4" name="2-4" role="tab" tabindex="-1">iPhone/iPad</button><button aria-controls="panel-2-2-5" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-5" name="2-5" role="tab" tabindex="-1">Android</button></div><div aria-labelledby="tab-2-2-0" class="sphinx-tabs-panel" id="panel-2-2-0" name="2-0" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>cuda<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
</div><div aria-labelledby="tab-2-2-1" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-1" name="2-1" role="tabpanel" tabindex="0"><p>On Apple Silicon powered Mac, build for Apple Silicon Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>metal<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
<p>On Apple Silicon powered Mac, build for x86 Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>metal_x86_64<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
</div><div aria-labelledby="tab-2-2-2" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-2" name="2-2" role="tabpanel" tabindex="0"><p>On Linux, build for Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
<p>On Linux, build for Windows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0<span class="w"> </span>--llvm-mingw<span class="w"> </span>path/to/llvm-mingw
</pre></div>
</div>
</div><div aria-labelledby="tab-2-2-3" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-3" name="2-3" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>llvm<span class="w"> </span>--quantization<span class="w"> </span>q4f32_0
</pre></div>
</div>
</div><div aria-labelledby="tab-2-2-4" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-4" name="2-4" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>iphone<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
</div><div aria-labelledby="tab-2-2-5" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-5" name="2-5" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>android<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q4f16_0
</pre></div>
</div>
</div></div>
</div><div aria-labelledby="tab-0-0-2" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-2" name="0-2" role="tabpanel" tabindex="0"><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-3-3-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-3-3-0" name="3-0" role="tab" tabindex="0">Target: CUDA</button><button aria-controls="panel-3-3-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-1" name="3-1" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-3-3-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-2" name="3-2" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-3-3-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-3" name="3-3" role="tab" tabindex="-1">WebGPU</button><button aria-controls="panel-3-3-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-4" name="3-4" role="tab" tabindex="-1">iPhone/iPad</button><button aria-controls="panel-3-3-5" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-5" name="3-5" role="tab" tabindex="-1">Android</button></div><div aria-labelledby="tab-3-3-0" class="sphinx-tabs-panel" id="panel-3-3-0" name="3-0" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>cuda<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-1" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-1" name="3-1" role="tabpanel" tabindex="0"><p>On Apple Silicon powered Mac, build for Apple Silicon Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>metal<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
<p>On Apple Silicon powered Mac, build for x86 Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>metal_x86_64<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-2" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-2" name="3-2" role="tabpanel" tabindex="0"><p>On Linux, build for Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
<p>On Linux, build for Windows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q3f16_0<span class="w"> </span>--llvm-mingw<span class="w"> </span>path/to/llvm-mingw
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-3" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-3" name="3-3" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>llvm<span class="w"> </span>--quantization<span class="w"> </span>q4f32_0
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-4" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-4" name="3-4" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>iphone<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q3f16_0
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-5" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-5" name="3-5" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>build.py<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>android<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q4f16_0
</pre></div>
</div>
</div></div>
</div></div>
<p>Here are some notes on the build commands above:</p>
<ul class="simple">
<li><p>For each model and each backend, we only provide the most recommended build command (which is the most optimized). You can also try with different argument values (e.g., different quantization modes), whose build results do not run as fast and robustly as the provided one in deployment.</p></li>
<li><p>After a successful build, the build script outputs some cache files for quicker future builds. If you want to ignore the cached files and want to build from the very beginning, please append <code class="docutils literal notranslate"><span class="pre">--use-cache=0</span></code> to the end of the build command.</p></li>
<li><p>You can add <code class="docutils literal notranslate"><span class="pre">--debug-dump</span></code> to the build command to  optionally specifies if we will write some dump files for debugging.</p></li>
</ul>
<p>After running the build script successfully, you can proceed to the next tutorial on <a class="reference external" href="http:127.0.0.1">how to deploy models to different backends</a>.</p>
</section>
<section id="why-need-build">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Why need build?</a><a class="headerlink" href="#why-need-build" title="Permalink to this heading"></a></h2>
<p>As supplementary, this section explains what the <strong>build</strong> means in MLC LLM. Compared with PyTorch that runs every model in <em>eager mode</em>, the overall workflow of MLC LLM separates model execution into two major stages: <strong>build</strong> and <strong>deployment</strong>.
This separation enables us to build LLM to different backends using a single common flow and also supports us to optimize the LLM execution towards better runtime performance (less run time).</p>
<ul class="simple">
<li><p>In the build stage, MLC LLM takes the model, the target backend, and other configurable arguments as input, applies optimizations and transformations that accelerate the execution of the model on the target backend, and generates a set of output for the deployment stage. The set of output includes a binary library file for the model specific to the target backend, the quantized model weights, the tokenizer files specific to the model, and a config JSON file that contains some model basic information as well as the configurable parameters for deployment (such as the chat temperature). The output (and only the output) generated by the build stage will be consumed by the deployment stage.</p></li>
<li><p>The deployment stage runs on the target backend (e.g., web browser, mobile phones, etc.). It takes the output of the build stage as input and provides an interface for people to interact with the model we build. The interface can be a command line if the model is deployed to the native desktop/laptop environment or a chat box if the model is deployed to web browser and mobile phones.</p></li>
</ul>
<img alt="compilation workflow" class="align-center" src="https://mlc.ai/blog/img/redpajama/customization.svg" /></section>
<section id="troubleshooting-faq">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Troubleshooting FAQ</a><a class="headerlink" href="#troubleshooting-faq" title="Permalink to this heading"></a></h2>
<p>(draft)</p>
<details class="summary-q-i-encountered-the-unable-to-parse-tuningrecord-error-immediately-when-i-run-the-build-script">
<summary>Q: I encountered the ``Unable to parse TuningRecord`` error immediately when I run the build script.</summary><p>Please update your MLC LLM codebase to the latest by git.</p>
</details><details class="summary-q-i-encountered-error-when-building-the-moss-model">
<summary>Q: I encountered error when building the Moss model.</summary><p>Moss support is still ongoing and we are now working on it. Please try other models first.</p>
</details><ul class="simple">
<li><p>LLVM error (<a class="reference external" href="https://github.com/mlc-ai/mlc-llm/issues/182">https://github.com/mlc-ai/mlc-llm/issues/182</a>)</p></li>
<li><p>Windows unresolved external symbols (<a class="reference external" href="https://github.com/mlc-ai/mlc-llm/issues/194">https://github.com/mlc-ai/mlc-llm/issues/194</a>)</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../install/software-dependencies.html" class="btn btn-neutral float-left" title="Software Dependencies" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="deploy-models.html" class="btn btn-neutral float-right" title="How to Deploy Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, MLC-LLM community.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
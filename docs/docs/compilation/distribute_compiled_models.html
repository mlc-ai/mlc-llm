





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Distribute Compiled Models &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/tabs.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Python API for Model Compilation" href="python.html" />
    <link rel="prev" title="Compile Models via MLC" href="compile_models.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/project_overview.html">Project Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/mlc_chat_config.html">Configure MLCChat in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deploy/javascript.html">WebLLM and Javascript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/rest.html">Rest API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/cli.html">CLI and C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/python.html">Python API and Gradio Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/android.html">Android App</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="compile_models.html">Compile Models via MLC</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Distribute Compiled Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#step-1-check-the-build-artifact">Step 1. Check the Build Artifact</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-2-update-mlc-chat-configuration-json">Step 2. Update MLC Chat Configuration JSON</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-3-specify-the-model-lib">Step 3. Specify the Model Lib</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-4-upload-the-compiled-model-weights">Step 4. Upload the Compiled Model Weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="#download-the-distributed-models-and-run-in-cli">Download the Distributed Models and Run in CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="#download-the-distributed-models-and-run-in-ios-app">Download the Distributed Models and Run in iOS App</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="python.html">Python API for Model Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="configure_quantization.html">üöß Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/customize/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Distribute Compiled Models</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/compilation/distribute_compiled_models.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="distribute-compiled-models">
<span id="id1"></span><h1>Distribute Compiled Models<a class="headerlink" href="#distribute-compiled-models" title="Permalink to this heading">¬∂</a></h1>
<p>This page describes how to distribute the model you compiled so others can use the model in MLC chat runtime.
For demonstration purposes, we show how to compile the <a class="reference external" href="https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1">RedPajama-3B instruct model</a>
(which has different weights from the RedPajama chat model).</p>
<p>If you have not compiled the RedPajama-3B instruct model,
you can use the following command to compile it:</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-TWV0YWw=" aria-selected="true" class="sphinx-tabs-tab group-tab" id="tab-0-TWV0YWw=" name="TWV0YWw=" role="tab" tabindex="0">Metal</button><button aria-controls="panel-0-TGludXggLSBDVURB" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-TGludXggLSBDVURB" name="TGludXggLSBDVURB" role="tab" tabindex="-1">Linux - CUDA</button><button aria-controls="panel-0-VnVsa2Fu" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-VnVsa2Fu" name="VnVsa2Fu" role="tab" tabindex="-1">Vulkan</button></div><div aria-labelledby="tab-0-TWV0YWw=" class="sphinx-tabs-panel group-tab" id="panel-0-TWV0YWw=" name="TWV0YWw=" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="w"> </span>togethercomputer/RedPajama-INCITE-Instruct-3B-v1<span class="w"> </span>--target<span class="w"> </span>metal<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-0-TGludXggLSBDVURB" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-TGludXggLSBDVURB" name="TGludXggLSBDVURB" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="w"> </span>togethercomputer/RedPajama-INCITE-Instruct-3B-v1<span class="w"> </span>--target<span class="w"> </span>cuda<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-0-VnVsa2Fu" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-VnVsa2Fu" name="VnVsa2Fu" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="w"> </span>togethercomputer/RedPajama-INCITE-Instruct-3B-v1<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div></div>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#step-1-check-the-build-artifact" id="id2">Step 1. Check the Build Artifact</a></p></li>
<li><p><a class="reference internal" href="#step-2-update-mlc-chat-configuration-json" id="id3">Step 2. Update MLC Chat Configuration JSON</a></p></li>
<li><p><a class="reference internal" href="#step-3-specify-the-model-lib" id="id4">Step 3. Specify the Model Lib</a></p></li>
<li><p><a class="reference internal" href="#step-4-upload-the-compiled-model-weights" id="id5">Step 4. Upload the Compiled Model Weights</a></p></li>
<li><p><a class="reference internal" href="#download-the-distributed-models-and-run-in-cli" id="id6">Download the Distributed Models and Run in CLI</a></p></li>
<li><p><a class="reference internal" href="#download-the-distributed-models-and-run-in-ios-app" id="id7">Download the Distributed Models and Run in iOS App</a></p></li>
</ul>
</nav>
<section id="step-1-check-the-build-artifact">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Step 1. Check the Build Artifact</a><a class="headerlink" href="#step-1-check-the-build-artifact" title="Permalink to this heading">¬∂</a></h2>
<p>To begin with, we can check that we have the compilation artifact ready on the disk.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Instruct-3B-v1-q4f16_1
<span class="w">    </span>RedPajama-INCITE-Instruct-3B-v1-q4f16_1-metal.so<span class="w">  </span><span class="c1"># ===&gt; the model library</span>
<span class="w">    </span>mod_cache_before_build_metal.pkl<span class="w">                  </span><span class="c1"># ===&gt; a cached file for future builds</span>
<span class="w">    </span>params<span class="w">                                            </span><span class="c1"># ===&gt; containing the model weights, tokenizer and chat config</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Instruct-3B-v1-q4f16_1/params
<span class="w">    </span>mlc-chat-config.json<span class="w">                              </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">    </span>ndarray-cache.json<span class="w">                                </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">    </span>params_shard_0.bin<span class="w">                                </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">    </span>params_shard_1.bin
<span class="w">    </span>...
<span class="w">    </span>tokenizer.json<span class="w">                                    </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">    </span>tokenizer_config.json
</pre></div>
</div>
<p>You are expected to see the same folder structure for the model you compiled.</p>
</section>
<section id="step-2-update-mlc-chat-configuration-json">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Step 2. Update MLC Chat Configuration JSON</a><a class="headerlink" href="#step-2-update-mlc-chat-configuration-json" title="Permalink to this heading">¬∂</a></h2>
<p>You can <strong>optionally</strong> customize the chat config file
<code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Instruct-3B-v1-q4f16_1/params/mlc-chat-config.json</span></code> (checkout <a class="reference internal" href="../get_started/mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Configure MLCChat in JSON</span></a> for more detailed instructions).
You can also simply use the default configuration and skip this step.</p>
<p>For demonstration purpose, we update <code class="docutils literal notranslate"><span class="pre">mean_gen_len</span></code> to 32 and <code class="docutils literal notranslate"><span class="pre">max_gen_len</span></code> to 64.
We also update <code class="docutils literal notranslate"><span class="pre">conv_template</span></code> to <code class="docutils literal notranslate"><span class="pre">&quot;LM&quot;</span></code> because the model is instruction-tuned.</p>
</section>
<section id="step-3-specify-the-model-lib">
<span id="distribute-model-step3-specify-model-lib"></span><h2><a class="toc-backref" href="#id4" role="doc-backlink">Step 3. Specify the Model Lib</a><a class="headerlink" href="#step-3-specify-the-model-lib" title="Permalink to this heading">¬∂</a></h2>
<p>An MLC chat app needs to look for the model library to run the model.
In the case of RedPajama-3B instruct model, we already have a prebuilt model lib for RedPajama-3B chat model that shares the
same model architecture and quantization mode as the instruct model.
We can edit <code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Instruct-3B-v1-q4f16_1/params/mlc-chat-config.json</span></code>
and update the value of field <code class="docutils literal notranslate"><span class="pre">model_lib</span></code> to <code class="docutils literal notranslate"><span class="pre">&quot;RedPajama-INCITE-Chat-3B-v1-q4f16_1&quot;</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We recommend reusing the model lib for the same architecture with different weight variants.
You can leverage the <code class="docutils literal notranslate"><span class="pre">--reuse-lib</span></code> in the compilation command to specify the library you want to reuse or edit the chat config afterward.
Reusing model lib allows us to run the model on existing MLC apps (e.g. iOS) that requires static packaging.</p>
<p>For example, if you have compiled RedPajama-3B chat model before, then you can use the following command to compile the instruct model,
which reuses the compiled chat model library:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="w"> </span>togethercomputer/RedPajama-INCITE-Instruct-3B-v1<span class="w"> </span>--reuse-lib<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1<span class="w"> </span>--target<span class="w"> </span><span class="o">[</span>your<span class="w"> </span>target<span class="o">]</span><span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
<p>In this way, <cite>mlc_llm.build</cite> does not produce the model library for the instruct model, and in <cite>mlc-chat-config.json</cite>
the <code class="docutils literal notranslate"><span class="pre">model_lib</span></code> field is set to <code class="docutils literal notranslate"><span class="pre">RedPajama-INCITE-Chat-3B-v1-q4f16_1</span></code>.</p>
<p>Please note that only models with same architecture and compiled with same quantization modes can reuse and share model library.</p>
</div>
<p>We should distribute the generated model lib if we want to build a new model architecture or try out customized compilation optimizations.
In this case, we should keep the <code class="docutils literal notranslate"><span class="pre">model_lib</span></code> field as <code class="docutils literal notranslate"><span class="pre">&quot;RedPajama-INCITE-Instruct-3B-v1-q4f16_1&quot;</span></code>.
You can upload the model library <code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Instruct-3B-v1-q4f16_1/RedPajama-INCITE-Instruct-3B-v1-q4f16_1-metal.so</span></code>
and ask others to download it to  <cite>dist/prebuilt/lib</cite> directory so the CLI app can pick it up.</p>
</section>
<section id="step-4-upload-the-compiled-model-weights">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Step 4. Upload the Compiled Model Weights</a><a class="headerlink" href="#step-4-upload-the-compiled-model-weights" title="Permalink to this heading">¬∂</a></h2>
<p>As a next step, we need to upload the model weights.
We only need to upload the files in <code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Instruct-3B-v1-q4f16_1/params</span></code>.
If you also want to host the compiled models on Hugging Face, you can follow the instructions below:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># First, please create a repository on Hugging Face.</span>
<span class="c1"># With the repository created, run</span>
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/my-huggingface-account/my-redpajama3b-weight-huggingface-repo
<span class="nb">cd</span><span class="w"> </span>my-redpajama3b-weight-huggingface-repo
cp<span class="w"> </span>path/to/mlc-llm/dist/RedPajama-INCITE-Instruct-3B-v1-q4f16_1/params/*<span class="w"> </span>.
git<span class="w"> </span>add<span class="w"> </span>.<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>git<span class="w"> </span>commit<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;Add redpajama-3b instruct model weights&quot;</span>
git<span class="w"> </span>push<span class="w"> </span>origin<span class="w"> </span>main
</pre></div>
</div>
<p>Here we provide an <a class="reference external" href="https://huggingface.co/mlc-ai/RedPajama-INCITE-Instruct-3B-v1-q4f16_1/tree/main">example distributed RedPajama-3B instruct model repository</a> which you can refer to.</p>
<hr class="docutils" />
<p>Good job, you have successfully distributed the model you compiled.
Next, we will talk about how we can consume the model weights in applications.</p>
</section>
<section id="download-the-distributed-models-and-run-in-cli">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Download the Distributed Models and Run in CLI</a><a class="headerlink" href="#download-the-distributed-models-and-run-in-cli" title="Permalink to this heading">¬∂</a></h2>
<p>The steps needed to run models in CLI are similar to the steps to download the prebuilt model weights and libraries.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Clone prebuilt libs so we can reuse them:</span>
mkdir<span class="w"> </span>-p<span class="w"> </span>dist/prebuilt
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mlc-ai/binary-mlc-llm-libs.git<span class="w"> </span>dist/prebuilt/lib

<span class="c1"># Or download the model library (only needed if we do not reuse the model lib):</span>
<span class="nb">cd</span><span class="w"> </span>dist/prebuilt/lib
wget<span class="w"> </span>url-to-my-model-lib
<span class="nb">cd</span><span class="w"> </span>../../..

<span class="c1"># Download the model weights</span>
<span class="nb">cd</span><span class="w"> </span>dist/prebuilt
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/my-huggingface-account/my-redpajama3b-weight-huggingface-repo<span class="w"> </span>RedPajama-INCITE-Instruct-3B-v1-q4f16_1
<span class="nb">cd</span><span class="w"> </span>../..
<span class="c1"># Run CLI</span>
mlc_chat_cli<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Instruct-3B-v1-q4f16_1
</pre></div>
</div>
</section>
<section id="download-the-distributed-models-and-run-in-ios-app">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Download the Distributed Models and Run in iOS App</a><a class="headerlink" href="#download-the-distributed-models-and-run-in-ios-app" title="Permalink to this heading">¬∂</a></h2>
<p>For iOS app, model libraries are statically packed into the app at the time of app building.
Therefore, the iOS app supports running any models whose model libraries are integrated into the app.
You can check the <a class="reference internal" href="../prebuilt_models.html#using-prebuilt-models-ios"><span class="std std-ref">list of supported model libraries</span></a>.</p>
<p>To download and run the compiled RedPajama-3B instruct model on iPhone, we need to reuse the integrated <code class="docutils literal notranslate"><span class="pre">RedPajama-INCITE-Chat-3B-v1-q4f16_1</span></code> model library.
Please revisit <a class="reference internal" href="#distribute-model-step3-specify-model-lib"><span class="std std-ref">Step 3. Specify the Model Lib</span></a> and make sure the <code class="docutils literal notranslate"><span class="pre">model_lib</span></code> field of <cite>mlc-chat-config.json</cite> is set to <code class="docutils literal notranslate"><span class="pre">RedPajama-INCITE-Chat-3B-v1-q4f16_1</span></code>.</p>
<p>Now we can download the model weights in iOS app and run the model by following the steps below:</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-1-1-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-1-1-0" name="1-0" role="tab" tabindex="0">Step 1</button><button aria-controls="panel-1-1-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-1" name="1-1" role="tab" tabindex="-1">Step 2</button><button aria-controls="panel-1-1-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-2" name="1-2" role="tab" tabindex="-1">Step 3</button><button aria-controls="panel-1-1-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-3" name="1-3" role="tab" tabindex="-1">Step 4</button></div><div aria-labelledby="tab-1-1-0" class="sphinx-tabs-panel" id="panel-1-1-0" name="1-0" role="tabpanel" tabindex="0"><p>Open ‚ÄúMLCChat‚Äù app, click ‚ÄúAdd model variant‚Äù.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-distribute-1.jpeg"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-distribute-1.jpeg" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-distribute-1.jpeg" style="width: 30%;" /></a>
</div><div aria-labelledby="tab-1-1-1" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-1" name="1-1" role="tabpanel" tabindex="0"><p>Paste the repository URL of the model built on your own, and click ‚ÄúAdd‚Äù.</p>
<p>You can refer to the link in the image as an example.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-distribute-2.jpeg"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-distribute-2.jpeg" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-distribute-2.jpeg" style="width: 30%;" /></a>
</div><div aria-labelledby="tab-1-1-2" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-2" name="1-2" role="tabpanel" tabindex="0"><p>After adding the model, you can download your model from the URL by clicking the download button.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-distribute-3.jpeg"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-distribute-3.jpeg" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-distribute-3.jpeg" style="width: 30%;" /></a>
</div><div aria-labelledby="tab-1-1-3" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-3" name="1-3" role="tabpanel" tabindex="0"><p>When the download is finished, click into the model and enjoy.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-distribute-4.jpeg"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-distribute-4.jpeg" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-distribute-4.jpeg" style="width: 30%;" /></a>
</div></div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="python.html" class="btn btn-neutral float-right" title="Python API for Model Compilation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="compile_models.html" class="btn btn-neutral float-left" title="Compile Models via MLC" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">¬© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>






<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>iOS and Swift SDK &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Android SDK" href="android.html" />
    <link rel="prev" title="Python API" href="python_engine.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/introduction.html">Introduction to MLC LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="javascript.html">WebLLM and JavaScript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_engine.html">Python API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">iOS and Swift SDK</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#use-pre-built-ios-app">Use Pre-built iOS App</a></li>
<li class="toctree-l2"><a class="reference internal" href="#build-ios-app-from-source">Build iOS App from Source</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-install-build-dependencies">Step 1. Install Build Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-build-runtime-and-model-libraries">Step 2. Build Runtime and Model Libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-optional-bundle-model-weights-into-the-app">Step 3. (Optional) Bundle model weights into the app</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-build-ios-app">Step 4. Build iOS App</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#customize-the-app">Customize the App</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bring-your-own-model">Bring Your Own Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#build-apps-with-mlc-swift-api">Build Apps with MLC Swift API</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="ide_integration.html">Code Completion IDE Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlc_chat_config.html">Customize MLC Config File in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/convert_weights.html">Convert Weights via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Model Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/package_model_libraries_weights.html">Package Model Libraries &amp; Weights</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Prebuilts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>iOS and Swift SDK</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/ios.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="ios-and-swift-sdk">
<span id="deploy-ios"></span><h1>iOS and Swift SDK<a class="headerlink" href="#ios-and-swift-sdk" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#use-pre-built-ios-app" id="id2">Use Pre-built iOS App</a></p></li>
<li><p><a class="reference internal" href="#build-ios-app-from-source" id="id3">Build iOS App from Source</a></p>
<ul>
<li><p><a class="reference internal" href="#step-1-install-build-dependencies" id="id4">Step 1. Install Build Dependencies</a></p></li>
<li><p><a class="reference internal" href="#step-2-build-runtime-and-model-libraries" id="id5">Step 2. Build Runtime and Model Libraries</a></p></li>
<li><p><a class="reference internal" href="#step-3-optional-bundle-model-weights-into-the-app" id="id6">Step 3. (Optional) Bundle model weights into the app</a></p></li>
<li><p><a class="reference internal" href="#step-4-build-ios-app" id="id7">Step 4. Build iOS App</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#customize-the-app" id="id8">Customize the App</a></p></li>
<li><p><a class="reference internal" href="#bring-your-own-model" id="id9">Bring Your Own Model</a></p></li>
<li><p><a class="reference internal" href="#build-apps-with-mlc-swift-api" id="id10">Build Apps with MLC Swift API</a></p></li>
</ul>
</nav>
<p>The MLC LLM iOS app can be installed in two ways: through the pre-built package or by building from the source.
If you are an iOS user looking to try out the models, the pre-built package is recommended. If you are a
developer seeking to integrate new features into the package, building the iOS package from the source is required.</p>
<section id="use-pre-built-ios-app">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Use Pre-built iOS App</a><a class="headerlink" href="#use-pre-built-ios-app" title="Permalink to this heading">¶</a></h2>
<p>The MLC Chat app is now available in App Store at no cost. You can download and explore it by simply clicking the button below:</p>
<blockquote>
<div><a class="reference external image-reference" href="https://apps.apple.com/us/app/mlc-chat/id6448482937"><img alt="https://developer.apple.com/assets/elements/badges/download-on-the-app-store.svg" src="https://developer.apple.com/assets/elements/badges/download-on-the-app-store.svg" width="135" /></a>
</div></blockquote>
</section>
<section id="build-ios-app-from-source">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Build iOS App from Source</a><a class="headerlink" href="#build-ios-app-from-source" title="Permalink to this heading">¶</a></h2>
<p>This section shows how we can build the app from the source.</p>
<section id="step-1-install-build-dependencies">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Step 1. Install Build Dependencies</a><a class="headerlink" href="#step-1-install-build-dependencies" title="Permalink to this heading">¶</a></h3>
<p>First and foremost, please clone the <a class="reference external" href="https://github.com/mlc-ai/mlc-llm">MLC LLM GitHub repository</a>.
After cloning, go to the <code class="docutils literal notranslate"><span class="pre">ios/</span></code> directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mlc-ai/mlc-llm.git
<span class="nb">cd</span><span class="w"> </span>mlc-llm
git<span class="w"> </span>submodule<span class="w"> </span>update<span class="w"> </span>--init<span class="w"> </span>--recursive
<span class="nb">cd</span><span class="w"> </span>./ios
</pre></div>
</div>
<p>Please follow <a class="reference internal" href="../install/tvm.html"><span class="doc">Install TVM Unity Compiler</span></a> to install TVM Unity.</p>
<p>We also need to have the following build dependencies:</p>
<ul class="simple">
<li><p>CMake &gt;= 3.24,</p></li>
<li><p>Git and Git-LFS,</p></li>
<li><p><a class="reference external" href="https://www.rust-lang.org/tools/install">Rust and Cargo</a>, which are required by Hugging Face’s tokenizer.</p></li>
</ul>
</section>
<section id="step-2-build-runtime-and-model-libraries">
<span id="ios-build-runtime-and-model-libraries"></span><h3><a class="toc-backref" href="#id5" role="doc-backlink">Step 2. Build Runtime and Model Libraries</a><a class="headerlink" href="#step-2-build-runtime-and-model-libraries" title="Permalink to this heading">¶</a></h3>
<p>The models to be built for the iOS app are specified in <code class="docutils literal notranslate"><span class="pre">MLCChat/mlc-package-config.json</span></code>:
in the <code class="docutils literal notranslate"><span class="pre">model_list</span></code>,</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code> points to the Hugging Face repository which contains the pre-converted model weights. The iOS app will download model weights from the Hugging Face URL.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_id</span></code> is a unique model identifier.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">estimated_vram_bytes</span></code> is an estimation of the vRAM the model takes at runtime.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;bundle_weight&quot;:</span> <span class="pre">true</span></code> means the model weights of the model will be bundled into the app when building.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">overrides</span></code> specifies some model config parameter overrides.</p></li>
</ul>
<p>We have a one-line command to build and prepare all the model libraries:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/path/to/MLCChat<span class="w">  </span><span class="c1"># e.g., &quot;ios/MLCChat&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_LLM_HOME</span><span class="o">=</span>/path/to/mlc-llm<span class="w">  </span><span class="c1"># e.g., &quot;../..&quot;</span>
mlc_llm<span class="w"> </span>package
</pre></div>
</div>
<p>This command mainly executes the following two steps:</p>
<ol class="arabic simple">
<li><p><strong>Compile models.</strong> We compile each model in <code class="docutils literal notranslate"><span class="pre">model_list</span></code> of <code class="docutils literal notranslate"><span class="pre">MLCChat/mlc-package-config.json</span></code> into a binary model library.</p></li>
<li><p><strong>Build runtime and tokenizer.</strong> In addition to the model itself, a lightweight runtime and tokenizer are required to actually run the LLM.</p></li>
</ol>
<p>The command creates a <code class="docutils literal notranslate"><span class="pre">./dist/</span></code> directory that contains the runtime and model build output.
Please make sure <code class="docutils literal notranslate"><span class="pre">dist/</span></code> follows the structure below, except the optional model weights.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>dist
├── bundle                   # The directory for mlc-app-config.json (and optionally model weights)
│   │                        # that will be bundled into the iOS app.
│   ├── mlc-app-config.json  # The app config JSON file.
│   └── [optional model weights]
└── lib
   ├── libmlc_llm.a          # A lightweight interface to interact with LLM, tokenizer, and TVM Unity runtime.
   ├── libmodel_iphone.a     # The compiled model lib.
   ├── libsentencepiece.a    # SentencePiece tokenizer
   ├── libtokenizers_cpp.a   # Huggingface tokenizer.
   └── libtvm_runtime.a      # TVM Unity runtime.
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We leverage a local JIT cache to avoid repetitive compilation of the same input.
However, sometimes it is helpful to force rebuild when we have a new compiler update
or when something goes wrong with the ached library.
You can do so by setting the environment variable <code class="docutils literal notranslate"><span class="pre">MLC_JIT_POLICY=REDO</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">MLC_JIT_POLICY</span><span class="o">=</span>REDO<span class="w"> </span>mlc_llm<span class="w"> </span>package
</pre></div>
</div>
</div>
</section>
<section id="step-3-optional-bundle-model-weights-into-the-app">
<span id="ios-bundle-model-weights"></span><h3><a class="toc-backref" href="#id6" role="doc-backlink">Step 3. (Optional) Bundle model weights into the app</a><a class="headerlink" href="#step-3-optional-bundle-model-weights-into-the-app" title="Permalink to this heading">¶</a></h3>
<p>By default, we download the model weights from Hugging Face when running the app.
<strong>As an option,</strong>, we bundle model weights into the app:
set the field <code class="docutils literal notranslate"><span class="pre">&quot;bundle_weight&quot;:</span> <span class="pre">true</span></code> for any model you want to bundle weights
in <code class="docutils literal notranslate"><span class="pre">MLCChat/mlc-package-config.json</span></code>, and run <code class="docutils literal notranslate"><span class="pre">mlc_llm</span> <span class="pre">package</span></code> again.
Below is an example:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;iphone&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;model_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;HF://mlc-ai/gemma-2b-it-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;model_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gemma-2b-q4f16_1&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;estimated_vram_bytes&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3000000000</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;overrides&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;prefill_chunk_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">128</span>
<span class="w">         </span><span class="p">},</span>
<span class="w">         </span><span class="nt">&quot;bundle_weight&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The outcome of running <code class="docutils literal notranslate"><span class="pre">mlc_llm</span> <span class="pre">package</span></code> should be as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>dist
├── bundle
│   ├── gemma-2b-q4f16_1   # The model weights that will be bundled into the app.
│   └── mlc-app-config.json
└── ...
</pre></div>
</div>
</section>
<section id="step-4-build-ios-app">
<span id="ios-build-app"></span><h3><a class="toc-backref" href="#id7" role="doc-backlink">Step 4. Build iOS App</a><a class="headerlink" href="#step-4-build-ios-app" title="Permalink to this heading">¶</a></h3>
<p>Open <code class="docutils literal notranslate"><span class="pre">./ios/MLCChat/MLCChat.xcodeproj</span></code> using Xcode. Note that you will need an
Apple Developer Account to use Xcode, and you may be prompted to use
your own developer team credential and product bundle identifier.</p>
<p>Ensure that all the necessary dependencies and configurations are
correctly set up in the Xcode project.</p>
<p>Once you have made the necessary changes, build the iOS app using Xcode.
If you have an Apple Silicon Mac, you can select target “My Mac (designed for iPad)”
to run on your Mac. You can also directly run it on your iPad or iPhone.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/xcode-build.jpg"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/xcode-build.jpg" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/xcode-build.jpg" style="width: 60%;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
<section id="customize-the-app">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Customize the App</a><a class="headerlink" href="#customize-the-app" title="Permalink to this heading">¶</a></h2>
<p>We can customize the models built in the iOS app by customizing <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/ios/MLCChat/mlc-package-config.json">MLCChat/mlc-package-config.json</a>.
We introduce each field of the JSON file here.</p>
<p>Each entry in <code class="docutils literal notranslate"><span class="pre">&quot;model_list&quot;</span></code> of the JSON file has the following fields:</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">model</span></code></dt><dd><p>(Required) The path to the MLC-converted model to be built into the app.</p>
<p>It can be either a Hugging Face URL (e.g., <code class="docutils literal notranslate"><span class="pre">&quot;model&quot;:</span> <span class="pre">&quot;HF://mlc-ai/phi-2-q4f16_1-MLC&quot;`</span></code>), or a path to a local model directory which contains converted model weights (e.g., <code class="docutils literal notranslate"><span class="pre">&quot;model&quot;:</span> <span class="pre">&quot;../dist/gemma-2b-q4f16_1&quot;</span></code>). Please check out <a class="reference internal" href="../compilation/convert_weights.html#convert-weights-via-mlc"><span class="std std-ref">Convert Weights via MLC</span></a> if you want to build local model into the app.</p>
<p><em>Note: the local path (if relative) is relative to the</em> <code class="docutils literal notranslate"><span class="pre">ios/</span></code> <em>directory.</em></p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">model_id</span></code></dt><dd><p>(Required) A unique local identifier to identify the model.
It can be an arbitrary one.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">estimated_vram_bytes</span></code></dt><dd><p>(Required) Estimated requirements of vRAM to run the model.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">bundle_weight</span></code></dt><dd><p>(Optional) A boolean flag indicating whether to bundle model weights into the app. See <a class="reference internal" href="#ios-bundle-model-weights"><span class="std std-ref">Step 3. (Optional) Bundle model weights into the app</span></a>.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">overrides</span></code></dt><dd><p>(Optional) A dictionary to override the default model context window size (to limit the KV cache size) and prefill chunk size (to limit the model temporary execution memory).
Example:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;iphone&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;model_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;HF://mlc-ai/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;model_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;RedPajama-INCITE-Chat-3B-v1-q4f16_1&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;estimated_vram_bytes&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2960000000</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;overrides&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">               </span><span class="nt">&quot;context_window_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">512</span><span class="p">,</span>
<span class="w">               </span><span class="nt">&quot;prefill_chunk_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">128</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">model_lib</span></code></dt><dd><p>(Optional) A string specifying the system library prefix to use for the model.
Usually this is used when you want to build multiple model variants with the same architecture into the app.
<strong>This field does not affect any app functionality.</strong>
The <code class="docutils literal notranslate"><span class="pre">&quot;model_lib_path_for_prepare_libs&quot;</span></code> introduced below is also related.
Example:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;iphone&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;model_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;HF://mlc-ai/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;model_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;RedPajama-INCITE-Chat-3B-v1-q4f16_1&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;estimated_vram_bytes&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2960000000</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;model_lib&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gpt_neox_q4f16_1&quot;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
</dl>
<p>Besides <code class="docutils literal notranslate"><span class="pre">model_list</span></code> in <code class="docutils literal notranslate"><span class="pre">MLCChat/mlc-package-config.json</span></code>,
you can also <strong>optionally</strong> specify a dictionary of <code class="docutils literal notranslate"><span class="pre">&quot;model_lib_path_for_prepare_libs&quot;</span></code>,
<strong>if you want to use model libraries that are manually compiled</strong>.
The keys of this dictionary should be the <code class="docutils literal notranslate"><span class="pre">model_lib</span></code> that specified in model list,
and the values of this dictionary are the paths (absolute, or relative) to the manually compiled model libraries.
The model libraries specified in <code class="docutils literal notranslate"><span class="pre">&quot;model_lib_path_for_prepare_libs&quot;</span></code> will be built into the app when running <code class="docutils literal notranslate"><span class="pre">mlc_llm</span> <span class="pre">package</span></code>.
Example:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;iphone&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;model_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;HF://mlc-ai/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;model_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;RedPajama-INCITE-Chat-3B-v1-q4f16_1&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;estimated_vram_bytes&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2960000000</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;model_lib&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gpt_neox_q4f16_1&quot;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">],</span>
<span class="w">   </span><span class="nt">&quot;model_lib_path_for_prepare_libs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;gpt_neox_q4f16_1&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;../../dist/lib/RedPajama-INCITE-Chat-3B-v1-q4f16_1-iphone.tar&quot;</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="bring-your-own-model">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Bring Your Own Model</a><a class="headerlink" href="#bring-your-own-model" title="Permalink to this heading">¶</a></h2>
<p>This section introduces how to build your own model into the iOS app.
We use the example of <a class="reference external" href="https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B">NeuralHermes</a> model, which a variant of Mistral model.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This section largely replicates <a class="reference internal" href="../compilation/convert_weights.html#convert-weights-via-mlc"><span class="std std-ref">Convert Weights via MLC</span></a>.
See that page for more details. Note that the weights are shared across
all platforms in MLC.</p>
</div>
<p><strong>Step 1. Clone from HF and convert_weight</strong></p>
<p>You can be under the mlc-llm repo, or your own working directory. Note that all platforms
can share the same compiled/quantized weights. See <a class="reference internal" href="../compilation/compile_models.html#compile-command-specification"><span class="std std-ref">Compile Command Specification</span></a>
for specification of <code class="docutils literal notranslate"><span class="pre">convert_weight</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create directory</span>
mkdir<span class="w"> </span>-p<span class="w"> </span>dist/models<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>dist/models
<span class="c1"># Clone HF weights</span>
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B
<span class="nb">cd</span><span class="w"> </span>../..
<span class="c1"># Convert weight</span>
mlc_llm<span class="w"> </span>convert_weight<span class="w"> </span>./dist/models/NeuralHermes-2.5-Mistral-7B/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC
</pre></div>
</div>
<p><strong>Step 2. Generate MLC Chat Config</strong></p>
<p>Use <code class="docutils literal notranslate"><span class="pre">mlc_llm</span> <span class="pre">gen_config</span></code> to generate <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> and process tokenizers.
See <a class="reference internal" href="../compilation/compile_models.html#compile-command-specification"><span class="std std-ref">Compile Command Specification</span></a> for specification of <code class="docutils literal notranslate"><span class="pre">gen_config</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/NeuralHermes-2.5-Mistral-7B/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q3f16_1<span class="w"> </span>--conv-template<span class="w"> </span>neural_hermes_mistral<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC
</pre></div>
</div>
<p>For the <code class="docutils literal notranslate"><span class="pre">conv-template</span></code>, <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/python/mlc_llm/conversation_template.py">conversation_template.py</a>
contains a full list of conversation templates that MLC provides.</p>
<p>If the model you are adding requires a new conversation template, you would need to add your own.
Follow <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/pull/2163">this PR</a> as an example.
We look up the template to use with the <code class="docutils literal notranslate"><span class="pre">conv_template</span></code> field in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p>
<p>For more details, please see <a class="reference internal" href="mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Customize MLC Config File in JSON</span></a>.</p>
<p><strong>Step 3. Upload weights to HF</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># First, please create a repository on Hugging Face.</span>
<span class="c1"># With the repository created, run</span>
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/my-huggingface-account/my-mistral-weight-huggingface-repo
<span class="nb">cd</span><span class="w"> </span>my-mistral-weight-huggingface-repo
cp<span class="w"> </span>path/to/mlc-llm/dist/NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC/*<span class="w"> </span>.
git<span class="w"> </span>add<span class="w"> </span>.<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>git<span class="w"> </span>commit<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;Add mistral model weights&quot;</span>
git<span class="w"> </span>push<span class="w"> </span>origin<span class="w"> </span>main
</pre></div>
</div>
<p>After successfully following all steps, you should end up with a Huggingface repo similar to
<a class="reference external" href="https://huggingface.co/mlc-ai/NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC">NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC</a>,
which includes the converted/quantized weights, the <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, and tokenizer files.</p>
<p><strong>Step 4. Register in Model List</strong></p>
<p>Finally, we add the model into the <code class="docutils literal notranslate"><span class="pre">model_list</span></code> of
<a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/ios/MLCChat/mlc-package-config.json">MLCChat/mlc-package-config.json</a> by specifying the Hugging Face link as <code class="docutils literal notranslate"><span class="pre">model</span></code>:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;iphone&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;model_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;HF://mlc-ai/NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;model_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Mistral-7B-Instruct-v0.2-q3f16_1&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;estimated_vram_bytes&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3316000000</span><span class="p">,</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Now, go through <a class="reference internal" href="#ios-build-runtime-and-model-libraries"><span class="std std-ref">Step 2. Build Runtime and Model Libraries</span></a> and <a class="reference internal" href="#ios-build-app"><span class="std std-ref">Step 4. Build iOS App</span></a> again.
The app will use the <code class="docutils literal notranslate"><span class="pre">NeuralHermes-Mistral</span></code> model you just added.</p>
</section>
<section id="build-apps-with-mlc-swift-api">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Build Apps with MLC Swift API</a><a class="headerlink" href="#build-apps-with-mlc-swift-api" title="Permalink to this heading">¶</a></h2>
<p>We also provide a Swift package that you can use to build
your own app. The package is located under <code class="docutils literal notranslate"><span class="pre">ios/MLCSwift</span></code>.</p>
<ul>
<li><p>First, create <code class="docutils literal notranslate"><span class="pre">mlc-package-config.json</span></code> in your project folder.
You do so by copying the files in MLCChat folder.
Run <code class="docutils literal notranslate"><span class="pre">mlc_llm</span> <span class="pre">package</span></code>.
This will give us the necessary libraries under <code class="docutils literal notranslate"><span class="pre">/path/to/project/dist</span></code>.</p></li>
<li><p>Under “Build phases”, add <code class="docutils literal notranslate"><span class="pre">/path/to/project/dist/bundle</span></code> this will copying
this folder into your app to include bundled weights and configs.</p></li>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">ios/MLCSwift</span></code> package to your app in Xcode.
Under “Frameworks, Libraries, and Embedded Content”, click add package dependencies
and add local package that points to <code class="docutils literal notranslate"><span class="pre">ios/MLCSwift</span></code>.</p></li>
<li><p>Finally, we need to add the libraries dependencies. Under build settings:</p>
<ul class="simple">
<li><p>Add library search path <code class="docutils literal notranslate"><span class="pre">/path/to/project/dist/lib</span></code>.</p></li>
<li><p>Add the following items to “other linker flags”.</p></li>
</ul>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="n">Wl</span><span class="p">,</span><span class="o">-</span><span class="n">all_load</span>
<span class="o">-</span><span class="n">lmodel_iphone</span>
<span class="o">-</span><span class="n">lmlc_llm</span> <span class="o">-</span><span class="n">ltvm_runtime</span>
<span class="o">-</span><span class="n">ltokenizers_cpp</span>
<span class="o">-</span><span class="n">lsentencepiece</span>
<span class="o">-</span><span class="n">ltokenizers_c</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
<p>You can then import the <cite>MLCSwift</cite> package into your app.
The following code shows an illustrative example of how to use the chat module.</p>
<div class="highlight-swift notranslate"><div class="highlight"><pre><span></span><span class="kd">import</span> <span class="nc">MLCSwift</span>

<span class="kd">let</span> <span class="nv">threadWorker</span> <span class="p">=</span> <span class="n">ThreadWorker</span><span class="p">()</span>
<span class="kd">let</span> <span class="nv">chat</span> <span class="p">=</span> <span class="n">ChatModule</span><span class="p">()</span>

<span class="n">threadWorker</span><span class="p">.</span><span class="n">push</span> <span class="p">{</span>
   <span class="kd">let</span> <span class="nv">modelLib</span> <span class="p">=</span> <span class="s">&quot;model-lib-name&quot;</span>
   <span class="kd">let</span> <span class="nv">modelPath</span> <span class="p">=</span> <span class="s">&quot;/path/to/model/weights&quot;</span>
   <span class="kd">let</span> <span class="nv">input</span> <span class="p">=</span> <span class="s">&quot;What is the capital of Canada?&quot;</span>
   <span class="n">chat</span><span class="p">.</span><span class="n">reload</span><span class="p">(</span><span class="n">modelLib</span><span class="p">,</span> <span class="n">modelPath</span><span class="p">:</span> <span class="n">modelPath</span><span class="p">)</span>

   <span class="n">chat</span><span class="p">.</span><span class="n">prefill</span><span class="p">(</span><span class="n">input</span><span class="p">)</span>
   <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">chat</span><span class="p">.</span><span class="n">stopped</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">displayReply</span><span class="p">(</span><span class="n">chat</span><span class="p">.</span><span class="n">getMessage</span><span class="p">())</span>
      <span class="n">chat</span><span class="p">.</span><span class="n">decode</span><span class="p">()</span>
   <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Because the chat module makes heavy use of GPU and thread-local
resources, it needs to run on a dedicated background thread.
Therefore, <strong>avoid using</strong> <cite>DispatchQueue</cite>, which can cause context switching to
different threads and segfaults due to thread-safety issues.
Use the <cite>ThreadWorker</cite> class to launch all the jobs related
to the chat module. You can check out the source code of
the MLCChat app for a complete example.</p>
</div>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="android.html" class="btn btn-neutral float-right" title="Android SDK" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="python_engine.html" class="btn btn-neutral float-left" title="Python API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>






<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Python API (Chat Module) &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/introduction.html">Introduction to MLC LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="javascript.html">WebLLM and JavaScript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_engine.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android App</a></li>
<li class="toctree-l1"><a class="reference internal" href="ide_integration.html">Code Completion IDE Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlc_chat_config.html">Customize MLC Config File in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/convert_weights.html">Convert Weights via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Model Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/define_new_models.html">Define New Model Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/configure_quantization.html">üöß Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Prebuilts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Python API (Chat Module)</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/python_chat_module.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="python-api-chat-module">
<span id="deploy-python-chat-module"></span><h1>Python API (Chat Module)<a class="headerlink" href="#python-api-chat-module" title="Permalink to this heading">¬∂</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>‚ùó The Python API with <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code> introduced in this page will be
deprecated in the near future.
Please go to <a class="reference internal" href="python_engine.html#deploy-python-engine"><span class="std std-ref">Python API</span></a> for the latest Python API with complete
OpenAI API support.</p>
</div>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#verify-installation" id="id1">Verify Installation</a></p></li>
<li><p><a class="reference internal" href="#run-mlc-models-w-python" id="id2">Run MLC Models w/ Python</a></p></li>
<li><p><a class="reference internal" href="#configure-mlcchat-in-python" id="id3">Configure MLCChat in Python</a></p></li>
<li><p><a class="reference internal" href="#raw-text-generation-in-python" id="id4">Raw Text Generation in Python</a></p></li>
<li><p><a class="reference internal" href="#stream-iterator-in-python" id="id5">Stream Iterator in Python</a></p></li>
<li><p><a class="reference internal" href="#api-reference" id="id6">API Reference</a></p></li>
</ul>
</nav>
<p>We expose ChatModule Python API for the MLC-LLM for easy integration into other Python projects.</p>
<p>The Python API is a part of the MLC-LLM package, which we have prepared pre-built pip wheels via
the <a class="reference internal" href="../install/mlc_llm.html"><span class="doc">installation page</span></a>.</p>
<p>Instead of following this page, you could also checkout the following tutorials in
Python notebook (all runnable in Colab):</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb">Getting Started with MLC-LLM</a>:
how to quickly download prebuilt models and chat with it</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_raw_text_generation.ipynb">Raw Text Generation with MLC-LLM</a>:
how to perform raw text generation with MLC-LLM in Python</p></li>
</ul>
<section id="verify-installation">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Verify Installation</a><a class="headerlink" href="#verify-installation" title="Permalink to this heading">¬∂</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from mlc_llm import ChatModule; print(ChatModule)&quot;</span>
</pre></div>
</div>
<p>You are expected to see the information about the <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code> class.</p>
<p>If the command above results in error, follow <a class="reference internal" href="../install/mlc_llm.html#install-mlc-packages"><span class="std std-ref">Install MLC LLM Python Package</span></a> (either install the prebuilt pip wheels
or <a class="reference internal" href="../install/mlc_llm.html#mlcchat-build-from-source"><span class="std std-ref">Option 2. Build from Source</span></a>).</p>
</section>
<section id="run-mlc-models-w-python">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Run MLC Models w/ Python</a><a class="headerlink" href="#run-mlc-models-w-python" title="Permalink to this heading">¬∂</a></h2>
<p>To run a model with MLC LLM in any platform/runtime, you need:</p>
<ol class="arabic simple">
<li><p><strong>Model weights</strong> converted to MLC format (e.g. <a class="reference external" href="https://huggingface.co/mlc-ai/RedPajama-INCITE-Chat-3B-v1-MLC/tree/main">RedPajama-INCITE-Chat-3B-v1-MLC</a>.)</p></li>
<li><p><strong>Model library</strong> that comprises the inference logic (see repo <a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs">binary-mlc-llm-libs</a>).</p></li>
</ol>
<p>There are two ways to obtain the model weights and libraries:</p>
<ol class="arabic simple">
<li><p>Compile your own model weights and libraries following <a class="reference internal" href="../compilation/compile_models.html"><span class="doc">the model compilation page</span></a>.</p></li>
<li><p>Use off-the-shelf <a class="reference external" href="https://huggingface.co/mlc-ai">prebuilt models weights</a> and
<a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs">prebuilt model libraries</a> (see <a class="reference internal" href="../prebuilt_models.html#model-prebuilts"><span class="std std-ref">Model Prebuilts</span></a> for details).</p></li>
</ol>
<p>We use off-the-shelf prebuilt models in this page. However, same steps apply if you want to run
the models you compiled yourself.</p>
<p><strong>Step 1: Download prebuilt model weights and libraries</strong></p>
<p>Skip this step if you have already obtained the model weights and libraries.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Activate your conda environment</span>
conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>git-lfs

<span class="c1"># Download pre-conveted weights</span>
git<span class="w"> </span>lfs<span class="w"> </span>install<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>mkdir<span class="w"> </span>dist/
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mlc-ai/Llama-2-7b-chat-hf-q4f16_1-MLC<span class="w"> </span><span class="se">\</span>
<span class="w">                                 </span>dist/Llama-2-7b-chat-hf-q4f16_1-MLC

<span class="c1"># Download pre-compiled model library</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mlc-ai/binary-mlc-llm-libs.git<span class="w"> </span>dist/prebuilt_libs
</pre></div>
</div>
<p><strong>Step 2: Run the model in Python</strong></p>
<p>Use the conda environment you used to install <code class="docutils literal notranslate"><span class="pre">mlc_llm</span></code>.
From the <code class="docutils literal notranslate"><span class="pre">mlc-llm</span></code> directory, you can create a Python
file <code class="docutils literal notranslate"><span class="pre">sample_mlc_llm.py</span></code> and paste the following lines:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_llm.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span>
   <span class="n">model</span><span class="o">=</span><span class="s2">&quot;dist/Llama-2-7b-chat-hf-q4f16_1-MLC&quot;</span><span class="p">,</span>
   <span class="n">model_lib_path</span><span class="o">=</span><span class="s2">&quot;dist/prebuilt_libs/Llama-2-7b-chat-hf/Llama-2-7b-chat-hf-q4f16_1-cuda.so&quot;</span>
   <span class="c1"># Vulkan on Linux: Llama-2-7b-chat-hf-q4f16_1-vulkan.so</span>
   <span class="c1"># Metal on macOS: Llama-2-7b-chat-hf-q4f16_1-metal.so</span>
   <span class="c1"># Other platforms: Llama-2-7b-chat-hf-q4f16_1-{backend}.{suffix}</span>
<span class="p">)</span>

<span class="c1"># You can change to other models that you downloaded</span>
<span class="c1"># Model variants of the same architecture can reuse the same model library</span>
<span class="c1"># Here WizardMath reuses Mistral&#39;s model library</span>
<span class="c1"># cm = ChatModule(</span>
<span class="c1">#     model=&quot;dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC&quot;,  # or &quot;dist/WizardMath-7B-V1.1-q4f16_1-MLC&quot;</span>
<span class="c1">#     model_lib_path=&quot;dist/prebuilt_libs/Mistral-7B-Instruct-v0.2/Mistral-7B-Instruct-v0.2-q4f16_1-cuda.so&quot;</span>
<span class="c1"># )</span>

<span class="c1"># Generate a response for a given prompt</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Print prefill and decode performance statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;How many points did you list out?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Reset the chat module by</span>
<span class="c1"># cm.reset_chat()</span>
</pre></div>
</div>
<p>Now run the Python file to start the chat</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>sample_mlc_llm.py
</pre></div>
</div>
<details class="summary-see-output">
<summary>See output</summary><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Using model folder: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
Using mlc chat config: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1/mlc-chat-config.json
Using library model: ./dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so

Thank you for your question! The meaning of life is a complex and subjective topic that has been debated by philosophers, theologians, scientists, and many others for centuries. There is no one definitive answer to this question, as it can vary depending on a person&#39;s beliefs, values, experiences, and perspectives.

However, here are some possible ways to approach the question:

1. Religious or spiritual beliefs: Many people believe that the meaning of life is to fulfill a divine or spiritual purpose, whether that be to follow a set of moral guidelines, to achieve spiritual enlightenment, or to fulfill a particular destiny.
2. Personal growth and development: Some people believe that the meaning of life is to learn, grow, and evolve as individuals, to develop one&#39;s talents and abilities, and to become the best version of oneself.
3. Relationships and connections: Others believe that the meaning of life is to form meaningful connections and relationships with others, to love and be loved, and to build a supportive and fulfilling social network.
4. Contribution and impact: Some people believe that the meaning of life is to make a positive impact on the world, to contribute to society in a meaningful way, and to leave a lasting legacy.
5. Simple pleasures and enjoyment: Finally, some people believe that the meaning of life is to simply enjoy the present moment, to find pleasure and happiness in the simple things in life, and to appreciate the beauty and wonder of the world around us.

Ultimately, the meaning of life is a deeply personal and subjective question, and each person must find their own answer based on their own beliefs, values, and experiences.

Statistics: prefill: 3477.5 tok/s, decode: 153.6 tok/s

I listed out 5 possible ways to approach the question of the meaning of life.
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>Running other models</strong></p>
<p>Checkout the <a class="reference internal" href="../prebuilt_models.html"><span class="doc">Model Prebuilts</span></a> page to run other pre-compiled models.</p>
<p>For models other than the prebuilt ones we provided:</p>
<ol class="arabic simple">
<li><p>If the model is a variant to an existing model library (e.g. <code class="docutils literal notranslate"><span class="pre">WizardMathV1.1</span></code> and <code class="docutils literal notranslate"><span class="pre">OpenHermes</span></code> are variants of <code class="docutils literal notranslate"><span class="pre">Mistral</span></code> as
shown in the code snippet), follow <a class="reference internal" href="../compilation/convert_weights.html#convert-weights-via-mlc"><span class="std std-ref">Convert Weights via MLC</span></a> to convert the weights and reuse existing model libraries.</p></li>
<li><p>Otherwise, follow <a class="reference internal" href="../compilation/compile_models.html#compile-model-libraries"><span class="std std-ref">Compile Model Libraries</span></a> to compile both the model library and weights.</p></li>
</ol>
</section>
<section id="configure-mlcchat-in-python">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Configure MLCChat in Python</a><a class="headerlink" href="#configure-mlcchat-in-python" title="Permalink to this heading">¬∂</a></h2>
<p>If you have checked out <a class="reference internal" href="mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Configure MLCChat in JSON</span></a>, you would know
that you could configure MLCChat through various fields such as <code class="docutils literal notranslate"><span class="pre">temperature</span></code>. We provide the
option of overriding any field you‚Äôd like in Python, so that you do not need to manually edit
<code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p>
<p>Since there are two concepts ‚Äì <cite>MLCChat Configuration</cite> and <cite>Conversation Configuration</cite> ‚Äì we correspondingly
provide two dataclasses <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatConfig</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ConvConfig</span></code>.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">ChatModule</span><span class="p">,</span> <span class="n">ChatConfig</span><span class="p">,</span> <span class="n">ConvConfig</span>
<span class="kn">from</span> <span class="nn">mlc_llm.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Using a `ConvConfig`, we modify `system`, a field in the conversation template</span>
<span class="c1"># `system` refers to the prompt encoded before starting the chat</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">system_message</span><span class="o">=</span><span class="s1">&#39;Please show as much happiness as you can when talking to me.&#39;</span><span class="p">)</span>

<span class="c1"># We then include the `ConvConfig` instance in `ChatConfig` while overriding `max_gen_len`</span>
<span class="c1"># Note that `conv_config` is an optional subfield of `chat_config`</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">max_gen_len</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">)</span>

<span class="c1"># Using the `chat_config` we created, instantiate a `ChatModule`</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span>
   <span class="n">chat_config</span><span class="o">=</span><span class="n">chat_config</span><span class="p">,</span>
   <span class="n">model</span><span class="o">=</span><span class="s2">&quot;dist/Llama-2-7b-chat-hf-q4f16_1-MLC&quot;</span><span class="p">,</span>
   <span class="n">model_lib_path</span><span class="o">=</span><span class="s2">&quot;dist/prebuilt_libs/Llama-2-7b-chat-hf/Llama-2-7b-chat-hf-q4f16_1-cuda.so&quot;</span>
   <span class="c1"># Vulkan on Linux: Llama-2-7b-chat-hf-q4f16_1-vulkan.so</span>
   <span class="c1"># Metal on macOS: Llama-2-7b-chat-hf-q4f16_1-metal.so</span>
   <span class="c1"># Other platforms: Llama-2-7b-chat-hf-q4f16_1-{backend}.{suffix}</span>
<span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is one plus one?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># You could also pass in a `ConvConfig` instance to `reset_chat()`</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">system</span><span class="o">=</span><span class="s1">&#39;Please show as much sadness as you can when talking to me.&#39;</span><span class="p">)</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">max_gen_len</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">)</span>
<span class="n">cm</span><span class="o">.</span><span class="n">reset_chat</span><span class="p">(</span><span class="n">chat_config</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is one plus one?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<details class="summary-see-output">
<summary>See output</summary><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Using model folder: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
Using mlc chat config: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1/mlc-chat-config.json
Using library model: ./dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so

Oh, wow, *excitedly* one plus one? *grinning* Well, let me see... *counting on fingers* One plus one is... *eureka* Two!
...

*Sobs* Oh, the tragedy of it all... *sobs* One plus one... *chokes back tears* It&#39;s... *gulps* it&#39;s... *breaks down in tears* TWO!
...
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You do not need to specify the entire <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> or <code class="docutils literal notranslate"><span class="pre">ConvConfig</span></code>. Instead, we will first
load all the fields defined in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, a file required when instantiating
a <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code>. Then, we will load in the optional <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> you provide, overriding the
fields specified.</p>
<p>It is also worth noting that <code class="docutils literal notranslate"><span class="pre">ConvConfig</span></code> itself is overriding the original conversation template
specified by the field <code class="docutils literal notranslate"><span class="pre">conv_template</span></code> in the chat configuration. Learn more about it in
<a class="reference internal" href="mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Configure MLCChat in JSON</span></a>.</p>
</div>
</section>
<section id="raw-text-generation-in-python">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Raw Text Generation in Python</a><a class="headerlink" href="#raw-text-generation-in-python" title="Permalink to this heading">¬∂</a></h2>
<p>Raw text generation allows the user to have more flexibility over his prompts,
without being forced to create a new conversational template, making prompt customization easier.
This serves other demands for APIs to handle LLM generation without the usual system prompts and other items.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">ChatModule</span><span class="p">,</span> <span class="n">ChatConfig</span><span class="p">,</span> <span class="n">ConvConfig</span>
<span class="kn">from</span> <span class="nn">mlc_llm.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Use a `ConvConfig` to define the generation settings</span>
<span class="c1"># Since the &quot;LM&quot; template only supports raw text generation,</span>
<span class="c1"># System prompts will not be executed even if provided</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">stop_tokens</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,],</span> <span class="n">add_bos</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stop_str</span><span class="o">=</span><span class="s2">&quot;[INST]&quot;</span><span class="p">)</span>

<span class="c1"># Note that `conv_config` is an optional subfield of `chat_config`</span>
<span class="c1"># The &quot;LM&quot; template serves the basic purposes of raw text generation</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">,</span> <span class="n">conv_template</span><span class="o">=</span><span class="s2">&quot;LM&quot;</span><span class="p">)</span>

<span class="c1"># Using the `chat_config` we created, instantiate a `ChatModule`</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span>
   <span class="n">chat_config</span><span class="o">=</span><span class="n">chat_config</span><span class="p">,</span>
   <span class="n">model</span><span class="o">=</span><span class="s2">&quot;dist/Llama-2-7b-chat-hf-q4f16_1-MLC&quot;</span><span class="p">,</span>
   <span class="n">model_lib_path</span><span class="o">=</span><span class="s2">&quot;dist/prebuilt_libs/Llama-2-7b-chat-hf/Llama-2-7b-chat-hf-q4f16_1-cuda.so&quot;</span>
   <span class="c1"># Vulkan on Linux: Llama-2-7b-chat-hf-q4f16_1-vulkan.so</span>
   <span class="c1"># Metal on macOS: Llama-2-7b-chat-hf-q4f16_1-metal.so</span>
   <span class="c1"># Other platforms: Llama-2-7b-chat-hf-q4f16_1-{backend}.{suffix}</span>
<span class="p">)</span>
<span class="c1"># To make the model follow conversations a chat structure should be provided</span>
<span class="c1"># This allows users to build their own prompts without building a new template</span>
<span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&lt;&lt;SYS&gt;&gt;</span><span class="se">\n</span><span class="s2">You are a helpful, respectful and honest assistant.</span><span class="se">\n</span><span class="s2">&lt;&lt;/SYS&gt;&gt;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
<span class="n">inst_prompt</span> <span class="o">=</span> <span class="s2">&quot;What is mother nature?&quot;</span>

<span class="c1"># Concatenate system and instruction prompts, and add instruction tags</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;[INST] </span><span class="si">{</span><span class="n">system_prompt</span><span class="o">+</span><span class="n">inst_prompt</span><span class="si">}</span><span class="s2"> [/INST]&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># The LM template has no memory, so it will be reset every single generation</span>
<span class="c1"># In this case the model will just follow normal text completion</span>
<span class="c1"># because there isn&#39;t a chat structure</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Life is a quality that distinguishes&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">LM</span></code> is a template without memory, which means that every execution will be cleared.
Additionally, system prompts will not be run when instantiating a <cite>mlc_llm.ChatModule</cite>,
unless explicitly given inside the prompt.</p>
</div>
</section>
<section id="stream-iterator-in-python">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Stream Iterator in Python</a><a class="headerlink" href="#stream-iterator-in-python" title="Permalink to this heading">¬∂</a></h2>
<p>Stream Iterator gives users an option to stream generated text to the function that the API is called from,
instead of streaming to stdout, which could be a necessity when building services on top of MLC Chat.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_llm.callback</span> <span class="kn">import</span> <span class="n">StreamIterator</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span>
   <span class="n">model</span><span class="o">=</span><span class="s2">&quot;dist/Llama-2-7b-chat-hf-q4f16_1-MLC&quot;</span><span class="p">,</span>
   <span class="n">model_lib_path</span><span class="o">=</span><span class="s2">&quot;dist/prebuilt_libs/Llama-2-7b-chat-hf/Llama-2-7b-chat-hf-q4f16_1-cuda.so&quot;</span>
   <span class="c1"># Vulkan on Linux: Llama-2-7b-chat-hf-q4f16_1-vulkan.so</span>
   <span class="c1"># Metal on macOS: Llama-2-7b-chat-hf-q4f16_1-metal.so</span>
   <span class="c1"># Other platforms: Llama-2-7b-chat-hf-q4f16_1-{backend}.{suffix}</span>
<span class="p">)</span>

<span class="c1"># Stream to an Iterator</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Thread</span>

<span class="n">stream</span> <span class="o">=</span> <span class="n">StreamIterator</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">generation_thread</span> <span class="o">=</span> <span class="n">Thread</span><span class="p">(</span>
   <span class="n">target</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">,</span>
   <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span> <span class="s2">&quot;progress_callback&quot;</span><span class="p">:</span> <span class="n">stream</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">generation_thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="n">output</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="k">for</span> <span class="n">delta_message</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
   <span class="n">output</span> <span class="o">+=</span> <span class="n">delta_message</span>

<span class="n">generation_thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="api-reference">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">API Reference</a><a class="headerlink" href="#api-reference" title="Permalink to this heading">¬∂</a></h2>
<p>User can initiate a chat module by creating <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code> class, which is a wrapper of the MLC-LLM model.
The <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code> class provides the following methods:</p>
</section>
</section>


           </div>
           
          </div>
          

<footer>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">¬© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
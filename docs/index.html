<!DOCTYPE html>
<html>
    <head>
        <title>MLC LLM | Home</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css"
              integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
              crossorigin="anonymous">
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="/mlc-llm/assets/css/main.css">
        <link rel="stylesheet" href="/mlc-llm/assets/css/group.css">
        <!-- <link rel="stylesheet" href="/mlc-llm/css/table.css">          -->
        <link rel="shortcut icon" href="/mlc-llm/assets/img/logo/mlc-favicon.png">
        
    </head>
    <body>
        <div class="container">
            <!-- This is a bit nasty, but it basically says be a column first, and on larger screens be a spaced out row -->
            <div class="header d-flex
                        flex-column
                        flex-md-row justify-content-md-between">
              <a href="/" id="navtitle">
                  <img src="/mlc-llm/assets/img/logo/mlc-logo-with-text-landscape.svg" height="70px"
                       alt="MLC" id="logo">
              </a>
              <ul id="topbar" class="nav nav-pills justify-content-center">
                    
                    

                        

                        
                        
                        

                      <li class="nav-item">
                         
                            <a class="nav-link active"
                               href="/mlc-llm/">
                                Home
                            </a>
                         
                        </li>

                    

                        

                        
                        
                        

                      <li class="nav-item">
                         
                            <a class="nav-link "
                               href="https://github.com/mlc-ai/mlc-llm">
                                Github
                            </a>
                         
                        </li>

                    

                </ul>
            </div>

            

            
            <!-- Schedule  -->
            
                <h1 id="mlc-llm">MLC LLM</h1>

<p>MLC LLM is a universal solution that allows any language model to be deployed natively on a diverse set of hardware backends and native applications, plus a productive framework for everyone to further optimize model performance for their own use cases. Check out our <a href="https://github.com/mlc-ai/mlc-llm">GitHub repository</a> to see how we did it. You can also read through instructions below for trying out demos.</p>

<p align="center">
<img src="demo.gif" height="700" />
</p>

<h2 id="try-it-out">Try it out</h2>

<p>This section contains the instructions to run large-language models and chatbot natively on your environment.</p>

<h3 id="windows-linux-mac">Windows, Linux, Mac</h3>

<p>We provide a CLI (command-line interface) app to chat with the bot in your terminal. Before installing
the CLI app, we should install some dependencies first.</p>
<ol>
  <li>We use Conda to manage our app, so we need to install a version of conda. We can install <a href="https://docs.conda.io/en/latest/miniconda.html">Miniconda</a> or <a href="https://github.com/conda-forge/miniforge">Miniforge</a>.</li>
  <li>On Windows and Linux, the chatbot application runs on GPU via the Vulkan platform. For Windows and Linux users,
please install the latest <a href="https://developer.nvidia.com/vulkan-driver">Vulkan driver</a>. For NVIDIA GPU users, please make sure to install
Vulkan driver, as the CUDA driver may not be good.</li>
</ol>

<p>After installing all the dependencies, just follow the instructions below the install the CLI app:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create a new conda environment and activate the environment.</span>
conda create <span class="nt">-n</span> mlc-chat
conda activate mlc-chat

<span class="c"># Install Git and Git-LFS, which is used for downloading the model weights</span>
<span class="c"># from Hugging Face.</span>
conda <span class="nb">install </span>git git-lfs

<span class="c"># Install the chat CLI app from Conda.</span>
conda <span class="nb">install</span> <span class="nt">-c</span> mlc-ai <span class="nt">-c</span> conda-forge mlc-chat-nightly

<span class="c"># Create a directory, download the model weights from HuggingFace, and download the binary libraries</span>
<span class="c"># from GitHub.</span>
<span class="nb">mkdir</span> <span class="nt">-p</span> dist
git lfs <span class="nb">install
</span>git clone https://huggingface.co/mlc-ai/demo-vicuna-v1-7b-int3 dist/vicuna-v1-7b
git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/lib

<span class="c"># Enter this line and enjoy chatting with the bot running natively on your machine!</span>
mlc_chat_cli
</code></pre></div></div>

<p align="center">
<img src="linux.gif" width="80%" />
</p>

<h3 id="ios">iOS</h3>

<p>Try out this <a href="https://testflight.apple.com/join/57zd7oxa">TestFlight page</a> (limited to the first 9000 users) to install and use
our example iOS chat app built for iPhone. Our app itself needs about 4GB of memory to run. Considering the iOS and other running applications, we will need a recent iPhone with 6GB (or more) of memory to run the app. We only tested the
application on iPhone 14 Pro Max and iPhone 12 Pro. You can also check out our <a href="(https://github.com/mlc-ai/mlc-llm)">GitHub repo</a> to
build the iOS app from source.</p>

<p>Note: The text generation speed on the iOS app can be unstable from time to time. It might run slow
for a while and recover to a normal speed then.</p>

<h3 id="web-browser">Web Browser</h3>

<p>Please check out <a href="https://mlc.ai/web-llm/">WebLLM</a>, our companion project that deploys models natively to browsers. Everything here runs inside the browser with no server support and accelerated with WebGPU.</p>

<h2 id="links">Links</h2>

<ul>
  <li>Check out our <a href="https://github.com/mlc-ai/mlc-llm">GitHub repo</a> to see how we build, optimize and deploy the bring large-language models to various devices and backends.</li>
  <li>Check out our companion project <a href="https://mlc.ai/web-llm/">WebLLM</a> to run the chatbot purely in your browser.</li>
  <li>You might also be interested in <a href="https://mlc.ai/web-stable-diffusion/">Web Stable Diffusion</a>, which runs the stable-diffusion model purely in the browser.</li>
</ul>

<h2 id="disclaimer">Disclaimer</h2>
<p>The pre-packaged demos are for research purposes only, subject to the model License.</p>

            
        </div> <!-- /container -->

        <!-- Support retina images. -->
        <script type="text/javascript"
                src="/mlc-llm/assets/js/srcset-polyfill.js"></script>
    </body>

</html>

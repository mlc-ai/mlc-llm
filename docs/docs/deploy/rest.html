





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Rest API &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CLI and C++ API" href="cli.html" />
    <link rel="prev" title="WebLLM and Javascript API" href="javascript.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/project_overview.html">Project Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/mlc_chat_config.html">Configure MLCChat in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="javascript.html">WebLLM and Javascript API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Rest API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#install-mlc-chat-package">Install MLC-Chat Package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#verify-installation">Verify Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optional-build-from-source">Optional: Build from Source</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#launch-the-server">Launch the Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="#api-endpoints">API Endpoints</a></li>
<li class="toctree-l2"><a class="reference internal" href="#request-objects">Request Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="#response-objects">Response Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="#use-rest-api-in-your-own-program">Use REST API in your own program</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI and C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="python.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android App</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Models via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/distribute_compiled_models.html">Distribute Compiled Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/configure_quantization.html">ðŸš§ Configure Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/define_new_models.html">ðŸš§ Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Rest API</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/rest.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="rest-api">
<h1>Rest API<a class="headerlink" href="#rest-api" title="Permalink to this heading">Â¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#install-mlc-chat-package" id="id2">Install MLC-Chat Package</a></p>
<ul>
<li><p><a class="reference internal" href="#verify-installation" id="id3">Verify Installation</a></p></li>
<li><p><a class="reference internal" href="#optional-build-from-source" id="id4">Optional: Build from Source</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#launch-the-server" id="id5">Launch the Server</a></p></li>
<li><p><a class="reference internal" href="#api-endpoints" id="id6">API Endpoints</a></p></li>
<li><p><a class="reference internal" href="#request-objects" id="id7">Request Objects</a></p></li>
<li><p><a class="reference internal" href="#response-objects" id="id8">Response Objects</a></p></li>
<li><p><a class="reference internal" href="#use-rest-api-in-your-own-program" id="id9">Use REST API in your own program</a></p></li>
</ul>
</nav>
<p>We provide <a class="reference external" href="https://www.ibm.com/topics/rest-apis#:~:text=the%20next%20step-,What%20is%20a%20REST%20API%3F,representational%20state%20transfer%20architectural%20style.">REST API</a>
for a user to interact with MLC-Chat in their own programs.</p>
<section id="install-mlc-chat-package">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Install MLC-Chat Package</a><a class="headerlink" href="#install-mlc-chat-package" title="Permalink to this heading">Â¶</a></h2>
<p>The REST API is a part of the MLC-Chat package, which we have prepared pre-built <a class="reference internal" href="../install/mlc_llm.html"><span class="doc">pip wheels</span></a>.</p>
<section id="verify-installation">
<h3><a class="toc-backref" href="#id3" role="doc-backlink">Verify Installation</a><a class="headerlink" href="#verify-installation" title="Permalink to this heading">Â¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>mlc_chat.rest<span class="w"> </span>--help
</pre></div>
</div>
<p>You are expected to see the help information of the REST API.</p>
</section>
<section id="optional-build-from-source">
<span id="mlcchat-package-build-from-source"></span><h3><a class="toc-backref" href="#id4" role="doc-backlink">Optional: Build from Source</a><a class="headerlink" href="#optional-build-from-source" title="Permalink to this heading">Â¶</a></h3>
<p>If the prebuilt is unavailable on your platform, or you would like to build a runtime
that supports other GPU runtime than the prebuilt version. We can build a customized version
of mlc chat runtime. You only need to do this if you choose not to use the prebuilt.</p>
<p>First, make sure you install TVM unity (following the instruction in <a class="reference internal" href="../install/tvm.html#install-tvm-unity"><span class="std std-ref">Install TVM Unity Compiler</span></a>).
You can choose to only pip install <cite>mlc-ai-nightly</cite> that comes with the tvm unity but skip <cite>mlc-chat-nightly</cite>.
Then please follow the instructions in <a class="reference internal" href="../install/mlc_llm.html#mlcchat-build-from-source"><span class="std std-ref">Option 2. Build from Source</span></a> to build the necessary libraries.</p>
<p>You can now use <code class="docutils literal notranslate"><span class="pre">mlc_chat</span></code> package by including the <cite>python</cite> directory to <code class="docutils literal notranslate"><span class="pre">PYTHONPATH</span></code> environment variable.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">PYTHONPATH</span><span class="o">=</span>python<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>mlc_chat.rest<span class="w"> </span>--help
</pre></div>
</div>
</section>
</section>
<section id="launch-the-server">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Launch the Server</a><a class="headerlink" href="#launch-the-server" title="Permalink to this heading">Â¶</a></h2>
<p>To launch the REST server for MLC-Chat, run the following command in your terminal.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>mlc_chat.rest<span class="w"> </span>--model<span class="w"> </span>MODEL<span class="w"> </span><span class="o">[</span>--lib-path<span class="w"> </span>LIB_PATH<span class="o">]</span><span class="w"> </span><span class="o">[</span>--device<span class="w"> </span>DEVICE<span class="o">]</span><span class="w"> </span><span class="o">[</span>--host<span class="w"> </span>HOST<span class="o">]</span><span class="w"> </span><span class="o">[</span>--port<span class="w"> </span>PORT<span class="o">]</span>
</pre></div>
</div>
<dl class="option-list">
<dt><kbd><span class="option">--model</span></kbd></dt>
<dd><p>The model folder after compiling with MLC-LLM build process. The parameter
can either be the model name with its quantization scheme
(e.g. <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>), or a full path to the model
folder. In the former case, we will use the provided name to search
for the model folder over possible paths.</p>
</dd>
<dt><kbd><span class="option">--lib-path</span></kbd></dt>
<dd><p>An optional field to specify the full path to the model library file to use (e.g. a <code class="docutils literal notranslate"><span class="pre">.so</span></code> file).</p>
</dd>
<dt><kbd><span class="option">--device</span></kbd></dt>
<dd><p>The description of the device to run on. User should provide a string in the
form of â€˜device_name:device_idâ€™ or â€˜device_nameâ€™, where â€˜device_nameâ€™ is one of
â€˜cudaâ€™, â€˜metalâ€™, â€˜vulkanâ€™, â€˜rocmâ€™, â€˜openclâ€™, â€˜autoâ€™ (automatically detect the
local device), and â€˜device_idâ€™ is the device id to run on. The default value is <code class="docutils literal notranslate"><span class="pre">auto</span></code>,
with the device id set to 0 for default.</p>
</dd>
<dt><kbd><span class="option">--host</span></kbd></dt>
<dd><p>The host at which the server should be started, defaults to <code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code>.</p>
</dd>
<dt><kbd><span class="option">--port</span></kbd></dt>
<dd><p>The port on which the server should be started, defaults to <code class="docutils literal notranslate"><span class="pre">8000</span></code>.</p>
</dd>
</dl>
<p>You can access <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:PORT/docs</span></code> (replace <code class="docutils literal notranslate"><span class="pre">PORT</span></code> with the port number you specified) to see the list of
supported endpoints.</p>
</section>
<section id="api-endpoints">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">API Endpoints</a><a class="headerlink" href="#api-endpoints" title="Permalink to this heading">Â¶</a></h2>
<p>The REST API provides the following endpoints:</p>
<dl class="http get">
<dt class="sig sig-object http" id="get--v1-completions">
<span class="sig-name descname"><span class="pre">GET</span> </span><span class="sig-name descname"><span class="pre">/v1/completions</span></span><a class="headerlink" href="#get--v1-completions" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<hr class="docutils" />
<blockquote>
<div><p>Get a completion from MLC-Chat using a prompt.</p>
</div></blockquote>
<p><strong>Request body</strong></p>
<dl>
<dt><strong>model</strong>: <em>str</em> (required)</dt><dd><p>The model folder after compiling with MLC-LLM build process. The parameter
can either be the model name with its quantization scheme
(e.g. <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>), or a full path to the model
folder. In the former case, we will use the provided name to search
for the model folder over possible paths.</p>
</dd>
<dt><strong>prompt</strong>: <em>str</em> (required)</dt><dd><p>A list of chat messages. The last message should be from the user.</p>
</dd>
<dt><strong>stream</strong>: <em>bool</em> (optional)</dt><dd><p>Whether to stream the response. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the response will be streamed
as the model generates the response. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the response will be
returned after the model finishes generating the response.</p>
</dd>
<dt><strong>temperature</strong>: <em>float</em> (optional)</dt><dd><p>The temperature applied to logits before sampling. The default value is
<code class="docutils literal notranslate"><span class="pre">0.7</span></code>. A higher temperature encourages more diverse outputs, while a
lower temperature produces more deterministic outputs.</p>
</dd>
<dt><strong>top_p</strong>: <em>float</em> (optional)</dt><dd><p>This parameter determines the set of tokens from which we sample during
decoding. The default value is set to <code class="docutils literal notranslate"><span class="pre">0.95</span></code>. At each step, we select
tokens from the minimal set that has a cumulative probability exceeding
the <code class="docutils literal notranslate"><span class="pre">top_p</span></code> parameter.</p>
<p>For additional information on top-p sampling, please refer to this blog
post: <a class="reference external" href="https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling">https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling</a>.</p>
</dd>
<dt><strong>repetition_penalty</strong>: <em>float</em> (optional)</dt><dd><p>The repetition penalty controls the likelihood of the model generating
repeated texts. The default value is set to <code class="docutils literal notranslate"><span class="pre">1.0</span></code>, indicating that no
repetition penalty is applied. Increasing the value reduces the
likelihood of repeat text generation. However, setting a high
<code class="docutils literal notranslate"><span class="pre">repetition_penalty</span></code> may result in the model generating meaningless
texts. The ideal choice of repetition penalty may vary among models.</p>
<p>For more details on how repetition penalty controls text generation, please
check out the CTRL paper (<a class="reference external" href="https://arxiv.org/pdf/1909.05858.pdf">https://arxiv.org/pdf/1909.05858.pdf</a>).</p>
</dd>
<dt><strong>presence_penalty</strong>: <em>float</em> (optional)</dt><dd><p>Positive values penalize new tokens if they are already present in the text so far,
decreasing the modelâ€™s likelihood to repeat tokens.</p>
</dd>
<dt><strong>frequency_penalty</strong>: <em>float</em> (optional)</dt><dd><p>Positive values penalize new tokens based on their existing frequency in the text so far,
decreasing the modelâ€™s likelihood to repeat tokens.</p>
</dd>
<dt><strong>mean_gen_len</strong>: <em>int</em> (optional)</dt><dd><p>The approximated average number of generated tokens in each round. Used
to determine whether the maximum window size would be exceeded.</p>
</dd>
<dt><strong>max_gen_len</strong>: <em>int</em> (optional)</dt><dd><p>This parameter determines the maximum length of the generated text. If it is
not set, the model will generate text until it encounters a stop token.</p>
</dd>
</dl>
<hr class="docutils" />
<dl class="simple">
<dt><strong>Returns</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">stream</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the response will be a <code class="docutils literal notranslate"><span class="pre">CompletionResponse</span></code> object.
If <code class="docutils literal notranslate"><span class="pre">stream</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the response will be a stream of <code class="docutils literal notranslate"><span class="pre">CompletionStreamResponse</span></code> objects.</p>
</dd>
</dl>
<dl class="http get">
<dt class="sig sig-object http" id="get--v1-chat-completions">
<span class="sig-name descname"><span class="pre">GET</span> </span><span class="sig-name descname"><span class="pre">/v1/chat/completions</span></span><a class="headerlink" href="#get--v1-chat-completions" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<hr class="docutils" />
<blockquote>
<div><p>Get a response from MLC-Chat using a prompt, either with or without streaming.</p>
</div></blockquote>
<p><strong>Request body</strong></p>
<dl>
<dt><strong>model</strong>: <em>str</em> (required)</dt><dd><p>The model folder after compiling with MLC-LLM build process. The parameter
can either be the model name with its quantization scheme
(e.g. <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>), or a full path to the model
folder. In the former case, we will use the provided name to search
for the model folder over possible paths.</p>
</dd>
<dt><strong>messages</strong>: <em>list[ChatMessage]</em> (required)</dt><dd><p>A list of chat messages. The last message should be from the user.</p>
</dd>
<dt><strong>stream</strong>: <em>bool</em> (optional)</dt><dd><p>Whether to stream the response. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the response will be streamed
as the model generates the response. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the response will be
returned after the model finishes generating the response.</p>
</dd>
<dt><strong>temperature</strong>: <em>float</em> (optional)</dt><dd><p>The temperature applied to logits before sampling. The default value is
<code class="docutils literal notranslate"><span class="pre">0.7</span></code>. A higher temperature encourages more diverse outputs, while a
lower temperature produces more deterministic outputs.</p>
</dd>
<dt><strong>top_p</strong>: <em>float</em> (optional)</dt><dd><p>This parameter determines the set of tokens from which we sample during
decoding. The default value is set to <code class="docutils literal notranslate"><span class="pre">0.95</span></code>. At each step, we select
tokens from the minimal set that has a cumulative probability exceeding
the <code class="docutils literal notranslate"><span class="pre">top_p</span></code> parameter.</p>
<p>For additional information on top-p sampling, please refer to this blog
post: <a class="reference external" href="https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling">https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling</a>.</p>
</dd>
<dt><strong>repetition_penalty</strong>: <em>float</em> (optional)</dt><dd><p>The repetition penalty controls the likelihood of the model generating
repeated texts. The default value is set to <code class="docutils literal notranslate"><span class="pre">1.0</span></code>, indicating that no
repetition penalty is applied. Increasing the value reduces the
likelihood of repeat text generation. However, setting a high
<code class="docutils literal notranslate"><span class="pre">repetition_penalty</span></code> may result in the model generating meaningless
texts. The ideal choice of repetition penalty may vary among models.</p>
<p>For more details on how repetition penalty controls text generation, please
check out the CTRL paper (<a class="reference external" href="https://arxiv.org/pdf/1909.05858.pdf">https://arxiv.org/pdf/1909.05858.pdf</a>).</p>
</dd>
<dt><strong>presence_penalty</strong>: <em>float</em> (optional)</dt><dd><p>Positive values penalize new tokens if they are already present in the text so far,
decreasing the modelâ€™s likelihood to repeat tokens.</p>
</dd>
<dt><strong>frequency_penalty</strong>: <em>float</em> (optional)</dt><dd><p>Positive values penalize new tokens based on their existing frequency in the text so far,
decreasing the modelâ€™s likelihood to repeat tokens.</p>
</dd>
<dt><strong>mean_gen_len</strong>: <em>int</em> (optional)</dt><dd><p>The approximated average number of generated tokens in each round. Used
to determine whether the maximum window size would be exceeded.</p>
</dd>
<dt><strong>max_gen_len</strong>: <em>int</em> (optional)</dt><dd><p>This parameter determines the maximum length of the generated text. If it is
not set, the model will generate text until it encounters a stop token.</p>
</dd>
<dt><strong>n</strong>: <em>int</em> (optional)</dt><dd><p>This parameter determines the number of text samples to generate. The default
value is <code class="docutils literal notranslate"><span class="pre">1</span></code>. Note that this parameter is only used when <code class="docutils literal notranslate"><span class="pre">stream</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt><strong>stop</strong>: <em>str</em> or <em>list[str]</em> (optional)</dt><dd><p>When <code class="docutils literal notranslate"><span class="pre">stop</span></code> is encountered, the model will stop generating output.
It can be a string or a list of strings. If it is a list of strings, the model
will stop generating output when any of the strings in the list is encountered.
Note that this parameter does not override the default stop string of the model.</p>
</dd>
</dl>
<hr class="docutils" />
<dl class="simple">
<dt><strong>Returns</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">stream</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the response will be a <code class="docutils literal notranslate"><span class="pre">ChatCompletionResponse</span></code> object.
If <code class="docutils literal notranslate"><span class="pre">stream</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the response will be a stream of <code class="docutils literal notranslate"><span class="pre">ChatCompletionStreamResponse</span></code> objects.</p>
</dd>
</dl>
<dl class="http get">
<dt class="sig sig-object http" id="get--chat-reset">
<span class="sig-name descname"><span class="pre">GET</span> </span><span class="sig-name descname"><span class="pre">/chat/reset</span></span><a class="headerlink" href="#get--chat-reset" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Reset the chat.</p>
</dd></dl>

<dl class="http get">
<dt class="sig sig-object http" id="get--stats">
<span class="sig-name descname"><span class="pre">GET</span> </span><span class="sig-name descname"><span class="pre">/stats</span></span><a class="headerlink" href="#get--stats" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Get the latest runtime stats (encode/decode speed).</p>
</dd></dl>

<dl class="http get">
<dt class="sig sig-object http" id="get--verbose_stats">
<span class="sig-name descname"><span class="pre">GET</span> </span><span class="sig-name descname"><span class="pre">/verbose_stats</span></span><a class="headerlink" href="#get--verbose_stats" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Get the verbose runtime stats (encode/decode speed, total runtime).</p>
</dd></dl>

</section>
<section id="request-objects">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Request Objects</a><a class="headerlink" href="#request-objects" title="Permalink to this heading">Â¶</a></h2>
<p><strong>ChatMessage</strong></p>
<dl class="simple">
<dt><strong>role</strong>: <em>str</em> (required)</dt><dd><p>The role(author) of the message. It can be either <code class="docutils literal notranslate"><span class="pre">user</span></code> or <code class="docutils literal notranslate"><span class="pre">assistant</span></code>.</p>
</dd>
<dt><strong>content</strong>: <em>str</em> (required)</dt><dd><p>The content of the message.</p>
</dd>
<dt><strong>name</strong>: <em>str</em> (optional)</dt><dd><p>The name of the author of the message.</p>
</dd>
</dl>
</section>
<section id="response-objects">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Response Objects</a><a class="headerlink" href="#response-objects" title="Permalink to this heading">Â¶</a></h2>
<p><strong>CompletionResponse</strong></p>
<dl class="simple">
<dt><strong>id</strong>: <em>str</em></dt><dd><p>The id of the completion.</p>
</dd>
<dt><strong>object</strong>: <em>str</em></dt><dd><p>The object name <code class="docutils literal notranslate"><span class="pre">text.completion</span></code>.</p>
</dd>
<dt><strong>created</strong>: <em>int</em></dt><dd><p>The time when the completion is created.</p>
</dd>
<dt><strong>choices</strong>: <em>list[CompletionResponseChoice]</em></dt><dd><p>A list of choices generated by the model.</p>
</dd>
<dt><strong>usage</strong>: <em>UsageInfo</em> or <em>None</em></dt><dd><p>The usage information of the model.</p>
</dd>
</dl>
<hr class="docutils" />
<p><strong>CompletionResponseChoice</strong></p>
<dl class="simple">
<dt><strong>index</strong>: <em>int</em></dt><dd><p>The index of the choice.</p>
</dd>
<dt><strong>text</strong>: <em>str</em></dt><dd><p>The message generated by the model.</p>
</dd>
<dt><strong>finish_reason</strong>: <em>str</em></dt><dd><p>The reason why the model finishes generating the message. It can be either
<code class="docutils literal notranslate"><span class="pre">stop</span></code> or <code class="docutils literal notranslate"><span class="pre">length</span></code>.</p>
</dd>
</dl>
<hr class="docutils" />
<p><strong>CompletionStreamResponse</strong></p>
<dl class="simple">
<dt><strong>id</strong>: <em>str</em></dt><dd><p>The id of the completion.</p>
</dd>
<dt><strong>object</strong>: <em>str</em></dt><dd><p>The object name <code class="docutils literal notranslate"><span class="pre">text.completion.chunk</span></code>.</p>
</dd>
<dt><strong>created</strong>: <em>int</em></dt><dd><p>The time when the completion is created.</p>
</dd>
<dt><strong>choices</strong>: <em>list[ChatCompletionResponseStreamhoice]</em></dt><dd><p>A list of choices generated by the model.</p>
</dd>
</dl>
<hr class="docutils" />
<p><strong>ChatCompletionResponseStreamChoice</strong></p>
<dl class="simple">
<dt><strong>index</strong>: <em>int</em></dt><dd><p>The index of the choice.</p>
</dd>
<dt><strong>text</strong>: <em>str</em></dt><dd><p>The message generated by the model.</p>
</dd>
<dt><strong>finish_reason</strong>: <em>str</em></dt><dd><p>The reason why the model finishes generating the message. It can be either
<code class="docutils literal notranslate"><span class="pre">stop</span></code> or <code class="docutils literal notranslate"><span class="pre">length</span></code>.</p>
</dd>
</dl>
<hr class="docutils" />
<p><strong>ChatCompletionResponse</strong></p>
<dl class="simple">
<dt><strong>id</strong>: <em>str</em></dt><dd><p>The id of the completion.</p>
</dd>
<dt><strong>object</strong>: <em>str</em></dt><dd><p>The object name <code class="docutils literal notranslate"><span class="pre">chat.completion</span></code>.</p>
</dd>
<dt><strong>created</strong>: <em>int</em></dt><dd><p>The time when the completion is created.</p>
</dd>
<dt><strong>choices</strong>: <em>list[ChatCompletionResponseChoice]</em></dt><dd><p>A list of choices generated by the model.</p>
</dd>
<dt><strong>usage</strong>: <em>UsageInfo</em> or <em>None</em></dt><dd><p>The usage information of the model.</p>
</dd>
</dl>
<hr class="docutils" />
<p><strong>ChatCompletionResponseChoice</strong></p>
<dl class="simple">
<dt><strong>index</strong>: <em>int</em></dt><dd><p>The index of the choice.</p>
</dd>
<dt><strong>message</strong>: <em>ChatMessage</em></dt><dd><p>The message generated by the model.</p>
</dd>
<dt><strong>finish_reason</strong>: <em>str</em></dt><dd><p>The reason why the model finishes generating the message. It can be either
<code class="docutils literal notranslate"><span class="pre">stop</span></code> or <code class="docutils literal notranslate"><span class="pre">length</span></code>.</p>
</dd>
</dl>
<hr class="docutils" />
<p><strong>ChatCompletionStreamResponse</strong></p>
<dl class="simple">
<dt><strong>id</strong>: <em>str</em></dt><dd><p>The id of the completion.</p>
</dd>
<dt><strong>object</strong>: <em>str</em></dt><dd><p>The object name <code class="docutils literal notranslate"><span class="pre">chat.completion.chunk</span></code>.</p>
</dd>
<dt><strong>created</strong>: <em>int</em></dt><dd><p>The time when the completion is created.</p>
</dd>
<dt><strong>choices</strong>: <em>list[ChatCompletionResponseStreamhoice]</em></dt><dd><p>A list of choices generated by the model.</p>
</dd>
</dl>
<hr class="docutils" />
<p><strong>ChatCompletionResponseStreamChoice</strong></p>
<dl class="simple">
<dt><strong>index</strong>: <em>int</em></dt><dd><p>The index of the choice.</p>
</dd>
<dt><strong>delta</strong>: <em>DeltaMessage</em></dt><dd><p>The delta message generated by the model.</p>
</dd>
<dt><strong>finish_reason</strong>: <em>str</em></dt><dd><p>The reason why the model finishes generating the message. It can be either
<code class="docutils literal notranslate"><span class="pre">stop</span></code> or <code class="docutils literal notranslate"><span class="pre">length</span></code>.</p>
</dd>
</dl>
<hr class="docutils" />
<p><strong>DeltaMessage</strong></p>
<dl class="simple">
<dt><strong>role</strong>: <em>str</em></dt><dd><p>The role(author) of the message. It can be either <code class="docutils literal notranslate"><span class="pre">user</span></code> or <code class="docutils literal notranslate"><span class="pre">assistant</span></code>.</p>
</dd>
<dt><strong>content</strong>: <em>str</em></dt><dd><p>The content of the message.</p>
</dd>
</dl>
</section>
<hr class="docutils" />
<section id="use-rest-api-in-your-own-program">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Use REST API in your own program</a><a class="headerlink" href="#use-rest-api-in-your-own-program" title="Permalink to this heading">Â¶</a></h2>
<p>Once you have launched the REST server, you can use the REST API in your own program. Below is an example of using REST API to interact with MLC-Chat in Python (suppose the server is running on <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:8000/</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>import<span class="w"> </span>requests
import<span class="w"> </span>json

<span class="c1"># Get a response using a prompt without streaming</span>
<span class="nv">payload</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>
<span class="w">   </span><span class="s2">&quot;model&quot;</span>:<span class="w"> </span><span class="s2">&quot;vicuna-v1-7b&quot;</span>,
<span class="w">   </span><span class="s2">&quot;messages&quot;</span>:<span class="w"> </span><span class="o">[{</span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,<span class="w"> </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;Write a haiku&quot;</span><span class="o">}]</span>,
<span class="w">   </span><span class="s2">&quot;stream&quot;</span>:<span class="w"> </span>False
<span class="o">}</span>
<span class="nv">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>requests.post<span class="o">(</span><span class="s2">&quot;http://127.0.0.1:8000/v1/chat/completions&quot;</span>,<span class="w"> </span><span class="nv">json</span><span class="o">=</span>payload<span class="o">)</span>
print<span class="o">(</span>f<span class="s2">&quot;Without streaming:\n{r.json()[&#39;choices&#39;][0][&#39;message&#39;][&#39;content&#39;]}\n&quot;</span><span class="o">)</span>

<span class="c1"># Reset the chat</span>
<span class="nv">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>requests.post<span class="o">(</span><span class="s2">&quot;http://127.0.0.1:8000/chat/reset&quot;</span>,<span class="w"> </span><span class="nv">json</span><span class="o">=</span>payload<span class="o">)</span>
print<span class="o">(</span>f<span class="s2">&quot;Reset chat: {str(r)}\n&quot;</span><span class="o">)</span>

<span class="c1"># Get a response using a prompt with streaming</span>
<span class="nv">payload</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>
<span class="w">   </span><span class="s2">&quot;model&quot;</span>:<span class="w"> </span><span class="s2">&quot;vicuna-v1-7b&quot;</span>,
<span class="w">   </span><span class="s2">&quot;messages&quot;</span>:<span class="w"> </span><span class="o">[{</span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,<span class="w"> </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;Write a haiku&quot;</span><span class="o">}]</span>,
<span class="w">   </span><span class="s2">&quot;stream&quot;</span>:<span class="w"> </span>True
<span class="o">}</span>
with<span class="w"> </span>requests.post<span class="o">(</span><span class="s2">&quot;http://127.0.0.1:8000/v1/chat/completions&quot;</span>,<span class="w"> </span><span class="nv">json</span><span class="o">=</span>payload,<span class="w"> </span><span class="nv">stream</span><span class="o">=</span>True<span class="o">)</span><span class="w"> </span>as<span class="w"> </span>r:
<span class="w">   </span>print<span class="o">(</span>f<span class="s2">&quot;With streaming:&quot;</span><span class="o">)</span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span>chunk<span class="w"> </span><span class="k">in</span><span class="w"> </span>r:
<span class="w">      </span><span class="nv">content</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>json.loads<span class="o">(</span>chunk<span class="o">[</span><span class="m">6</span>:-2<span class="o">])[</span><span class="s2">&quot;choices&quot;</span><span class="o">][</span><span class="m">0</span><span class="o">][</span><span class="s2">&quot;delta&quot;</span><span class="o">]</span>.get<span class="o">(</span><span class="s2">&quot;content&quot;</span>,<span class="w"> </span><span class="s2">&quot;&quot;</span><span class="o">)</span>
<span class="w">      </span>print<span class="o">(</span>f<span class="s2">&quot;{content}&quot;</span>,<span class="w"> </span><span class="nv">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span>,<span class="w"> </span><span class="nv">flush</span><span class="o">=</span>True<span class="o">)</span>
<span class="w">   </span>print<span class="o">(</span><span class="s2">&quot;\n&quot;</span><span class="o">)</span>

<span class="c1"># Get the latest runtime stats</span>
<span class="nv">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>requests.get<span class="o">(</span><span class="s2">&quot;http://127.0.0.1:8000/stats&quot;</span><span class="o">)</span>
print<span class="o">(</span>f<span class="s2">&quot;Runtime stats: {r.json()}\n&quot;</span><span class="o">)</span>
</pre></div>
</div>
<p>Please check <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/tree/main/examples/rest">example folder</a> for more examples using REST API.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The REST API is a uniform interface that supports multiple languages. You can also utilize the REST API in languages other than Python.</p>
</div>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="cli.html" class="btn btn-neutral float-right" title="CLI and C++ API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="javascript.html" class="btn btn-neutral float-left" title="WebLLM and Javascript API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">Â© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
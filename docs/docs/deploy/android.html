





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Android SDK &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="IDE Integration" href="ide_integration.html" />
    <link rel="prev" title="iOS Swift SDK" href="ios.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/introduction.html">Introduction to MLC LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="webllm.html">WebLLM Javascript SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_engine.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS Swift SDK</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Android SDK</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#demo-app">Demo App</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prerequisite">Prerequisite</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#additional-guides-for-windows-users">Additional Guides for Windows Users</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#build-android-app-from-source">Build Android App from Source</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-install-build-dependencies">Step 1. Install Build Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-build-runtime-and-model-libraries">Step 2. Build Runtime and Model Libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-build-android-app">Step 3. Build Android App</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#customize-the-app">Customize the App</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bundle-model-weights">Bundle Model Weights</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ide_integration.html">IDE Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlc_chat_config.html">Customize MLC Chat Config</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/convert_weights.html">Convert Model Weights</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Model Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/package_libraries_and_weights.html">Package Libraries and Weights</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Android SDK</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/android.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="android-sdk">
<span id="deploy-android"></span><h1>Android SDK<a class="headerlink" href="#android-sdk" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#demo-app" id="id1">Demo App</a></p></li>
<li><p><a class="reference internal" href="#prerequisite" id="id2">Prerequisite</a></p>
<ul>
<li><p><a class="reference internal" href="#additional-guides-for-windows-users" id="id3">Additional Guides for Windows Users</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#build-android-app-from-source" id="id4">Build Android App from Source</a></p>
<ul>
<li><p><a class="reference internal" href="#step-1-install-build-dependencies" id="id5">Step 1. Install Build Dependencies</a></p></li>
<li><p><a class="reference internal" href="#step-2-build-runtime-and-model-libraries" id="id6">Step 2. Build Runtime and Model Libraries</a></p></li>
<li><p><a class="reference internal" href="#step-3-build-android-app" id="id7">Step 3. Build Android App</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#customize-the-app" id="id8">Customize the App</a></p></li>
<li><p><a class="reference internal" href="#bundle-model-weights" id="id9">Bundle Model Weights</a></p></li>
</ul>
</nav>
<section id="demo-app">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Demo App</a><a class="headerlink" href="#demo-app" title="Permalink to this heading">¶</a></h2>
<p>The demo APK below is built for Samsung S23 with Snapdragon 8 Gen 2 chip.</p>
<a class="reference external image-reference" href="https://github.com/mlc-ai/binary-mlc-llm-libs/releases/download/Android-06062024/mlc-chat.apk"><img alt="https://seeklogo.com/images/D/download-android-apk-badge-logo-D074C6882B-seeklogo.com.png" src="https://seeklogo.com/images/D/download-android-apk-badge-logo-D074C6882B-seeklogo.com.png" style="width: 135px;" /></a>
</section>
<section id="prerequisite">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Prerequisite</a><a class="headerlink" href="#prerequisite" title="Permalink to this heading">¶</a></h2>
<p><strong>Rust</strong> (<a class="reference external" href="https://www.rust-lang.org/tools/install">install</a>) is needed to cross-compile HuggingFace tokenizers to Android. Make sure rustc, cargo, and rustup are available in <code class="docutils literal notranslate"><span class="pre">$PATH</span></code>.</p>
<p><strong>Android Studio</strong> (<a class="reference external" href="https://developer.android.com/studio">install</a>) with NDK and CMake. To install NDK and CMake, in the Android Studio welcome page, click “Projects → SDK Manager → SDK Tools”. Set up the following environment variables:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ANDROID_NDK</span></code> so that <code class="docutils literal notranslate"><span class="pre">$ANDROID_NDK/build/cmake/android.toolchain.cmake</span></code> is available.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TVM_NDK_CC</span></code> that points to NDK’s clang compiler.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example on macOS</span>
ANDROID_NDK:<span class="w"> </span><span class="nv">$HOME</span>/Library/Android/sdk/ndk/25.2.9519653
TVM_NDK_CC:<span class="w"> </span><span class="nv">$ANDROID_NDK</span>/toolchains/llvm/prebuilt/darwin-x86_64/bin/aarch64-linux-android24-clang
<span class="c1"># Example on Linux</span>
ANDROID_NDK:<span class="w"> </span><span class="nv">$HOME</span>/Android/Sdk/ndk/25.2.9519653
TVM_NDK_CC:<span class="w"> </span><span class="nv">$ANDROID_NDK</span>/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang
<span class="c1"># Example on Windows</span>
ANDROID_NDK:<span class="w"> </span>%HOME%/AppData/Local/Android/Sdk/ndk/25.2.9519653
TVM_NDK_CC:<span class="w"> </span>%ANDROID_NDK%/toolchains/llvm/prebuilt/windows-x86_64/bin/aarch64-linux-android24-clang
</pre></div>
</div>
<p><strong>JDK</strong>, such as OpenJDK &gt;= 17, to compile Java bindings of TVM Unity runtime.
We strongly recommend setting the <code class="docutils literal notranslate"><span class="pre">JAVA_HOME</span></code> to the JDK bundled with Android Studio.
e.g.
<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">JAVA_HOME=/Applications/Android\</span> <span class="pre">Studio.app/Contents/jbr/Contents/Home</span></code> for macOS.
<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">JAVA_HOME=/opt/android-studio/jbr</span></code> for Linux.
Using Android Studio’s JBR bundle as recommended <cite>here https://developer.android.com/build/jdks</cite>
will reduce the chances of potential errors in JNI compilation.
Set up the following environment variable:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">JAVA_HOME=/path/to/java_home</span></code> you can then cross check and make sure <code class="docutils literal notranslate"><span class="pre">$JAVA_HOME/bin/java</span></code> exists.</p></li>
</ul>
<p>Please ensure that the JDK versions for Android Studio and JAVA_HOME are the same.</p>
<p><strong>TVM Unity runtime</strong> is placed under <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/tree/main/3rdparty">3rdparty/tvm</a> in MLC LLM, so there is no need to install anything extra. Set up the following environment variable:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">TVM_SOURCE_DIR=/path/to/mlc-llm/3rdparty/tvm</span></code>.</p></li>
</ul>
<p>Please follow <a class="reference internal" href="../install/mlc_llm.html"><span class="doc">Install MLC LLM Python Package</span></a> to obtain a binary build of mlc_llm package. Note that this
is independent from mlc-llm source code that we use for android package build in the following up section.
Once you installed this package, you do not need to build mlc llm from source.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>❗ Whenever using Python, it is highly recommended to use <strong>conda</strong> to manage an isolated Python environment to avoid missing dependencies, incompatible versions, and package conflicts.</p>
</div>
<p>Check if <strong>environment variable</strong> are properly set as the last check. One way to ensure this is to place them in <code class="docutils literal notranslate"><span class="pre">$HOME/.zshrc</span></code>, <code class="docutils literal notranslate"><span class="pre">$HOME/.bashrc</span></code> or environment management tools.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span><span class="w"> </span><span class="nv">$HOME</span>/.cargo/env<span class="w"> </span><span class="c1"># Rust</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ANDROID_NDK</span><span class="o">=</span>...<span class="w">  </span><span class="c1"># Android NDK toolchain</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TVM_NDK_CC</span><span class="o">=</span>...<span class="w">   </span><span class="c1"># Android NDK clang</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">JAVA_HOME</span><span class="o">=</span>...<span class="w">    </span><span class="c1"># Java</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TVM_SOURCE_DIR</span><span class="o">=</span>...<span class="w">     </span><span class="c1"># TVM Unity runtime</span>
</pre></div>
</div>
<section id="additional-guides-for-windows-users">
<h3><a class="toc-backref" href="#id3" role="doc-backlink">Additional Guides for Windows Users</a><a class="headerlink" href="#additional-guides-for-windows-users" title="Permalink to this heading">¶</a></h3>
<p>Building under Windows for Android is still experimental; please make sure you
first finish the above guides, then read and follow the instructions in this section
If you are using Windows, make sure you use conda to install cmake and Ninja.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>cmake<span class="w"> </span>ninja<span class="w"> </span>git<span class="w"> </span>git-lfs<span class="w"> </span>zstd
</pre></div>
</div>
<p>Windows Java findings have issues with environment variables that come with space.
Make sure you get a copy of Java in a path without space. The simplest way to do that
is to copy the Android Studio’s JBR bundle to a directory without any space.
If your Android studio’s installation is at <code class="docutils literal notranslate"><span class="pre">C:\Program</span> <span class="pre">Files\Android\Android</span> <span class="pre">Studio\</span></code>
you can try to do the following</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cp<span class="w"> </span>-r<span class="w"> </span><span class="s2">&quot;C:\Program Files\Android\Android Studio\jbr&quot;</span><span class="w"> </span>C:<span class="se">\a</span>ny-path-without-space
<span class="nb">set</span><span class="w"> </span><span class="nv">JAVA_HOME</span><span class="o">=</span>C:<span class="se">\a</span>ny-path-without-space
</pre></div>
</div>
<p>You can continue the next steps after you have set these steps correctly.</p>
</section>
</section>
<section id="build-android-app-from-source">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Build Android App from Source</a><a class="headerlink" href="#build-android-app-from-source" title="Permalink to this heading">¶</a></h2>
<p>This section shows how we can build the app from the source.</p>
<section id="step-1-install-build-dependencies">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Step 1. Install Build Dependencies</a><a class="headerlink" href="#step-1-install-build-dependencies" title="Permalink to this heading">¶</a></h3>
<p>First and foremost, please clone the <a class="reference external" href="https://github.com/mlc-ai/mlc-llm">MLC LLM GitHub repository</a>.
After cloning, go to the <code class="docutils literal notranslate"><span class="pre">android/</span></code> directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mlc-ai/mlc-llm.git
<span class="nb">cd</span><span class="w"> </span>mlc-llm
git<span class="w"> </span>submodule<span class="w"> </span>update<span class="w"> </span>--init<span class="w"> </span>--recursive
<span class="nb">cd</span><span class="w"> </span>android
</pre></div>
</div>
</section>
<section id="step-2-build-runtime-and-model-libraries">
<span id="android-build-runtime-and-model-libraries"></span><h3><a class="toc-backref" href="#id6" role="doc-backlink">Step 2. Build Runtime and Model Libraries</a><a class="headerlink" href="#step-2-build-runtime-and-model-libraries" title="Permalink to this heading">¶</a></h3>
<p>The models to be built for the Android app are specified in <code class="docutils literal notranslate"><span class="pre">MLCChat/mlc-package-config.json</span></code>:
in the <code class="docutils literal notranslate"><span class="pre">model_list</span></code>, <code class="docutils literal notranslate"><span class="pre">model</span></code> points to the Hugging Face repository which</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code> points to the Hugging Face repository which contains the pre-converted model weights. The Android app will download model weights from the Hugging Face URL.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_id</span></code> is a unique model identifier.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">estimated_vram_bytes</span></code> is an estimation of the vRAM the model takes at runtime.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;bundle_weight&quot;:</span> <span class="pre">true</span></code> means the model weights of the model will be bundled into the app when building.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">overrides</span></code> specifies some model config parameter overrides.</p></li>
</ul>
<p>We have a one-line command to build and prepare all the model libraries:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/path/to/MLCChat<span class="w">  </span><span class="c1"># e.g., &quot;android/MLCChat&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_LLM_SOURCE_DIR</span><span class="o">=</span>/path/to/mlc-llm<span class="w">  </span><span class="c1"># e.g., &quot;../..&quot;</span>
mlc_llm<span class="w"> </span>package
</pre></div>
</div>
<p>This command mainly executes the following two steps:</p>
<ol class="arabic simple">
<li><p><strong>Compile models.</strong> We compile each model in <code class="docutils literal notranslate"><span class="pre">model_list</span></code> of <code class="docutils literal notranslate"><span class="pre">MLCChat/mlc-package-config.json</span></code> into a binary model library.</p></li>
<li><p><strong>Build runtime and tokenizer.</strong> In addition to the model itself, a lightweight runtime and tokenizer are required to actually run the LLM.</p></li>
</ol>
<p>The command creates a <code class="docutils literal notranslate"><span class="pre">./dist/</span></code> directory that contains the runtime and model build output.
Please make sure all the following files exist in <code class="docutils literal notranslate"><span class="pre">./dist/</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>dist
└── lib
    └── mlc4j
        ├── build.gradle
        ├── output
        │   ├── arm64-v8a
        │   │   └── libtvm4j_runtime_packed.so
        │   └── tvm4j_core.jar
        └── src
            ├── cpp
            │   └── tvm_runtime.h
            └── main
                ├── AndroidManifest.xml
                ├── assets
                │   └── mlc-app-config.json
                └── java
                    └── ...
</pre></div>
</div>
<p>The model execution logic in mobile GPUs is incorporated into <code class="docutils literal notranslate"><span class="pre">libtvm4j_runtime_packed.so</span></code>,
while <code class="docutils literal notranslate"><span class="pre">tvm4j_core.jar</span></code> is a lightweight (~60 kb) <a class="reference external" href="https://tvm.apache.org/docs/reference/api/javadoc/">Java binding</a>
to it. <code class="docutils literal notranslate"><span class="pre">dist/lib/mlc4j</span></code> is a gradle subproject that you should include in your app
so the Android project can reference the mlc4j (MLC LLM java library).
This library packages the dependent model libraries and necessary runtime to execute the model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">include</span> <span class="s1">&#39;:mlc4j&#39;</span>
<span class="n">project</span><span class="p">(</span><span class="s1">&#39;:mlc4j&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">projectDir</span> <span class="o">=</span> <span class="n">file</span><span class="p">(</span><span class="s1">&#39;dist/lib/mlc4j&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We leverage a local JIT cache to avoid repetitive compilation of the same input.
However, sometimes it is helpful to force rebuild when we have a new compiler update
or when something goes wrong with the cached library.
You can do so by setting the environment variable <code class="docutils literal notranslate"><span class="pre">MLC_JIT_POLICY=REDO</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">MLC_JIT_POLICY</span><span class="o">=</span>REDO<span class="w"> </span>mlc_llm<span class="w"> </span>package
</pre></div>
</div>
</div>
</section>
<section id="step-3-build-android-app">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Step 3. Build Android App</a><a class="headerlink" href="#step-3-build-android-app" title="Permalink to this heading">¶</a></h3>
<p>Open folder <code class="docutils literal notranslate"><span class="pre">./android/MLCChat</span></code> as an Android Studio Project.
Connect your Android device to your machine.
In the menu bar of Android Studio, click <strong>“Build → Make Project”</strong>.
Once the build is finished, click <strong>“Run → Run ‘app’”</strong> and you will see the app launched on your phone.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>❗ This app cannot be run in an emulator and thus a physical phone is required, because MLC LLM needs an actual mobile GPU to meaningfully run at an accelerated speed.</p>
</div>
</section>
</section>
<section id="customize-the-app">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Customize the App</a><a class="headerlink" href="#customize-the-app" title="Permalink to this heading">¶</a></h2>
<p>We can customize the models built in the Android app by customizing <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/android/MLCChat/mlc-package-config.json">MLCChat/mlc-package-config.json</a>.
We introduce each field of the JSON file here.</p>
<p>Each entry in <code class="docutils literal notranslate"><span class="pre">&quot;model_list&quot;</span></code> of the JSON file has the following fields:</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">model</span></code></dt><dd><p>(Required) The path to the MLC-converted model to be built into the app.
It is a Hugging Face URL (e.g., <code class="docutils literal notranslate"><span class="pre">&quot;model&quot;:</span> <span class="pre">&quot;HF://mlc-ai/phi-2-q4f16_1-MLC&quot;`</span></code>) that contains
the pre-converted model weights.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">model_id</span></code></dt><dd><p>(Required) A unique local identifier to identify the model.
It can be an arbitrary one.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">estimated_vram_bytes</span></code></dt><dd><p>(Required) Estimated requirements of vRAM to run the model.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">bundle_weight</span></code></dt><dd><p>(Optional) A boolean flag indicating whether to bundle model weights into the app. See <a class="reference internal" href="#android-bundle-model-weights"><span class="std std-ref">Bundle Model Weights</span></a> below.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">overrides</span></code></dt><dd><p>(Optional) A dictionary to override the default model context window size (to limit the KV cache size) and prefill chunk size (to limit the model temporary execution memory).
Example:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;android&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;model_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;HF://mlc-ai/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;model_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;estimated_vram_bytes&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1948348579</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;overrides&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">               </span><span class="nt">&quot;context_window_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">512</span><span class="p">,</span>
<span class="w">               </span><span class="nt">&quot;prefill_chunk_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">128</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">model_lib</span></code></dt><dd><p>(Optional) A string specifying the system library prefix to use for the model.
Usually this is used when you want to build multiple model variants with the same architecture into the app.
<strong>This field does not affect any app functionality.</strong>
The <code class="docutils literal notranslate"><span class="pre">&quot;model_lib_path_for_prepare_libs&quot;</span></code> introduced below is also related.
Example:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;android&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;model_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;HF://mlc-ai/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;model_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;estimated_vram_bytes&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1948348579</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;model_lib&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gpt_neox_q4f16_1&quot;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
</dl>
<p>Besides <code class="docutils literal notranslate"><span class="pre">model_list</span></code> in <code class="docutils literal notranslate"><span class="pre">MLCChat/mlc-package-config.json</span></code>,
you can also <strong>optionally</strong> specify a dictionary of <code class="docutils literal notranslate"><span class="pre">&quot;model_lib_path_for_prepare_libs&quot;</span></code>,
<strong>if you want to use model libraries that are manually compiled</strong>.
The keys of this dictionary should be the <code class="docutils literal notranslate"><span class="pre">model_lib</span></code> that specified in model list,
and the values of this dictionary are the paths (absolute, or relative) to the manually compiled model libraries.
The model libraries specified in <code class="docutils literal notranslate"><span class="pre">&quot;model_lib_path_for_prepare_libs&quot;</span></code> will be built into the app when running <code class="docutils literal notranslate"><span class="pre">mlc_llm</span> <span class="pre">package</span></code>.
Example:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;android&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;model_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;HF://mlc-ai/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;model_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;estimated_vram_bytes&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1948348579</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;model_lib&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gpt_neox_q4f16_1&quot;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">],</span>
<span class="w">   </span><span class="nt">&quot;model_lib_path_for_prepare_libs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;gpt_neox_q4f16_1&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;../../dist/lib/RedPajama-INCITE-Chat-3B-v1-q4f16_1-android.tar&quot;</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="bundle-model-weights">
<span id="android-bundle-model-weights"></span><h2><a class="toc-backref" href="#id9" role="doc-backlink">Bundle Model Weights</a><a class="headerlink" href="#bundle-model-weights" title="Permalink to this heading">¶</a></h2>
<p>Instructions have been provided to build an Android App with MLC LLM in previous sections,
but it requires run-time weight downloading from HuggingFace,
as configured in <code class="docutils literal notranslate"><span class="pre">MLCChat/mlc-package-config.json</span></code>.
However, it could be desirable to bundle weights together into the app to avoid downloading over the network.
In this section, we provide a simple ADB-based walkthrough that hopefully helps with further development.</p>
<p><strong>Enable weight bundle</strong>.
Set the field <code class="docutils literal notranslate"><span class="pre">&quot;bundle_weight&quot;:</span> <span class="pre">true</span></code> for any model you want to bundle weights
in <code class="docutils literal notranslate"><span class="pre">MLCChat/mlc-package-config.json</span></code>, and run <code class="docutils literal notranslate"><span class="pre">mlc_llm</span> <span class="pre">package</span></code> again.
Below is an example:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;android&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;model_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;HF://mlc-ai/gemma-2b-it-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;model_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gemma-2b-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;estimated_vram_bytes&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3000000000</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;bundle_weight&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The outcome of running <code class="docutils literal notranslate"><span class="pre">mlc_llm</span> <span class="pre">package</span></code> should be as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>dist
├── bundle
│   ├── gemma-2b-q4f16_1   # The model weights that will be bundled into the app.
│   └── mlc-app-config.json
└── ...
</pre></div>
</div>
<p><strong>Generating APK</strong>. Enter Android Studio, and click <strong>“Build → Generate Signed Bundle/APK”</strong> to build an APK for release. If it is the first time you generate an APK, you will need to create a key according to <a class="reference external" href="https://developer.android.com/studio/publish/app-signing#generate-key">the official guide from Android</a>.
This APK will be placed under <code class="docutils literal notranslate"><span class="pre">android/MLCChat/app/release/app-release.apk</span></code>.</p>
<p><strong>Install ADB and USB debugging</strong>. Enable “USB debugging” in the developer mode in your phone settings.
In “SDK manager - SDK Tools”, install <a class="reference external" href="https://developer.android.com/studio/releases/platform-tools">Android SDK Platform-Tools</a>.
Add the path to platform-tool path to the environment variable <code class="docutils literal notranslate"><span class="pre">PATH</span></code> (on macOS, it is <code class="docutils literal notranslate"><span class="pre">$HOME/Library/Android/sdk/platform-tools</span></code>).
Run the following commands, and if ADB is installed correctly, your phone will appear as a device:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>adb<span class="w"> </span>devices
</pre></div>
</div>
<p><strong>Install the APK and weights to your phone</strong>.
Run the commands below to install the app, and push the local weights to the app data directory on your device.
Once it finishes, you can start the MLCChat app on your device.
The models with <code class="docutils literal notranslate"><span class="pre">bundle_weight</span></code> set to true will have their weights already on device.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/path/to/MLCChat<span class="w">  </span><span class="c1"># e.g., &quot;android/MLCChat&quot;</span>
python<span class="w"> </span>bundle_weight.py<span class="w"> </span>--apk-path<span class="w"> </span>app/release/app-release.apk
</pre></div>
</div>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ide_integration.html" class="btn btn-neutral float-right" title="IDE Integration" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="ios.html" class="btn btn-neutral float-left" title="iOS Swift SDK" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
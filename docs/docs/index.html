





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>üëã Welcome to MLC LLM &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="_static/tabs.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <script type="text/javascript" src="_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Configure MLCChat in JSON" href="tutorials/runtime/mlc_chat_config.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://mlc.ai/mlc-llm>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/blog>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/web-llm>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/blog>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/web-llm>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="#">
          

          
            
            <img src="_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/runtime/mlc_chat_config.html">Configure MLCChat in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build Apps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/runtime/terminologies.html">Terminologies</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/runtime/cpp.html">C++ APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/runtime/javascript.html">WebLLM Javascript APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/runtime/rest.html">Rest APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/app_build/cli.html">Build MLCChat-CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/app_build/ios.html">Build iOS Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/app_build/android.html">Build Android Package</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/compilation/compile_models.html">Compile Models in MLC LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/compilation/distribute_compiled_models.html">Distribute Compiled Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/compilation/configure_targets.html">üöß Configure Targets</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/compilation/configure_quantization.html">üöß Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/customize/define_new_models.html">üöß Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/prebuilts/prebuilt_models.html">üöß Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation and Dependency</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install/tvm.html">Install TVM Unity</a></li>
<li class="toctree-l1"><a class="reference internal" href="install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="install/gpu.html">GPU Drivers and SDKs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/faq.html">Frequently Asked Questions (FAQ)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/deploy-models.html">How to Deploy Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/bring-your-own-models.html">Add New Model Architectures</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> <span class="br-arrow">></span></li>
        
      <li>üëã Welcome to MLC LLM</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/index.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="welcome-to-mlc-llm">
<h1>üëã Welcome to MLC LLM<a class="headerlink" href="#welcome-to-mlc-llm" title="Permalink to this heading">¬∂</a></h1>
<p><a class="reference external" href="https://discord.gg/9Xpy2HGBuD">Discord</a> | <a class="reference external" href="https://mlc.ai/mlc-llm">Demo</a> | <a class="reference external" href="https://github.com/mlc-ai/mlc-llm">GitHub</a></p>
<p>üöß This document is currently undergoing heavy construction.</p>
<p>Machine Learning Compilation for LLM (MLC LLM) is a universal deployment solution that enables LLMs to run efficiently on consumer devices, leveraging native hardware acceleration like GPUs.</p>
<table class="docutils align-center" id="id1">
<caption><span class="caption-text">MLC LLM: A universal deployment solution for large language models</span><a class="headerlink" href="#id1" title="Permalink to this table">¬∂</a></caption>
<colgroup>
<col style="width: 17%" />
<col style="width: 21%" />
<col style="width: 21%" />
<col style="width: 21%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>AMD GPU</p></th>
<th class="head"><p>NVIDIA GPU</p></th>
<th class="head"><p>Apple M1/M2 GPU</p></th>
<th class="head"><p>Intel GPU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Linux / Win</p></td>
<td><p>‚úÖ Vulkan, ROCm</p></td>
<td><p>‚úÖ Vulkan, CUDA</p></td>
<td><p>‚ùå</p></td>
<td><p>‚úÖ Vulkan</p></td>
</tr>
<tr class="row-odd"><td><p>macOS</p></td>
<td><p>‚úÖ Metal</p></td>
<td><p>‚ùå</p></td>
<td><p>‚úÖ Metal</p></td>
<td><p>‚úÖ Metal</p></td>
</tr>
<tr class="row-even"><td><p>Web Browser</p></td>
<td><p>‚úÖ WebGPU</p></td>
<td><p>‚úÖ WebGPU</p></td>
<td><p>‚úÖ WebGPU</p></td>
<td><p>‚úÖ WebGPU</p></td>
</tr>
<tr class="row-odd"><td><p>iOS / iPadOS</p></td>
<td colspan="4"><p>‚úÖ Metal on Apple M1/M2 GPU</p></td>
</tr>
<tr class="row-even"><td><p>Android</p></td>
<td colspan="2"><p>‚úÖ OpenCL on Adreno GPU</p></td>
<td colspan="2"><p>üöß  OpenCL on Mali GPU</p></td>
</tr>
</tbody>
</table>
<section id="get-started-try-out-mlc-llm-on-your-device">
<span id="get-started"></span><h2>Get Started ‚Äî Try out MLC LLM on your device<a class="headerlink" href="#get-started-try-out-mlc-llm-on-your-device" title="Permalink to this heading">¬∂</a></h2>
<p>We have prepared packages for you to try out MLC LLM locally, and you can try out prebuilt models on your device:</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">iOS</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Android</button><button aria-controls="panel-0-0-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-2" name="0-2" role="tab" tabindex="-1">CLI (Linux/MacOS/Windows)</button><button aria-controls="panel-0-0-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-3" name="0-3" role="tab" tabindex="-1">Web Browser</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><p>The MLC LLM app is now accessible on the App Store at no cost. You can download and explore it by simply clicking the button below:</p>
<a class="reference external image-reference" href="https://apps.apple.com/us/app/mlc-chat/id6448482937"><img alt="https://linkmaker.itunes.apple.com/assets/shared/badges/en-us/appstore-lrg.svg" src="https://linkmaker.itunes.apple.com/assets/shared/badges/en-us/appstore-lrg.svg" width="135" /></a>
<p>Once the app is installed, you can download the models and then engage in chat with the model without requiring an internet connection.</p>
<p>Memory requirements vary across different models. The Vicuna-7B model necessitates an iPhone device with a minimum of 6GB RAM, whereas the RedPajama-3B model can run on an iPhone with at least 4GB RAM.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://mlc.ai/blog/img/redpajama/ios.gif"><img alt="https://mlc.ai/blog/img/redpajama/ios.gif" src="https://mlc.ai/blog/img/redpajama/ios.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">MLC LLM on iOS</span><a class="headerlink" href="#id2" title="Permalink to this image">¬∂</a></p>
</figcaption>
</figure>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><p>The MLC LLM Android app is free and available for download and can be tried out by simply clicking the button below:</p>
<a class="reference external image-reference" href="https://github.com/mlc-ai/binary-mlc-llm-libs/raw/main/mlc-chat.apk"><img alt="https://seeklogo.com/images/D/download-android-apk-badge-logo-D074C6882B-seeklogo.com.png" src="https://seeklogo.com/images/D/download-android-apk-badge-logo-D074C6882B-seeklogo.com.png" style="width: 135px;" /></a>
<p>Once the app is installed, you can engage in a chat with the model without the need for an internet connection:</p>
<p>Memory requirements vary across different models. The Vicuna-7B model necessitates an Android device with a minimum of 6GB RAM, whereas the RedPajama-3B model can run on an Android device with at least 4GB RAM.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://mlc.ai/blog/img/android/android-recording.gif"><img alt="https://mlc.ai/blog/img/android/android-recording.gif" src="https://mlc.ai/blog/img/android/android-recording.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">MLC LLM on Android</span><a class="headerlink" href="#id3" title="Permalink to this image">¬∂</a></p>
</figcaption>
</figure>
</div><div aria-labelledby="tab-0-0-2" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-2" name="0-2" role="tabpanel" tabindex="0"><p>To utilize the models on your PC, we highly recommend trying out the CLI version of MLC LLM.</p>
<p>We have prepared Conda packages for MLC Chat CLI. If you haven‚Äôt installed Conda yet, please refer to <a class="reference internal" href="install/conda.html"><span class="doc">this tutorial</span></a> to install Conda.
To install MLC Chat CLI, simply run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>mlc-chat-venv<span class="w"> </span>-c<span class="w"> </span>mlc-ai<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>mlc-chat-nightly
conda<span class="w"> </span>activate<span class="w"> </span>mlc-chat-venv
</pre></div>
</div>
<p>If you are using Windows or Linux and want to use your native GPU, please follow the instructions in <a class="reference internal" href="install/gpu.html"><span class="doc">GPU Drivers and SDKs</span></a> tutorial to prepare the environment.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://mlc.ai/blog/img/redpajama/cli.gif"><img alt="https://mlc.ai/blog/img/redpajama/cli.gif" src="https://mlc.ai/blog/img/redpajama/cli.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">MLC LLM on CLI</span><a class="headerlink" href="#id4" title="Permalink to this image">¬∂</a></p>
</figcaption>
</figure>
</div><div aria-labelledby="tab-0-0-3" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-3" name="0-3" role="tabpanel" tabindex="0"><p>With the advancements of WebGPU, we can now run LLM directly on web browsers. You have the opportunity to experience the web version of MLC LLM through <a class="reference external" href="https://mlc.ai/webllm">WebLLM</a>.</p>
<p>Once the parameters have been fetched and stored in the local cache, you can begin interacting with the model without the need for an internet connection.</p>
<p>You can use <a class="reference external" href="https://webgpureport.org/">WebGPU Report</a> to verify the functionality of WebGPU on your browser.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://mlc.ai/blog/img/redpajama/web.gif"><img alt="https://mlc.ai/blog/img/redpajama/web.gif" src="https://mlc.ai/blog/img/redpajama/web.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">MLC LLM on Web</span><a class="headerlink" href="#id5" title="Permalink to this image">¬∂</a></p>
</figcaption>
</figure>
</div></div>
</section>
<section id="project-overview">
<h2>Project Overview<a class="headerlink" href="#project-overview" title="Permalink to this heading">¬∂</a></h2>
<p>The project consists of three distinct submodules: model definition, model compilation, and runtimes.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="_images/project-structure.svg"><img alt="Project Structure" src="_images/project-structure.svg" width="600" /></a>
<figcaption>
<p><span class="caption-text">Three independent submodules in MLC LLM</span><a class="headerlink" href="#id6" title="Permalink to this image">¬∂</a></p>
</figcaption>
</figure>
<p><strong>‚ûÄ Model definition in Python.</strong> MLC offers a variety of pre-defined architectures, such as Llama (e.g., Vicuna, OpenLlama, Llama, Wizard), GPT-NeoX (e.g., RedPajama, Dolly), RNNs (e.g., RWKV), and GPT-J (e.g., MOSS). Model developers could solely define the model in pure Python, without having to touch code generation and runtime.</p>
<p><strong>‚ûÅ Model compilation in Python.</strong> <a class="reference internal" href="install/tvm.html"><span class="doc">TVM Unity</span></a> compiler are configured in pure python, and it quantizes and exports the Python-based model to <a class="reference internal" href="tutorials/runtime/terminologies.html#model-lib"><span class="std std-ref">model lib</span></a> and quantized <a class="reference internal" href="tutorials/runtime/terminologies.html#model-weights"><span class="std std-ref">model weights</span></a>. Quantization and optimization algorithms can be developed in pure Python to compress and accelerate LLMs for specific usecases.</p>
<p><strong>‚ûÇ Platform-native runtimes.</strong> Variants of MLCChat are provided on each platform: <strong>C++</strong> for command line, <strong>Javascript</strong> for web, <strong>Swift</strong> for iOS, and <strong>Java</strong> for Android, configurable with a JSON <a class="reference internal" href="tutorials/runtime/terminologies.html#chat-config"><span class="std std-ref">chat config</span></a>. App developers only need to familiarize with the platform-naive runtimes to integrate MLC-compiled LLMs into their projects.</p>
</section>
<section id="customize-mlc-chat-configuration">
<h2>Customize MLC-Chat Configuration<a class="headerlink" href="#customize-mlc-chat-configuration" title="Permalink to this heading">¬∂</a></h2>
<p>The behavior of the chat can be customized by modifying the chat configuration file. To learn more about customizing the chat configuration JSON, you can refer to the following tutorials which provide a detailed walkthrough:</p>
<ul class="simple">
<li><p><a class="reference internal" href="tutorials/runtime/mlc_chat_config.html"><span class="doc">Configure MLCChat in JSON</span></a></p></li>
</ul>
</section>
<section id="model-prebuilts">
<h2>Model Prebuilts<a class="headerlink" href="#model-prebuilts" title="Permalink to this heading">¬∂</a></h2>
<p>To use different pre-built models, you can refer to the following tutorials:</p>
<ul class="simple">
<li><p><a class="reference internal" href="tutorials/prebuilts/prebuilt_models.html"><span class="doc">üöß Model Prebuilts</span></a></p></li>
</ul>
</section>
<section id="misc">
<h2>Misc<a class="headerlink" href="#misc" title="Permalink to this heading">¬∂</a></h2>
<p>If you find MLC LLM useful in your work, please consider citing the project using the following format:</p>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@software</span><span class="p">{</span><span class="nl">mlc-llm</span><span class="p">,</span>
<span class="w">   </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{MLC team}</span><span class="p">,</span>
<span class="w">   </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{{MLC-LLM}}</span><span class="p">,</span>
<span class="w">   </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/mlc-ai/mlc-llm}</span><span class="p">,</span>
<span class="w">   </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2023}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The underlying compiler techniques employed by MLC LLM are outlined in the following papers:</p>
<details class="summary-references-click-to-expand">
<summary>References (Click to expand)</summary><div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tensorir</span><span class="p">,</span>
<span class="w">   </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Feng, Siyuan and Hou, Bohan and Jin, Hongyi and Lin, Wuwei and Shao, Junru and Lai, Ruihang and Ye, Zihao and Zheng, Lianmin and Yu, Cody Hao and Yu, Yong and Chen, Tianqi}</span><span class="p">,</span>
<span class="w">   </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{TensorIR: An Abstraction for Automatic Tensorized Program Optimization}</span><span class="p">,</span>
<span class="w">   </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">   </span><span class="na">isbn</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{9781450399166}</span><span class="p">,</span>
<span class="w">   </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
<span class="w">   </span><span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{New York, NY, USA}</span><span class="p">,</span>
<span class="w">   </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://doi.org/10.1145/3575693.3576933}</span><span class="p">,</span>
<span class="w">   </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.1145/3575693.3576933}</span><span class="p">,</span>
<span class="w">   </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2}</span><span class="p">,</span>
<span class="w">   </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{804‚Äì817}</span><span class="p">,</span>
<span class="w">   </span><span class="na">numpages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{14}</span><span class="p">,</span>
<span class="w">   </span><span class="na">keywords</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Tensor Computation, Machine Learning Compiler, Deep Neural Network}</span><span class="p">,</span>
<span class="w">   </span><span class="na">location</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Vancouver, BC, Canada}</span><span class="p">,</span>
<span class="w">   </span><span class="na">series</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{ASPLOS 2023}</span>
<span class="p">}</span>

<span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">metaschedule</span><span class="p">,</span>
<span class="w">   </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shao, Junru and Zhou, Xiyou and Feng, Siyuan and Hou, Bohan and Lai, Ruihang and Jin, Hongyi and Lin, Wuwei and Masuda, Masahiro and Yu, Cody Hao and Chen, Tianqi}</span><span class="p">,</span>
<span class="w">   </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
<span class="w">   </span><span class="na">editor</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh}</span><span class="p">,</span>
<span class="w">   </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{35783--35796}</span><span class="p">,</span>
<span class="w">   </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
<span class="w">   </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Tensor Program Optimization with Probabilistic Programs}</span><span class="p">,</span>
<span class="w">   </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://proceedings.neurips.cc/paper_files/paper/2022/file/e894eafae43e68b4c8dfdacf742bcbf3-Paper-Conference.pdf}</span><span class="p">,</span>
<span class="w">   </span><span class="na">volume</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{35}</span><span class="p">,</span>
<span class="w">   </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span>
<span class="p">}</span>

<span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tvm</span><span class="p">,</span>
<span class="w">   </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Tianqi Chen and Thierry Moreau and Ziheng Jiang and Lianmin Zheng and Eddie Yan and Haichen Shen and Meghan Cowan and Leyuan Wang and Yuwei Hu and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy}</span><span class="p">,</span>
<span class="w">   </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{{TVM}: An Automated {End-to-End} Optimizing Compiler for Deep Learning}</span><span class="p">,</span>
<span class="w">   </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)}</span><span class="p">,</span>
<span class="w">   </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2018}</span><span class="p">,</span>
<span class="w">   </span><span class="na">isbn</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{978-1-939133-08-3}</span><span class="p">,</span>
<span class="w">   </span><span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Carlsbad, CA}</span><span class="p">,</span>
<span class="w">   </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{578--594}</span><span class="p">,</span>
<span class="w">   </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://www.usenix.org/conference/osdi18/presentation/chen}</span><span class="p">,</span>
<span class="w">   </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{USENIX Association}</span><span class="p">,</span>
<span class="w">   </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nv">oct</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<p>If you are interested in using Machine Learning Compilation in practice, we highly recommend the following course:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://mlc.ai/">Machine Learning Compilation</a></p></li>
</ul>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tutorials/runtime/mlc_chat_config.html" class="btn btn-neutral float-right" title="Configure MLCChat in JSON" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
    </div>

<div id="button" class="backtop"><img src="_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">¬© 2022-2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>






<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Project Overview &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction to MLC LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deploy/javascript.html">WebLLM and JavaScript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/rest.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/cli.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/python_engine.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/android.html">Android App</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/ide_integration.html">Code Completion IDE Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/mlc_chat_config.html">Customize MLC Config File in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/convert_weights.html">Convert Weights via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Model Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/define_new_models.html">Define New Model Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/configure_quantization.html">üöß Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Prebuilts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Project Overview</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/get_started/project_overview.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="project-overview">
<span id="id1"></span><h1>Project Overview<a class="headerlink" href="#project-overview" title="Permalink to this heading">¬∂</a></h1>
<p>This page introduces high-level project concepts to help us use and customize MLC LLM.
The MLC-LLM project consists of three distinct submodules: model definition, model compilation, and runtimes.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../_images/project-structure.svg"><img alt="Project Structure" src="../_images/project-structure.svg" width="600" /></a>
<figcaption>
<p><span class="caption-text">Three independent submodules in MLC LLM</span><a class="headerlink" href="#id3" title="Permalink to this image">¬∂</a></p>
</figcaption>
</figure>
<p><strong>‚ûÄ Model definition in Python.</strong> MLC offers a variety of pre-defined architectures, such as Llama (e.g., Llama2, Vicuna, OpenLlama, Wizard), GPT-NeoX (e.g., RedPajama, Dolly), RNNs (e.g., RWKV), and GPT-J (e.g., MOSS). Model developers could solely define the model in pure Python, without having to touch code generation and runtime.</p>
<p><strong>‚ûÅ Model compilation in Python.</strong> Models are compiled by <a class="reference internal" href="../install/tvm.html"><span class="doc">TVM Unity</span></a> compiler, where the compilation is configured in pure Python. MLC LLM quantizes and exports the Python-based model to a model library and quantized model weights. Quantization and optimization algorithms can be developed in pure Python to compress and accelerate LLMs for specific usecases.</p>
<p><strong>‚ûÇ Platform-native runtimes.</strong> Variants of MLCChat are provided on each platform: <strong>C++</strong> for command line, <strong>Javascript</strong> for web, <strong>Swift</strong> for iOS, and <strong>Java</strong> for Android, configurable with a JSON chat config. App developers only need to familiarize with the platform-naive runtimes to integrate MLC-compiled LLMs into their projects.</p>
<section id="terminologies">
<span id="id2"></span><h2>Terminologies<a class="headerlink" href="#terminologies" title="Permalink to this heading">¬∂</a></h2>
<p>It is helpful for us to familiarize the basic terminologies used in the MLC chat applications. Below are the
three things you need to run a model with MLC.</p>
<ul class="simple">
<li><p><strong>model lib</strong>: The model library refers to the executable libraries that enable
the execution of a specific model architecture. On Linux and M-chip macOS, these libraries have the suffix
<code class="docutils literal notranslate"><span class="pre">.so</span></code>; on intel macOS, the suffix is <code class="docutils literal notranslate"><span class="pre">.dylib</span></code>; on Windows, the library file ends with <code class="docutils literal notranslate"><span class="pre">.dll</span></code>;
on web browser, the library suffix is <code class="docutils literal notranslate"><span class="pre">.wasm</span></code>. (see <a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs">binary-mlc-llm-libs</a>).</p></li>
<li><p><strong>model weights</strong>: The model weight is a folder that contains the quantized neural network weights
of the language models as well as the tokenizer configurations. (e.g. <a class="reference external" href="https://huggingface.co/mlc-ai/Llama-2-7b-chat-hf-q4f16_1-MLC">Llama-2-7b-chat-hf-q4f16_1-MLC</a>)</p></li>
<li><p><strong>chat config</strong>: The chat configuration includes settings that allow customization of parameters such as temperature and system prompt.
The default chat config usually resides in the same directory as model weights. (e.g. see <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>‚Äôs
<a class="reference external" href="https://huggingface.co/mlc-ai/Llama-2-7b-chat-hf-q4f16_1-MLC/blob/main/mlc-chat-config.json">mlc-chat-config.json</a>)</p></li>
</ul>
</section>
<section id="model-preparation">
<h2>Model Preparation<a class="headerlink" href="#model-preparation" title="Permalink to this heading">¬∂</a></h2>
<p>There are several ways to prepare the model weights and model lib.</p>
<ul class="simple">
<li><p><a class="reference internal" href="../prebuilt_models.html#model-prebuilts"><span class="std std-ref">Model Prebuilts</span></a> contains models that can be directly used.</p></li>
<li><p>You can also <a class="reference internal" href="../compilation/compile_models.html"><span class="doc">run model compilation</span></a> for model weight variants for given supported architectures.</p></li>
<li><p>Finally, you can incorporate a new model architecture/inference logic following <a class="reference internal" href="../compilation/define_new_models.html"><span class="doc">Define New Models</span></a>.</p></li>
</ul>
<p>A default chat config usually comes with the model weight directory. You can further customize
the system prompt, temperature, and other options by modifying the JSON file.
MLC chat runtimes also provide API to override these options during model reload.
Please refer to <a class="reference internal" href="../deploy/mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Customize MLC Config File in JSON</span></a> for more details.</p>
</section>
<section id="runtime-flow-overview">
<h2>Runtime Flow Overview<a class="headerlink" href="#runtime-flow-overview" title="Permalink to this heading">¬∂</a></h2>
<p>Once the model weights, model library, and chat configuration are prepared, an MLC chat runtime can consume them as an engine to drive a chat application.
The diagram below shows a typical workflow for a MLC chat application.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/a05d4598bae6eb5a3133652d5cc0323ced3b0e17/images/mlc-llm/tutorials/mlc-llm-flow-slm.svg"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/a05d4598bae6eb5a3133652d5cc0323ced3b0e17/images/mlc-llm/tutorials/mlc-llm-flow-slm.svg" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/a05d4598bae6eb5a3133652d5cc0323ced3b0e17/images/mlc-llm/tutorials/mlc-llm-flow-slm.svg" width="90%" /></a>
<p>On the right side of the figure, you can see pseudo-code illustrating the structure of an MLC chat API during the execution of a chat app.
Typically, there is a <code class="docutils literal notranslate"><span class="pre">ChatModule</span></code> that manages the model. We instantiate the chat app with two files: the model weights (which include an <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>)
and the model library. We also have an optional chat configuration, which allows for overriding settings such as the system prompt and temperature.</p>
<p>All MLC runtimes, including iOS, Web, CLI, and others, use these three elements.
All the runtime can read the same model weight folder. The packaging of the model libraries may vary depending on the runtime.
For the CLI, the model libraries are stored in a DLL directory.
iOS and Android include pre-packaged model libraries within the app due to dynamic loading restrictions.
WebLLM utilizes URLs of local or Internet-hosted WebAssembly (Wasm) files.</p>
</section>
<section id="what-to-do-next">
<h2>What to Do Next<a class="headerlink" href="#what-to-do-next" title="Permalink to this heading">¬∂</a></h2>
<p>Thank you for reading and learning the high-level concepts.
Moving next, feel free to check out documents on the left navigation panel and
learn about topics you are interested in.</p>
<ul class="simple">
<li><p><a class="reference internal" href="../deploy/mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Customize MLC Config File in JSON</span></a> shows how to configure specific chat behavior.</p></li>
<li><p>Build and Deploy App section contains guides to build apps
and platform-specific MLC chat runtimes.</p></li>
<li><p>Compile models section provides guidelines to convert model weights and produce model libs.</p></li>
</ul>
</section>
</section>


           </div>
           
          </div>
          

<footer>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">¬© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
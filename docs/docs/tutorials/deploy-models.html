





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>How to Deploy Models &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/tabs.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="How to Compile Models" href="compile-models.html" />
    <link rel="prev" title="Frequently Asked Questions (FAQ)" href="../community/faq.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://mlc.ai/mlc-llm>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/blog>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/web-llm>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/blog>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/web-llm>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/mlcchat_terminologies.html">üöß Terminologies</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Use Compiled Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime/cpp.html">Run Models in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime/javascript.html">üöß Run Models in JavaScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime/android.html">üöß Run Models in Android</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime/ios.html">üöß Run Models in iOS</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime/python.html">üöß Run Models with Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="compilation/model_compilation_walkthrough.html">üöß Compile a Model in MLC LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="compilation/compiler_artifacts.html">üöß Compiler Artifact Spec</a></li>
<li class="toctree-l1"><a class="reference internal" href="compilation/configure_targets.html">üöß Configure Targets</a></li>
<li class="toctree-l1"><a class="reference internal" href="compilation/configure_quantization.html">üöß Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="customize/define_new_models.html">üöß Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="prebuilts/prebuilt_models.html">üöß Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation and Dependency</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions (FAQ)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other tutorials</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">How to Deploy Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#clone-the-mlc-llm-repository">Clone the MLC LLM Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="#get-to-know-model-s-local-id">Get to Know Model‚Äôs Local ID</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prepare-model-weight-and-library">Prepare Model Weight and Library</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#upload-the-model-you-built-to-internet">Upload the Model You Built to Internet</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deploy-models-on-your-laptop-desktop">Deploy Models on Your Laptop/Desktop</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#run-the-models-through-cli">Run the Models Through CLI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deploy-models-on-your-iphone-ipad">Deploy Models on Your iPhone/iPad</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#option-1-download-released-app">Option 1. Download Released App</a></li>
<li class="toctree-l3"><a class="reference internal" href="#option-2-build-application-from-source">Option 2. Build Application from Source</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#update-ios-app-config">Update iOS App Config</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-library-weight-preparation-scripts">Run Library/Weight Preparation Scripts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#open-xcode-to-build-the-app">Open Xcode to Build the App</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#run-the-model-you-built">Run the Model You Built</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deploy-model-on-your-android-phone">Deploy Model on Your Android Phone</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#download-released-app">Download Released App</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deploy-models-on-your-web-browser">Deploy Models on Your Web Browser</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="compile-models.html">How to Compile Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="bring-your-own-models.html">Add New Model Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="customize-conversation.html">How to Customize Conversation</a></li>
<li class="toctree-l1"><a class="reference internal" href="customize.html">Customize Model Compilation and Optimization</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>How to Deploy Models</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/tutorials/deploy-models.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="how-to-deploy-models">
<span id="id1"></span><h1>How to Deploy Models<a class="headerlink" href="#how-to-deploy-models" title="Permalink to this heading">¬∂</a></h1>
<p>In this tutorial, we will guide you on how to <strong>deploy</strong> models built by MLC LLM to different backends and devices.
We support four options of device categories for deployment: laptop/desktop, web browser, iPhone/iPad, and Android phones.</p>
<p>This page contains the following sections.
We first introduce how to prepare the (pre)built model libraries and weights, and one section for each backend device will then follow.</p>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#clone-the-mlc-llm-repository" id="id4">Clone the MLC LLM Repository</a></p></li>
<li><p><a class="reference internal" href="#get-to-know-model-s-local-id" id="id5">Get to Know Model‚Äôs Local ID</a></p></li>
<li><p><a class="reference internal" href="#prepare-model-weight-and-library" id="id6">Prepare Model Weight and Library</a></p></li>
<li><p><a class="reference internal" href="#deploy-models-on-your-laptop-desktop" id="id7">Deploy Models on Your Laptop/Desktop</a></p></li>
<li><p><a class="reference internal" href="#deploy-models-on-your-iphone-ipad" id="id8">Deploy Models on Your iPhone/iPad</a></p></li>
<li><p><a class="reference internal" href="#deploy-model-on-your-android-phone" id="id9">Deploy Model on Your Android Phone</a></p></li>
<li><p><a class="reference internal" href="#deploy-models-on-your-web-browser" id="id10">Deploy Models on Your Web Browser</a></p></li>
</ul>
</nav>
<section id="clone-the-mlc-llm-repository">
<span id="clone-repo"></span><h2><a class="toc-backref" href="#id4" role="doc-backlink">Clone the MLC LLM Repository</a><a class="headerlink" href="#clone-the-mlc-llm-repository" title="Permalink to this heading">¬∂</a></h2>
<p>If you haven‚Äôt already cloned the MLC-LLM repository, now is the perfect time to do so.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>git@github.com:mlc-ai/mlc-llm.git<span class="w"> </span>--recursive
<span class="nb">cd</span><span class="w"> </span>mlc-llm
</pre></div>
</div>
</section>
<section id="get-to-know-model-s-local-id">
<span id="knowing-local-id"></span><h2><a class="toc-backref" href="#id5" role="doc-backlink">Get to Know Model‚Äôs Local ID</a><a class="headerlink" href="#get-to-know-model-s-local-id" title="Permalink to this heading">¬∂</a></h2>
<p>In MLC LLM, we use <strong>local ids</strong> to denote the models we build.</p>
<p>A model‚Äôs local id is in the format <code class="docutils literal notranslate"><span class="pre">MODELNAME-QUANTMODE</span></code> (for example, <code class="docutils literal notranslate"><span class="pre">vicuna-v1-7b-q3f16_0</span></code>, <code class="docutils literal notranslate"><span class="pre">RedPajama-INCITE-Chat-3B-v1-q4f16_0</span></code>, etc.).
You can find the local id by checking the directory names under <code class="docutils literal notranslate"><span class="pre">dist</span></code> (if you built model on your own) or <code class="docutils literal notranslate"><span class="pre">dist/prebuilt</span></code> if you use prebuilt models:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check id for models manually built.</span>
~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist
RedPajama-INCITE-Chat-3B-v1-q4f16_0<span class="w">     </span>models<span class="w">                </span>vicuna-v1-7b-q3f16_0
RedPajama-INCITE-Chat-3B-v1-q4f32_0<span class="w">     </span>prebuilt<span class="w">              </span>vicuna-v1-7b-q4f32_0

<span class="c1"># Check id for prebuilt models.</span>
<span class="c1"># Note: Local ids start with the model name after `mlc-chat-`.</span>
~<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/prebuilt
mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_0<span class="w">    </span>mlc-chat-vicuna-v1-7b-q3f16_0
</pre></div>
</div>
<p>We will use local ids for model deployment.</p>
</section>
<section id="prepare-model-weight-and-library">
<span id="prepare-weight-library"></span><h2><a class="toc-backref" href="#id6" role="doc-backlink">Prepare Model Weight and Library</a><a class="headerlink" href="#prepare-model-weight-and-library" title="Permalink to this heading">¬∂</a></h2>
<p>In order to load the model correctly in deployment, we need to put the model libraries and weights to the right location before deployment.
You can select from the panel below for the preparation steps you will need.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Desktop/laptop</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">iPhone/iPad/Android phones</button><button aria-controls="panel-0-0-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-2" name="0-2" role="tab" tabindex="-1">Web browser</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-1-1-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-1-1-0" name="1-0" role="tab" tabindex="0">Use prebuilt model</button><button aria-controls="panel-1-1-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-1" name="1-1" role="tab" tabindex="-1">Use model you built</button></div><div aria-labelledby="tab-1-1-0" class="sphinx-tabs-panel" id="panel-1-1-0" name="1-0" role="tabpanel" tabindex="0"><p>MLC LLM provides a list of prebuilt models (check our <span class="xref std std-doc">/model-prebuilts</span> for the list).
If you want to use them, run the commands below under the root directory of MLC LLM to download the libraries and weights to the target location.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make sure you have installed git-lfs.</span>
mkdir<span class="w"> </span>-p<span class="w"> </span>dist/prebuilt
<span class="nb">cd</span><span class="w"> </span>dist/prebuilt
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mlc-ai/binary-mlc-llm-libs.git<span class="w"> </span>lib

<span class="c1"># Choose one or both commands from below according to the model(s) you want to deploy.</span>
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mlc-ai/mlc-chat-vicuna-v1-7b-q3f16_0
<span class="c1"># and/or</span>
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_0
</pre></div>
</div>
</div><div aria-labelledby="tab-1-1-1" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-1" name="1-1" role="tabpanel" tabindex="0"><p>You are all good. There is no further action to prepare the model libraries and weights.</p>
</div></div>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-2-2-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-2-2-0" name="2-0" role="tab" tabindex="0">Use prebuilt model</button><button aria-controls="panel-2-2-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-1" name="2-1" role="tab" tabindex="-1">Use model you built</button></div><div aria-labelledby="tab-2-2-0" class="sphinx-tabs-panel" id="panel-2-2-0" name="2-0" role="tabpanel" tabindex="0"><p>Please direct to the <a class="reference internal" href="#iphone-download-app"><span class="std std-ref">iOS/iPadOS app download</span></a> and/or the <a class="reference internal" href="#android-download-app"><span class="std std-ref">Android app download</span></a>.
This allows you to use the prebuilt models in the application.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To build the iOS/iPadOS/Android app from source, you need first build the model manually.
Please refer to <a class="reference internal" href="compile-models.html#how-to-compile-models"><span class="std std-ref">the model build tutorial</span></a>, build the model, and then come back to this page.</p>
</div>
</div><div aria-labelledby="tab-2-2-1" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-1" name="2-1" role="tabpanel" tabindex="0"><p>If you want to deploy the model you built on mobile devices via the released iOS/Android app, please go through the section about <a class="reference internal" href="#upload-model"><span class="std std-ref">uploading your model to the Internet</span></a>.</p>
<p>If you want to build the iOS/Android app on your own, the upload action is optional.
The ‚Äúbuild application from source‚Äù sections for iPhone/iPad and Android provide more detailed instructions.</p>
</div></div>
</div><div aria-labelledby="tab-0-0-2" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-2" name="0-2" role="tabpanel" tabindex="0"><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-3-3-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-3-3-0" name="3-0" role="tab" tabindex="0">Use prebuilt model</button><button aria-controls="panel-3-3-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-1" name="3-1" role="tab" tabindex="-1">Use model you built</button></div><div aria-labelledby="tab-3-3-0" class="sphinx-tabs-panel" id="panel-3-3-0" name="3-0" role="tabpanel" tabindex="0"><p>TBA.</p>
</div><div aria-labelledby="tab-3-3-1" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-1" name="3-1" role="tabpanel" tabindex="0"><p>TBA.</p>
</div></div>
</div></div>
<section id="upload-the-model-you-built-to-internet">
<span id="upload-model"></span><h3>Upload the Model You Built to Internet<a class="headerlink" href="#upload-the-model-you-built-to-internet" title="Permalink to this heading">¬∂</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you only want to deploy the model to your desktop/laptop, you can skip this section.</p>
</div>
<p>When you want to deploy the model built by yourself on mobile devices and/or web browser, but do not want to build the iOS/Android/web application on your own, you need to upload the model you built to the Internet (for example, as a repository of Hugging Face), so that the applications released by MLC LLM can download your model from the Internet location.</p>
<p>This section introduces how to prepare and upload the model you built.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before proceeding, you should first have the model built manually.
At this moment, the iOS/Android/web app released by MLC LLM only support <strong>specific model architectures with specific quantization modes</strong>. Particularly,</p>
<ul class="simple">
<li><p>the <a class="reference internal" href="#iphone-download-app"><span class="std std-ref">released iOS/iPadOS app</span></a> supports models structured by LLaMA-7B and quantized by <code class="docutils literal notranslate"><span class="pre">q3f16_0</span></code>, and models structured by GPT-NeoX-3B and quantized by <code class="docutils literal notranslate"><span class="pre">q4f16_0</span></code>.</p></li>
<li><p>the <a class="reference internal" href="#android-download-app"><span class="std std-ref">released Android app</span></a> supports models structured by LLaMA-7B and quantized by <code class="docutils literal notranslate"><span class="pre">q4f16_0</span></code>.</p></li>
<li><p>the <a class="reference external" href="https://mlc.ai/web-llm/">Web LLM demo page</a> supports models structured by LLaMA-7B and quantized by <code class="docutils literal notranslate"><span class="pre">q4f32_0</span></code>, and models structured by GPT-NeoX-3B and quantized by both <code class="docutils literal notranslate"><span class="pre">q4f16_0</span></code> and <code class="docutils literal notranslate"><span class="pre">q4f32_0</span></code>.</p></li>
</ul>
<p>If you have not built the model with supported quantization mode(s), please refer to <a class="reference internal" href="compile-models.html#how-to-compile-models"><span class="std std-ref">the model build tutorial</span></a>, build the model with supported quantization modes, and then come back to this page.</p>
</div>
<p>Assume you have built the model with <a class="reference internal" href="#knowing-local-id"><span class="std std-ref">local id</span></a> <code class="docutils literal notranslate"><span class="pre">MODELNAME-QUANTMODE</span></code>, the directory you need to upload finally will be <code class="docutils literal notranslate"><span class="pre">dist/MODELNAME-QUANTMODE/params</span></code>.
Before uploading, we need to update <code class="docutils literal notranslate"><span class="pre">dist/MODELNAME-QUANTMODE/params/mlc-chat-config.json</span></code>.</p>
<p>Opening that file, the <code class="docutils literal notranslate"><span class="pre">model_lib</span></code> field specifies the model library name we use when deploying this model. Please update this field according to the panel selection below.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-4-4-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-4-4-0" name="4-0" role="tab" tabindex="0">iPhone/iPad</button><button aria-controls="panel-4-4-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-1" name="4-1" role="tab" tabindex="-1">Android</button><button aria-controls="panel-4-4-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-2" name="4-2" role="tab" tabindex="-1">Web</button></div><div aria-labelledby="tab-4-4-0" class="sphinx-tabs-panel" id="panel-4-4-0" name="4-0" role="tabpanel" tabindex="0"><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-5-5-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-5-5-0" name="5-0" role="tab" tabindex="0">Model arch: LLaMA-7B</button><button aria-controls="panel-5-5-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-5-5-1" name="5-1" role="tab" tabindex="-1">GPT-NeoX-3B</button><button aria-controls="panel-5-5-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-5-5-2" name="5-2" role="tab" tabindex="-1">RWKV</button></div><div aria-labelledby="tab-5-5-0" class="sphinx-tabs-panel" id="panel-5-5-0" name="5-0" role="tabpanel" tabindex="0"><p>The model is expected to be quantized by <code class="docutils literal notranslate"><span class="pre">q3f16_0</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;model_lib&quot;</span><span class="p">:</span> <span class="s2">&quot;vicuna-v1-7b-q3f16_0&quot;</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-5-5-1" class="sphinx-tabs-panel" hidden="true" id="panel-5-5-1" name="5-1" role="tabpanel" tabindex="0"><p>The model is expected to be quantized by <code class="docutils literal notranslate"><span class="pre">q4f16_0</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;model_lib&quot;</span><span class="p">:</span> <span class="s2">&quot;RedPajama-INCITE-Chat-3B-v1-q4f16_0&quot;</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-5-5-2" class="sphinx-tabs-panel" hidden="true" id="panel-5-5-2" name="5-2" role="tabpanel" tabindex="0"><p>The model is expected to be quantized by <code class="docutils literal notranslate"><span class="pre">q8f16_0</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;model_lib&quot;</span><span class="p">:</span> <span class="s2">&quot;rwkv-raven-1b5-q8f16_0&quot;</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
</div></div>
</div><div aria-labelledby="tab-4-4-1" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-1" name="4-1" role="tabpanel" tabindex="0"><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-6-6-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-6-6-0" name="6-0" role="tab" tabindex="0">Model arch: LLaMA-7B</button></div><div aria-labelledby="tab-6-6-0" class="sphinx-tabs-panel" id="panel-6-6-0" name="6-0" role="tabpanel" tabindex="0"><p>The model is expected to be quantized by <code class="docutils literal notranslate"><span class="pre">q4f16_0</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;model_lib&quot;</span><span class="p">:</span> <span class="s2">&quot;vicuna-v1-7b-q4f16_0&quot;</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
</div></div>
</div><div aria-labelledby="tab-4-4-2" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-2" name="4-2" role="tabpanel" tabindex="0"><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-7-7-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-7-7-0" name="7-0" role="tab" tabindex="0">Model arch: LLaMA-7B</button><button aria-controls="panel-7-7-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-7-7-1" name="7-1" role="tab" tabindex="-1">GPT-NeoX-3B</button></div><div aria-labelledby="tab-7-7-0" class="sphinx-tabs-panel" id="panel-7-7-0" name="7-0" role="tabpanel" tabindex="0"><p>The model is expected to be quantized by <code class="docutils literal notranslate"><span class="pre">q4f32_0</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;model_lib&quot;</span><span class="p">:</span> <span class="s2">&quot;vicuna-v1-7b-q4f32_0&quot;</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-7-7-1" class="sphinx-tabs-panel" hidden="true" id="panel-7-7-1" name="7-1" role="tabpanel" tabindex="0"><p>If the model is quantized by <code class="docutils literal notranslate"><span class="pre">q4f16_0</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;model_lib&quot;</span><span class="p">:</span> <span class="s2">&quot;RedPajama-INCITE-Chat-3B-v1-q4f16_0&quot;</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Or if the model is quantized by <code class="docutils literal notranslate"><span class="pre">q4f32_0</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;model_lib&quot;</span><span class="p">:</span> <span class="s2">&quot;RedPajama-INCITE-Chat-3B-v1-q4f32_0&quot;</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
</div></div>
</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The necessity of updating <code class="docutils literal notranslate"><span class="pre">model_lib</span></code> is due to the app build restriction. For app development, developers are required to pack all libraries into the app at the time of app build. So the iOS/Android app released by MLC LLM and the Web LLM demo page now only contain the libraries for model architecture LLaMA-7B and GPT-NeoX-3B.</p>
</div>
<p>For other fields in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, you can choose to update those configurable parameters (e.g., <code class="docutils literal notranslate"><span class="pre">temperature</span></code>, <code class="docutils literal notranslate"><span class="pre">top_p</span></code>, etc.) if you want, to control the behavior of the model text generation. You can refer to documentation for these configurable parameters in <a class="reference external" href="https://huggingface.co/docs/api-inference/detailed_parameters">Hugging Face</a>.</p>
<p>After updating <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, you need to upload the directory <code class="docutils literal notranslate"><span class="pre">dist/MODELNAME-QUANTMODE/params</span></code> (including all its contents) to an Internet location that is publicly accessible. For example, we have uploaded a few prebuilt model weight directory (<a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-vicuna-v1-7b-q3f16_0/tree/main">example 1</a>, <a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f32_0/tree/main">example 2</a>) to Hugging Face.</p>
<p>This concludes the model upload, and you can proceed to the sections below to deploy or run models on your devices.</p>
</section>
</section>
<section id="deploy-models-on-your-laptop-desktop">
<span id="deploy-on-laptop-desktop"></span><h2><a class="toc-backref" href="#id7" role="doc-backlink">Deploy Models on Your Laptop/Desktop</a><a class="headerlink" href="#deploy-models-on-your-laptop-desktop" title="Permalink to this heading">¬∂</a></h2>
<p>This section goes through the process of deploying prebuilt model or the model you built on your laptop or desktop.
MLC LLM provides a Command-Line Interface (CLI) application to deploy and help interact with the model.</p>
<p>Please follow the instructions in <span class="xref std std-ref">install-mlc-chat-cli</span> section to install the CLI application, then you can <a class="reference internal" href="#cli-run-model"><span class="std std-ref">deploy and interact</span></a> with the model on your machine through CLI.</p>
<section id="run-the-models-through-cli">
<span id="cli-run-model"></span><h3>Run the Models Through CLI<a class="headerlink" href="#run-the-models-through-cli" title="Permalink to this heading">¬∂</a></h3>
<p>To run the model, we need to know the <a class="reference internal" href="#knowing-local-id"><span class="std std-ref">local id</span></a> of the model we want to deploy.
After confirming the local id, we can run the model in CLI by</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mlc_chat_cli<span class="w"> </span>--local-id<span class="w"> </span>LOCAL_ID
<span class="c1"># example:</span>
mlc_chat_cli<span class="w"> </span>--local-id<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1-q4f16_0
mlc_chat_cli<span class="w"> </span>--local-id<span class="w"> </span>vicuna-v1-7b-q3f16_0
</pre></div>
</div>
<img alt="https://mlc.ai/blog/img/redpajama/cli.gif" src="https://mlc.ai/blog/img/redpajama/cli.gif" />
</section>
</section>
<section id="deploy-models-on-your-iphone-ipad">
<span id="deploy-on-ios"></span><h2><a class="toc-backref" href="#id8" role="doc-backlink">Deploy Models on Your iPhone/iPad</a><a class="headerlink" href="#deploy-models-on-your-iphone-ipad" title="Permalink to this heading">¬∂</a></h2>
<p>This section introduces how to deploy model you built or prebuilt by us on your iPhone/iPad devices.
The iOS/iPadOS application supports chatting with prebuilt Vicuna or RedPajama models, and also supports using the model you manually built.</p>
<p>MLC LLM has released an iOS/iPadOS application which you can directly <a class="reference internal" href="#iphone-download-app"><span class="std std-ref">download and use</span></a>.
You can also <a class="reference internal" href="#iphone-build-xcode-app"><span class="std std-ref">build the application</span></a> on your own.</p>
<p>Please check the section ‚Äú<a class="reference internal" href="#iphone-deploy-custom-model"><span class="std std-ref">Run the model you built</span></a>‚Äù to run the model you built on iPhone/iPad.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The app needs about 2GB of memory to run RedPajama-v1-3B quantized by <code class="docutils literal notranslate"><span class="pre">q4f16_0</span></code>, and needs about 4GB of memory to run Vicuna-v1-7B quantized by <code class="docutils literal notranslate"><span class="pre">q3f16_0</span></code>.
Due to memory limitation, iPhone models with 4GB RAM may not be able to launch Vicuna-v1-7B successfully.</p>
</div>
<section id="option-1-download-released-app">
<span id="iphone-download-app"></span><h3>Option 1. Download Released App<a class="headerlink" href="#option-1-download-released-app" title="Permalink to this heading">¬∂</a></h3>
<p>Check out the <a class="reference external" href="https://testflight.apple.com/join/57zd7oxa">TestFlight page</a> to install the application for iPhone/iPad.
The link is valid for the first 9000 users.</p>
</section>
<section id="option-2-build-application-from-source">
<span id="iphone-build-xcode-app"></span><h3>Option 2. Build Application from Source<a class="headerlink" href="#option-2-build-application-from-source" title="Permalink to this heading">¬∂</a></h3>
<p>To start building the iOS/iPadOS application, you need first build the model manually.
If you have not built the model, please refer to <a class="reference internal" href="compile-models.html#how-to-compile-models"><span class="std std-ref">the model build tutorial</span></a>, build the model, and then come back to this page.
The following part of this subsection assumes you have built the model successfully and will not cover the model build part.</p>
<section id="update-ios-app-config">
<span id="id2"></span><h4>Update iOS App Config<a class="headerlink" href="#update-ios-app-config" title="Permalink to this heading">¬∂</a></h4>
<p>First, open <code class="docutils literal notranslate"><span class="pre">ios/MLCChat/app-config.json</span></code> and update the file according to the model(s) you build.</p>
<p>In this file, the <code class="docutils literal notranslate"><span class="pre">model_libs</span></code> list contains the local ids of the models we want to deploy on iPhone/iPad.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Due to the requirement of iOS app development, we need to pack all the model libraries into the application when we build the app.
The <code class="docutils literal notranslate"><span class="pre">model_libs</span></code> field specifies the model libraries we pack into the application.</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">model_list</span></code> contains the weight repo URLs and the local ids of the models whose weights are <strong>not</strong> packed into the application when building the app. In the app, the models in <code class="docutils literal notranslate"><span class="pre">model_list</span></code> are downloaded <em>on users‚Äô demands</em>. Please check section ‚Äú<a class="reference internal" href="#upload-model"><span class="std std-ref">Upload the model you built to Internet</span></a>‚Äù for how to upload the model you built to Internet and get the model URL.</p>
<p><code class="docutils literal notranslate"><span class="pre">add_model_samples</span></code> is providing a sample demo for downloading customized model weight with model URL inside the application. It can be left unchanged in the JSON.</p>
<p>For example, the existing <code class="docutils literal notranslate"><span class="pre">app-config.json</span></code> (<a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/ios/MLCChat/app-config.json">here</a>) means</p>
<ul class="simple">
<li><p>we pack the libraries of <code class="docutils literal notranslate"><span class="pre">vicuna-v1-7b-q3f16_0</span></code> and <code class="docutils literal notranslate"><span class="pre">RedPajama-INCITE-Chat-3B-v1-q4f16_0</span></code> into the app, so that the app supports <code class="docutils literal notranslate"><span class="pre">q3f16_0</span></code> quantization of Vicuna-v1-7B model and <code class="docutils literal notranslate"><span class="pre">q4f16_0</span></code> quantization of RedPajama-v1-3B model.</p></li>
<li><p>The weight of <code class="docutils literal notranslate"><span class="pre">vicuna-v1-7b-q3f16_0</span></code> model is not packed into the model by default. It can be downloaded from the Hugging Face repo URL in the app on users‚Äô demands.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><p>By packing the library of <code class="docutils literal notranslate"><span class="pre">vicuna-v1-7b-q3f16_0</span></code> and <code class="docutils literal notranslate"><span class="pre">RedPajama-INCITE-Chat-3B-v1-q4f16_0</span></code> into the app, it means we support the any LLaMA-7B structured model quantized by <code class="docutils literal notranslate"><span class="pre">q3f16_0</span></code> and any GPT-NeoX-3B model quantized by <code class="docutils literal notranslate"><span class="pre">q4f16_0</span></code>.</p></li>
<li><p>For models we want to deploy to iPhone/iPad, if we do not put it in <code class="docutils literal notranslate"><span class="pre">model_list</span></code>, we will need to pack the model weights to the application when building the app. This is done in the following steps. In the example app config, model <code class="docutils literal notranslate"><span class="pre">RedPajama-INCITE-Chat-3B-v1-q4f16_0</span></code> is not put in <code class="docutils literal notranslate"><span class="pre">model_list</span></code>. It means we will pack the weight of <code class="docutils literal notranslate"><span class="pre">RedPajama-INCITE-Chat-3B-v1-q4f16_0</span></code> directly into the app later.</p></li>
</ol>
</div>
</section>
<section id="run-library-weight-preparation-scripts">
<span id="id3"></span><h4>Run Library/Weight Preparation Scripts<a class="headerlink" href="#run-library-weight-preparation-scripts" title="Permalink to this heading">¬∂</a></h4>
<p>After updating <code class="docutils literal notranslate"><span class="pre">app-config.json</span></code>, run <code class="docutils literal notranslate"><span class="pre">prepare_libs.sh</span></code> under <code class="docutils literal notranslate"><span class="pre">ios/</span></code> to generate required libraries.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>ios
./prepare_libs.sh
</pre></div>
</div>
<p>Now we specify the id of the models whose weights <strong>we pack into the app when building</strong>.
Open <code class="docutils literal notranslate"><span class="pre">ios/prepare_params.sh</span></code> and update <code class="docutils literal notranslate"><span class="pre">builtin_list</span></code> with such models‚Äô local ids.
For example, the existing <code class="docutils literal notranslate"><span class="pre">prepare_params.sh</span></code> (<a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/ios/prepare_params.sh#L8-L11">here</a>) means we only pack the weight of <code class="docutils literal notranslate"><span class="pre">RedPajama-INCITE-Chat-3B-v1-q4f16_0</span></code> into the application when building.</p>
<p>Then run <code class="docutils literal notranslate"><span class="pre">prepare_params.sh</span></code> to copy the model weights to the pre-defined target location.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./prepare_params.sh
</pre></div>
</div>
</section>
<section id="open-xcode-to-build-the-app">
<span id="open-xcode-to-build-app"></span><h4>Open Xcode to Build the App<a class="headerlink" href="#open-xcode-to-build-the-app" title="Permalink to this heading">¬∂</a></h4>
<p>The final steps go with <a class="reference external" href="https://developer.apple.com/xcode/">Xcode</a>. Download Xcode and open <code class="docutils literal notranslate"><span class="pre">ios/MLCChat.xcodeproj</span></code> using Xcode.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will need an <a class="reference external" href="https://developer.apple.com/programs/">Apple Developer Account</a> to use Xcode, and you may be prompted to use your own developer team credential and product bundle identifier.</p>
</div>
<p>Once you have made the necessary changes, build the iOS app using Xcode. Make sure to select a target device (requires connecting your device to your Mac via wire) or simulator for the build.</p>
<p>After a successful build, you can run the iOS app on your device or simulator to use the LLM model for text generation and processing.</p>
</section>
</section>
<section id="run-the-model-you-built">
<span id="iphone-deploy-custom-model"></span><h3>Run the Model You Built<a class="headerlink" href="#run-the-model-you-built" title="Permalink to this heading">¬∂</a></h3>
<p>In section ‚Äú<a class="reference internal" href="#upload-model"><span class="std std-ref">Upload the model you built to Internet</span></a>‚Äù, we introduced how to upload the model you built to Internet for applications to download.</p>
<p>If you want to run the model you built, simply follow the steps below.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-8-8-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-8-8-0" name="8-0" role="tab" tabindex="0">Step 1</button><button aria-controls="panel-8-8-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-8-8-1" name="8-1" role="tab" tabindex="-1">Step 2</button><button aria-controls="panel-8-8-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-8-8-2" name="8-2" role="tab" tabindex="-1">Step 3</button><button aria-controls="panel-8-8-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-8-8-3" name="8-3" role="tab" tabindex="-1">Step 4</button></div><div aria-labelledby="tab-8-8-0" class="sphinx-tabs-panel" id="panel-8-8-0" name="8-0" role="tabpanel" tabindex="0"><p>Open ‚ÄúMLCChat‚Äù app, click ‚ÄúAdd model variant‚Äù.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-1.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-1.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-1.png" style="width: 30%;" /></a>
</div><div aria-labelledby="tab-8-8-1" class="sphinx-tabs-panel" hidden="true" id="panel-8-8-1" name="8-1" role="tabpanel" tabindex="0"><p>Paste the repository URL of the model built on your own, and click Add.</p>
<p>You can refer to the link in the image as an example.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-2.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-2.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-2.png" style="width: 30%;" /></a>
</div><div aria-labelledby="tab-8-8-2" class="sphinx-tabs-panel" hidden="true" id="panel-8-8-2" name="8-2" role="tabpanel" tabindex="0"><p>After adding the model, you can download your model from the URL by clicking the download button.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-3.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-3.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-3.png" style="width: 30%;" /></a>
</div><div aria-labelledby="tab-8-8-3" class="sphinx-tabs-panel" hidden="true" id="panel-8-8-3" name="8-3" role="tabpanel" tabindex="0"><p>When the download is finished, click into the model and enjoy.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-4.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-4.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-4.png" style="width: 30%;" /></a>
</div></div>
</section>
</section>
<section id="deploy-model-on-your-android-phone">
<span id="deploy-model-on-android"></span><h2><a class="toc-backref" href="#id9" role="doc-backlink">Deploy Model on Your Android Phone</a><a class="headerlink" href="#deploy-model-on-your-android-phone" title="Permalink to this heading">¬∂</a></h2>
<section id="download-released-app">
<span id="android-download-app"></span><h3>Download Released App<a class="headerlink" href="#download-released-app" title="Permalink to this heading">¬∂</a></h3>
<p>TBA.</p>
</section>
</section>
<section id="deploy-models-on-your-web-browser">
<span id="deploy-model-web-browser"></span><h2><a class="toc-backref" href="#id10" role="doc-backlink">Deploy Models on Your Web Browser</a><a class="headerlink" href="#deploy-models-on-your-web-browser" title="Permalink to this heading">¬∂</a></h2>
<p>TBA.</p>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="compile-models.html" class="btn btn-neutral float-right" title="How to Compile Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../community/faq.html" class="btn btn-neutral float-left" title="Frequently Asked Questions (FAQ)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">¬© 2022 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
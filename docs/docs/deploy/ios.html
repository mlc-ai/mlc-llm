





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>iOS App and Swift API &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Android App" href="android.html" />
    <link rel="prev" title="Python API" href="python_engine.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/intro.html">Introduction to MLC LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="javascript.html">WebLLM and JavaScript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_engine.html">Python API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">iOS App and Swift API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#use-pre-built-ios-app">Use Pre-built iOS App</a></li>
<li class="toctree-l2"><a class="reference internal" href="#build-ios-app-from-source">Build iOS App from Source</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-install-build-dependencies">Step 1. Install Build Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-download-prebuilt-weights-and-library">Step 2. Download Prebuilt Weights and Library</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-build-auxiliary-components">Step 3. Build Auxiliary Components</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-build-ios-app">Step 4. Build iOS App</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#customize-the-app">Customize the App</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bring-your-own-model-variant">Bring Your Own Model Variant</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bring-your-own-model-library">Bring Your Own Model Library</a></li>
<li class="toctree-l2"><a class="reference internal" href="#build-apps-with-mlc-swift-api">Build Apps with MLC Swift API</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android App</a></li>
<li class="toctree-l1"><a class="reference internal" href="ide_integration.html">Code Completion IDE Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlc_chat_config.html">Customize MLC Config File in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/convert_weights.html">Convert Weights via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Model Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/define_new_models.html">Define New Model Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/configure_quantization.html">üöß Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Prebuilts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>iOS App and Swift API</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/ios.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="ios-app-and-swift-api">
<span id="deploy-ios"></span><h1>iOS App and Swift API<a class="headerlink" href="#ios-app-and-swift-api" title="Permalink to this heading">¬∂</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#use-pre-built-ios-app" id="id2">Use Pre-built iOS App</a></p></li>
<li><p><a class="reference internal" href="#build-ios-app-from-source" id="id3">Build iOS App from Source</a></p>
<ul>
<li><p><a class="reference internal" href="#step-1-install-build-dependencies" id="id4">Step 1. Install Build Dependencies</a></p></li>
<li><p><a class="reference internal" href="#step-2-download-prebuilt-weights-and-library" id="id5">Step 2. Download Prebuilt Weights and Library</a></p></li>
<li><p><a class="reference internal" href="#step-3-build-auxiliary-components" id="id6">Step 3. Build Auxiliary Components</a></p></li>
<li><p><a class="reference internal" href="#step-4-build-ios-app" id="id7">Step 4. Build iOS App</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#customize-the-app" id="id8">Customize the App</a></p></li>
<li><p><a class="reference internal" href="#bring-your-own-model-variant" id="id9">Bring Your Own Model Variant</a></p></li>
<li><p><a class="reference internal" href="#bring-your-own-model-library" id="id10">Bring Your Own Model Library</a></p></li>
<li><p><a class="reference internal" href="#build-apps-with-mlc-swift-api" id="id11">Build Apps with MLC Swift API</a></p></li>
</ul>
</nav>
<p>The MLC LLM iOS app can be installed in two ways: through the pre-built package or by building from the source.
If you are an iOS user looking to try out the models, the pre-built package is recommended. If you are a
developer seeking to integrate new features into the package, building the iOS package from the source is required.</p>
<section id="use-pre-built-ios-app">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Use Pre-built iOS App</a><a class="headerlink" href="#use-pre-built-ios-app" title="Permalink to this heading">¬∂</a></h2>
<p>The MLC Chat app is now available in App Store at no cost. You can download and explore it by simply clicking the button below:</p>
<blockquote>
<div><a class="reference external image-reference" href="https://apps.apple.com/us/app/mlc-chat/id6448482937"><img alt="https://developer.apple.com/assets/elements/badges/download-on-the-app-store.svg" src="https://developer.apple.com/assets/elements/badges/download-on-the-app-store.svg" width="135" /></a>
</div></blockquote>
</section>
<section id="build-ios-app-from-source">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Build iOS App from Source</a><a class="headerlink" href="#build-ios-app-from-source" title="Permalink to this heading">¬∂</a></h2>
<p>This section shows how we can build the app from the source.</p>
<section id="step-1-install-build-dependencies">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Step 1. Install Build Dependencies</a><a class="headerlink" href="#step-1-install-build-dependencies" title="Permalink to this heading">¬∂</a></h3>
<p>First and foremost, please clone the <a class="reference external" href="https://github.com/mlc-ai/mlc-llm">MLC LLM GitHub repository</a>.</p>
<p>Please follow <a class="reference internal" href="../install/tvm.html"><span class="doc">Install TVM Unity Compiler</span></a> to install TVM Unity.
Note that we <strong>do not</strong> have to run <cite>build.py</cite> since we can use prebuilt weights.
We only need TVM Unity‚Äôs utility to combine the libraries (<cite>local-id-iphone.tar</cite>) into a single library.</p>
<p>We also need to have the following build dependencies:</p>
<ul class="simple">
<li><p>CMake &gt;= 3.24,</p></li>
<li><p>Git and Git-LFS,</p></li>
<li><p><a class="reference external" href="https://www.rust-lang.org/tools/install">Rust and Cargo</a>, which are required by Hugging Face‚Äôs tokenizer.</p></li>
</ul>
</section>
<section id="step-2-download-prebuilt-weights-and-library">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Step 2. Download Prebuilt Weights and Library</a><a class="headerlink" href="#step-2-download-prebuilt-weights-and-library" title="Permalink to this heading">¬∂</a></h3>
<p>You also need to obtain a copy of the MLC-LLM source code
by cloning the <a class="reference external" href="https://github.com/mlc-ai/mlc-llm">MLC LLM GitHub repository</a>.
To simplify the build, we will use prebuilt model
weights and libraries here. Run the following command
in the root directory of the MLC-LLM.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>dist/prebuilt
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mlc-ai/binary-mlc-llm-libs.git<span class="w"> </span>dist/prebuilt/lib

<span class="nb">cd</span><span class="w"> </span>dist/prebuilt
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mlc-ai/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC
<span class="nb">cd</span><span class="w"> </span>../..
</pre></div>
</div>
<p>Validate that the files and directories exist:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/prebuilt/lib/*/*-iphone.tar
./dist/prebuilt/lib/RedPajama-INCITE-Chat-3B-v1/RedPajama-INCITE-Chat-3B-v1-q4f16_1-iphone.tar
./dist/prebuilt/lib/Mistral-7B-Instruct-v0.2/Mistral-7B-Instruct-v0.2-q3f16_1-iphone.tar
...

&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/prebuilt/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC
<span class="c1"># chat config:</span>
mlc-chat-config.json
<span class="c1"># model weights:</span>
ndarray-cache.json
params_shard_*.bin
...
</pre></div>
</div>
</section>
<section id="step-3-build-auxiliary-components">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Step 3. Build Auxiliary Components</a><a class="headerlink" href="#step-3-build-auxiliary-components" title="Permalink to this heading">¬∂</a></h3>
<p><strong>Tokenizer and runtime</strong></p>
<p>In addition to the model itself, a lightweight runtime and tokenizer are
required to actually run the LLM. You can build and organize these
components by following these steps:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>submodule<span class="w"> </span>update<span class="w"> </span>--init<span class="w"> </span>--recursive
<span class="nb">cd</span><span class="w"> </span>./ios
./prepare_libs.sh
</pre></div>
</div>
<p>This will create a <code class="docutils literal notranslate"><span class="pre">./build</span></code> folder that contains the following files.
Please make sure all the following files exist in <code class="docutils literal notranslate"><span class="pre">./build/</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>./build/lib/
libmlc_llm.a<span class="w">         </span><span class="c1"># A lightweight interface to interact with LLM, tokenizer, and TVM Unity runtime</span>
libmodel_iphone.a<span class="w">    </span><span class="c1"># The compiled model lib</span>
libsentencepiece.a<span class="w">   </span><span class="c1"># SentencePiece tokenizer</span>
libtokenizers_cpp.a<span class="w">  </span><span class="c1"># Huggingface tokenizer</span>
libtvm_runtime.a<span class="w">     </span><span class="c1"># TVM Unity runtime</span>
</pre></div>
</div>
<p><strong>Add prepackage model</strong></p>
<p>We can also <em>optionally</em> add prepackage weights into the app,
run the following command under the <code class="docutils literal notranslate"><span class="pre">./ios</span></code> directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>./ios
open<span class="w"> </span>./prepare_params.sh<span class="w"> </span><span class="c1"># make sure builtin_list only contains &quot;RedPajama-INCITE-Chat-3B-v1-q4f16_1&quot;</span>
./prepare_params.sh
</pre></div>
</div>
<p>The outcome should be as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>./dist/
RedPajama-INCITE-Chat-3B-v1-q4f16_1
</pre></div>
</div>
</section>
<section id="step-4-build-ios-app">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Step 4. Build iOS App</a><a class="headerlink" href="#step-4-build-ios-app" title="Permalink to this heading">¬∂</a></h3>
<p>Open <code class="docutils literal notranslate"><span class="pre">./ios/MLCChat.xcodeproj</span></code> using Xcode. Note that you will need an
Apple Developer Account to use Xcode, and you may be prompted to use
your own developer team credential and product bundle identifier.</p>
<p>Ensure that all the necessary dependencies and configurations are
correctly set up in the Xcode project.</p>
<p>Once you have made the necessary changes, build the iOS app using Xcode.
If you have an Apple Silicon Mac, you can select target ‚ÄúMy Mac (designed for iPad)‚Äù
to run on your Mac. You can also directly run it on your iPad or iPhone.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/xcode-build.jpg"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/xcode-build.jpg" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/xcode-build.jpg" style="width: 60%;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
<section id="customize-the-app">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Customize the App</a><a class="headerlink" href="#customize-the-app" title="Permalink to this heading">¬∂</a></h2>
<p>We can customize the iOS app in several ways.
<a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/ios/MLCChat/app-config.json">MLCChat/app-config.json</a>
controls the list of local and remote models to be packaged into the app, given a local path or a URL respectively. Only models in <code class="docutils literal notranslate"><span class="pre">model_list</span></code> will have their libraries brought into the app when running <cite>./prepare_libs</cite> to package them into <code class="docutils literal notranslate"><span class="pre">libmodel_iphone.a</span></code>. Each model defined in <cite>app-config.json</cite> contain the following fields:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">model_path</span></code></dt><dd><p>(Required if local model) Name of the local folder containing the weights.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">model_url</span></code></dt><dd><p>(Required if remote model) URL to the repo containing the weights.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">model_id</span></code></dt><dd><p>(Required) Unique local identifier to identify the model.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">model_lib</span></code></dt><dd><p>(Required) Matches the system-lib-prefix, generally set during <code class="docutils literal notranslate"><span class="pre">mlc_llm</span> <span class="pre">compile</span></code> which can be specified using
<code class="docutils literal notranslate"><span class="pre">--system-lib-prefix</span></code> argument. By default, it is set to <code class="docutils literal notranslate"><span class="pre">&quot;${model_type}_${quantization}&quot;</span></code> e.g. <code class="docutils literal notranslate"><span class="pre">gpt_neox_q4f16_1</span></code>
for the RedPajama-INCITE-Chat-3B-v1 model. If the <code class="docutils literal notranslate"><span class="pre">--system-lib-prefix</span></code> argument is manually specified during
<code class="docutils literal notranslate"><span class="pre">mlc_llm</span> <span class="pre">compile</span></code>, the <code class="docutils literal notranslate"><span class="pre">model_lib</span></code> field should be updated accordingly.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">required_vram_bytes</span></code></dt><dd><p>(Required) Estimated requirements of VRAM to run the model.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">model_lib_path_for_prepare_libs</span></code></dt><dd><p>(Required) List of paths to the model libraries in the app (respective <code class="docutils literal notranslate"><span class="pre">.tar</span></code> file in the <code class="docutils literal notranslate"><span class="pre">binary-mlc-llm-libs</span></code>
repo, relative path in the <code class="docutils literal notranslate"><span class="pre">dist</span></code> artifact folder or full path to the library). Only used while running
<code class="docutils literal notranslate"><span class="pre">prepare_libs.sh</span></code> to determine which model library to use during runtime. Useful when selecting a library with
different settings (e.g. <code class="docutils literal notranslate"><span class="pre">prefill_chunk_size</span></code>, <code class="docutils literal notranslate"><span class="pre">context_window_size</span></code>, and <code class="docutils literal notranslate"><span class="pre">sliding_window_size</span></code>).</p>
</dd>
</dl>
<p>Additionally, the app prepackages the models under <code class="docutils literal notranslate"><span class="pre">./ios/dist</span></code>.
This built-in list can be controlled by editing <code class="docutils literal notranslate"><span class="pre">prepare_params.sh</span></code>.
You can package new prebuilt models or compiled models by changing the above fields and then repeating the steps above.</p>
</section>
<section id="bring-your-own-model-variant">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Bring Your Own Model Variant</a><a class="headerlink" href="#bring-your-own-model-variant" title="Permalink to this heading">¬∂</a></h2>
<p>In cases where the model you are adding is simply a variant of an existing
model, we only need to convert weights and reuse existing model library. For instance:</p>
<ul class="simple">
<li><p>Adding <code class="docutils literal notranslate"><span class="pre">NeuralHermes</span></code> when MLC already supports the <code class="docutils literal notranslate"><span class="pre">Mistral</span></code> architecture</p></li>
</ul>
<p>In this section, we walk you through adding <code class="docutils literal notranslate"><span class="pre">NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC</span></code> to the MLC iOS app.
According to the model‚Äôs <code class="docutils literal notranslate"><span class="pre">config.json</span></code> on <a class="reference external" href="https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/blob/main/config.json">its Huggingface repo</a>,
it reuses the Mistral model architecture.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This section largely replicates <a class="reference internal" href="../compilation/convert_weights.html#convert-weights-via-mlc"><span class="std std-ref">Convert Weights via MLC</span></a>.
See that page for more details. Note that the weights are shared across
all platforms in MLC.</p>
</div>
<p><strong>Step 1 Clone from HF and convert_weight</strong></p>
<p>You can be under the mlc-llm repo, or your own working directory. Note that all platforms
can share the same compiled/quantized weights. See <a class="reference internal" href="../compilation/compile_models.html#compile-command-specification"><span class="std std-ref">Compile Command Specification</span></a>
for specification of <code class="docutils literal notranslate"><span class="pre">convert_weight</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create directory</span>
mkdir<span class="w"> </span>-p<span class="w"> </span>dist/models<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>dist/models
<span class="c1"># Clone HF weights</span>
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B
<span class="nb">cd</span><span class="w"> </span>../..
<span class="c1"># Convert weight</span>
mlc_llm<span class="w"> </span>convert_weight<span class="w"> </span>./dist/models/NeuralHermes-2.5-Mistral-7B/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC
</pre></div>
</div>
<p><strong>Step 2 Generate MLC Chat Config</strong></p>
<p>Use <code class="docutils literal notranslate"><span class="pre">mlc_llm</span> <span class="pre">gen_config</span></code> to generate <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> and process tokenizers.
See <a class="reference internal" href="../compilation/compile_models.html#compile-command-specification"><span class="std std-ref">Compile Command Specification</span></a> for specification of <code class="docutils literal notranslate"><span class="pre">gen_config</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/NeuralHermes-2.5-Mistral-7B/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q3f16_1<span class="w"> </span>--conv-template<span class="w"> </span>neural_hermes_mistral<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC
</pre></div>
</div>
<p>For the <code class="docutils literal notranslate"><span class="pre">conv-template</span></code>, <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/cpp/conv_templates.cc">conv_template.cc</a>
contains a full list of conversation templates that MLC provides.</p>
<p>If the model you are adding requires a new conversation template, you would need to add your own.
Follow <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/pull/1402">this PR</a> as an example.
We look up the template to use with the <code class="docutils literal notranslate"><span class="pre">conv_template</span></code> field in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p>
<p>For more details, please see <a class="reference internal" href="mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Customize MLC Config File in JSON</span></a>.</p>
<p><strong>Step 3 Upload weights to HF</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># First, please create a repository on Hugging Face.</span>
<span class="c1"># With the repository created, run</span>
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/my-huggingface-account/my-mistral-weight-huggingface-repo
<span class="nb">cd</span><span class="w"> </span>my-mistral-weight-huggingface-repo
cp<span class="w"> </span>path/to/mlc-llm/dist/NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC/*<span class="w"> </span>.
git<span class="w"> </span>add<span class="w"> </span>.<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>git<span class="w"> </span>commit<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;Add mistral model weights&quot;</span>
git<span class="w"> </span>push<span class="w"> </span>origin<span class="w"> </span>main
</pre></div>
</div>
<p>After successfully following all steps, you should end up with a Huggingface repo similar to
<a class="reference external" href="https://huggingface.co/mlc-ai/NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC">NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC</a>,
which includes the converted/quantized weights, the <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, and tokenizer files.</p>
<p><strong>Step 4 Register as a ModelRecord</strong></p>
<p>Finally, we modify the code snippet for
<a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/ios/MLCChat/app-config.json">app-config.json</a>
pasted above.</p>
<p>We simply specify the Huggingface link as <code class="docutils literal notranslate"><span class="pre">model_url</span></code>, while reusing the <code class="docutils literal notranslate"><span class="pre">model_lib</span></code> for
<code class="docutils literal notranslate"><span class="pre">Mistral-7B</span></code>.</p>
<div class="highlight-javascript notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;model_list&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">   </span><span class="c1">// Other records here omitted...</span>
<span class="w">   </span><span class="p">{</span>
<span class="w">      </span><span class="c1">// Substitute model_url with the one you created `my-huggingface-account/my-mistral-weight-huggingface-repo`</span>
<span class="w">      </span><span class="s2">&quot;model_url&quot;</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;https://huggingface.co/mlc-ai/NeuralHermes-2.5-Mistral-7B-q3f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s2">&quot;model_id&quot;</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;Mistral-7B-Instruct-v0.2-q3f16_1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s2">&quot;model_lib&quot;</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;mistral_q3f16_1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s2">&quot;model_lib_path&quot;</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;lib/Mistral-7B-Instruct-v0.2/Mistral-7B-Instruct-v0.2-q3f16_1-iphone.tar&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s2">&quot;estimated_vram_bytes&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">3316000000</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
<p>Now, the app will use the <code class="docutils literal notranslate"><span class="pre">NeuralHermes-Mistral</span></code> model you just added.</p>
</section>
<section id="bring-your-own-model-library">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Bring Your Own Model Library</a><a class="headerlink" href="#bring-your-own-model-library" title="Permalink to this heading">¬∂</a></h2>
<p>A model library is specified by:</p>
<blockquote>
<div><ul class="simple">
<li><p>The model architecture (e.g. <code class="docutils literal notranslate"><span class="pre">mistral</span></code>, <code class="docutils literal notranslate"><span class="pre">phi-msft</span></code>)</p></li>
<li><p>Quantization Scheme (e.g. <code class="docutils literal notranslate"><span class="pre">q3f16_1</span></code>, <code class="docutils literal notranslate"><span class="pre">q0f32</span></code>)</p></li>
<li><p>Metadata (e.g. <code class="docutils literal notranslate"><span class="pre">context_window_size</span></code>, <code class="docutils literal notranslate"><span class="pre">sliding_window_size</span></code>, <code class="docutils literal notranslate"><span class="pre">prefill_chunk_size</span></code>), which affects memory planning</p></li>
<li><p>Platform (e.g. <code class="docutils literal notranslate"><span class="pre">cuda</span></code>, <code class="docutils literal notranslate"><span class="pre">webgpu</span></code>, <code class="docutils literal notranslate"><span class="pre">iphone</span></code>, <code class="docutils literal notranslate"><span class="pre">android</span></code>)</p></li>
</ul>
</div></blockquote>
<p>In cases where the model you want to run is not compatible with the provided MLC
prebuilt model libraries (e.g. having a different quantization, a different
metadata spec, or even a different model architecture), you need to build your
own model library.</p>
<p>In this section, we walk you through adding <code class="docutils literal notranslate"><span class="pre">phi-2</span></code> to the iOS app.</p>
<p>This section largely replicates <a class="reference internal" href="../compilation/compile_models.html#compile-model-libraries"><span class="std std-ref">Compile Model Libraries</span></a>. See that page for
more details, specifically the <code class="docutils literal notranslate"><span class="pre">iOS</span></code> option.</p>
<p><strong>Step 0. Install dependencies</strong></p>
<p>To compile model libraries for iOS, you need to <a class="reference internal" href="../install/mlc_llm.html#mlcchat-build-from-source"><span class="std std-ref">build mlc_llm from source</span></a>.</p>
<p><strong>Step 1. Clone from HF and convert_weight</strong></p>
<p>You can be under the mlc-llm repo, or your own working directory. Note that all platforms
can share the same compiled/quantized weights.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create directory</span>
mkdir<span class="w"> </span>-p<span class="w"> </span>dist/models<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>dist/models
<span class="c1"># Clone HF weights</span>
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/microsoft/phi-2
<span class="nb">cd</span><span class="w"> </span>../..
<span class="c1"># Convert weight</span>
mlc_llm<span class="w"> </span>convert_weight<span class="w"> </span>./dist/models/phi-2/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/phi-2-q4f16_1-MLC
</pre></div>
</div>
<p><strong>Step 2. Generate mlc-chat-config and compile</strong></p>
<p>A model library is specified by:</p>
<blockquote>
<div><ul class="simple">
<li><p>The model architecture (e.g. <code class="docutils literal notranslate"><span class="pre">mistral</span></code>, <code class="docutils literal notranslate"><span class="pre">phi-msft</span></code>)</p></li>
<li><p>Quantization Scheme (e.g. <code class="docutils literal notranslate"><span class="pre">q3f16_1</span></code>, <code class="docutils literal notranslate"><span class="pre">q0f32</span></code>)</p></li>
<li><p>Metadata (e.g. <code class="docutils literal notranslate"><span class="pre">context_window_size</span></code>, <code class="docutils literal notranslate"><span class="pre">sliding_window_size</span></code>, <code class="docutils literal notranslate"><span class="pre">prefill_chunk_size</span></code>), which affects memory planning</p></li>
<li><p>Platform (e.g. <code class="docutils literal notranslate"><span class="pre">cuda</span></code>, <code class="docutils literal notranslate"><span class="pre">webgpu</span></code>, <code class="docutils literal notranslate"><span class="pre">iphone</span></code>, <code class="docutils literal notranslate"><span class="pre">android</span></code>)</p></li>
</ul>
</div></blockquote>
<p>All these knobs are specified in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> generated by <code class="docutils literal notranslate"><span class="pre">gen_config</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. gen_config: generate mlc-chat-config.json and process tokenizers</span>
mlc_llm<span class="w"> </span>gen_config<span class="w"> </span>./dist/models/phi-2/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--conv-template<span class="w"> </span>phi-2<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>dist/phi-2-q4f16_1-MLC/
<span class="c1"># 2. compile: compile model library with specification in mlc-chat-config.json</span>
mlc_llm<span class="w"> </span>compile<span class="w"> </span>./dist/phi-2-q4f16_1-MLC/mlc-chat-config.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>iphone<span class="w"> </span>-o<span class="w"> </span>dist/libs/phi-2-q4f16_1-iphone.tar
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When compiling larger models like <code class="docutils literal notranslate"><span class="pre">Llama-2-7B</span></code>, you may want to add a lower chunk size
while prefilling prompts <code class="docutils literal notranslate"><span class="pre">--prefill_chunk_size</span> <span class="pre">128</span></code> or even lower <code class="docutils literal notranslate"><span class="pre">context_window_size</span></code>to decrease memory usage. Otherwise, during runtime, you may run out of memory.</p>
</div>
<p><strong>Step 3. Distribute model library and model weights</strong></p>
<p>After following the steps above, you should end up with:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/libs
<span class="w">  </span>phi-2-q4f16_1-iphone.tar<span class="w">  </span><span class="c1"># ===&gt; the model library</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/phi-2-q4f16_1-MLC
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>ndarray-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>Upload the <code class="docutils literal notranslate"><span class="pre">phi-2-q4f16_1-iphone.tar</span></code> to a github repository (for us,
it is in <a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs">binary-mlc-llm-libs</a>). Then
upload the weights <code class="docutils literal notranslate"><span class="pre">phi-2-q4f16_1-MLC</span></code> to a Huggingface repo:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># First, please create a repository on Hugging Face.</span>
<span class="c1"># With the repository created, run</span>
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/my-huggingface-account/my-phi-weight-huggingface-repo
<span class="nb">cd</span><span class="w"> </span>my-phi-weight-huggingface-repo
cp<span class="w"> </span>path/to/mlc-llm/dist/phi-2-q4f16_1-MLC/*<span class="w"> </span>.
git<span class="w"> </span>add<span class="w"> </span>.<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>git<span class="w"> </span>commit<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;Add phi-2 model weights&quot;</span>
git<span class="w"> </span>push<span class="w"> </span>origin<span class="w"> </span>main
</pre></div>
</div>
<p>This would result in something like <a class="reference external" href="https://huggingface.co/mlc-ai/phi-2-q4f16_1-MLC/tree/main">phi-2-q4f16_1-MLC</a>.</p>
<p><strong>Step 4. Calculate estimated VRAM usage</strong></p>
<p>Given the compiled library, it is possible to calculate an upper bound for the VRAM
usage during runtime. This useful to better understand if a model is able to fit particular
hardware. We can calculate this estimate using the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>mlc_llm.cli.model_metadata<span class="w"> </span>./dist/libs/phi-2-q4f16_1-iphone.tar<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>&gt;<span class="w"> </span>--memory-only<span class="w"> </span>--mlc-chat-config<span class="w"> </span>./dist/phi-2-q4f16_1-MLC/mlc-chat-config.json
<span class="w">  </span>INFO<span class="w"> </span>model_metadata.py:90:<span class="w"> </span>Total<span class="w"> </span>memory<span class="w"> </span>usage:<span class="w"> </span><span class="m">3042</span>.96<span class="w"> </span>MB<span class="w"> </span><span class="o">(</span>Parameters:<span class="w"> </span><span class="m">1492</span>.45<span class="w"> </span>MB.<span class="w"> </span>KVCache:<span class="w"> </span><span class="m">640</span>.00<span class="w"> </span>MB.<span class="w"> </span>Temporary<span class="w"> </span>buffer:<span class="w"> </span><span class="m">910</span>.51<span class="w"> </span>MB<span class="o">)</span>
<span class="w">  </span>INFO<span class="w"> </span>model_metadata.py:99:<span class="w"> </span>To<span class="w"> </span>reduce<span class="w"> </span>memory<span class="w"> </span>usage,<span class="w"> </span>tweak<span class="w"> </span><span class="sb">`</span>prefill_chunk_size<span class="sb">`</span>,<span class="w"> </span><span class="sb">`</span>context_window_size<span class="sb">`</span><span class="w"> </span>and<span class="w"> </span><span class="sb">`</span>sliding_window_size<span class="sb">`</span>
</pre></div>
</div>
<p><strong>Step 5. Register as a ModelRecord</strong></p>
<p>Finally, we update the code snippet for
<a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/ios/MLCChat/app-config.json">app-config.json</a>
pasted above.</p>
<p>We simply specify the Huggingface link as <code class="docutils literal notranslate"><span class="pre">model_url</span></code>, while using the new <code class="docutils literal notranslate"><span class="pre">model_lib</span></code> for
<code class="docutils literal notranslate"><span class="pre">phi-2</span></code>. Regarding the field <code class="docutils literal notranslate"><span class="pre">estimated_vram_bytes</span></code>, we can use the output of the last step
rounded up to MB.</p>
<div class="highlight-javascript notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;model_list&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">   </span><span class="c1">// Other records here omitted...</span>
<span class="w">   </span><span class="p">{</span>
<span class="w">      </span><span class="c1">// Substitute model_url with the one you created `my-huggingface-account/my-phi-weight-huggingface-repo`</span>
<span class="w">      </span><span class="s2">&quot;model_url&quot;</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;https://huggingface.co/mlc-ai/phi-2-q4f16_1-MLC&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s2">&quot;model_id&quot;</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;phi-2-q4f16_1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s2">&quot;model_lib&quot;</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;phi_msft_q4f16_1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s2">&quot;model_lib_path&quot;</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;lib/phi-2/phi-2-q4f16_1-iphone.tar&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s2">&quot;estimated_vram_bytes&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">3043000000</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
<p>Now, the app will use the <code class="docutils literal notranslate"><span class="pre">phi-2</span></code> model library you just added.</p>
</section>
<section id="build-apps-with-mlc-swift-api">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Build Apps with MLC Swift API</a><a class="headerlink" href="#build-apps-with-mlc-swift-api" title="Permalink to this heading">¬∂</a></h2>
<p>We also provide a Swift package that you can use to build
your own app. The package is located under <cite>ios/MLCSwift</cite>.</p>
<ul>
<li><p>First make sure you have run the same steps listed
in the previous section. This will give us the necessary libraries
under <code class="docutils literal notranslate"><span class="pre">/path/to/ios/build/lib</span></code>.</p></li>
<li><p>Then you can add <code class="docutils literal notranslate"><span class="pre">ios/MLCSwift</span></code> package to your app in Xcode.
Under ‚ÄúFrameworks, Libraries, and Embedded Content‚Äù, click add package dependencies
and add local package that points to <code class="docutils literal notranslate"><span class="pre">ios/MLCSwift</span></code>.</p></li>
<li><p>Finally, we need to add the libraries dependencies. Under build settings:</p>
<ul class="simple">
<li><p>Add library search path <code class="docutils literal notranslate"><span class="pre">/path/to/ios/build/lib</span></code>.</p></li>
<li><p>Add the following items to ‚Äúother linker flags‚Äù.</p></li>
</ul>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="n">Wl</span><span class="p">,</span><span class="o">-</span><span class="n">all_load</span>
<span class="o">-</span><span class="n">lmodel_iphone</span>
<span class="o">-</span><span class="n">lmlc_llm</span> <span class="o">-</span><span class="n">ltvm_runtime</span>
<span class="o">-</span><span class="n">ltokenizers_cpp</span>
<span class="o">-</span><span class="n">lsentencepiece</span>
<span class="o">-</span><span class="n">ltokenizers_c</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
<p>You can then import the <cite>MLCSwift</cite> package into your app.
The following code shows an illustrative example of how to use the chat module.</p>
<div class="highlight-swift notranslate"><div class="highlight"><pre><span></span><span class="kd">import</span> <span class="nc">MLCSwift</span>

<span class="kd">let</span> <span class="nv">threadWorker</span> <span class="p">=</span> <span class="n">ThreadWorker</span><span class="p">()</span>
<span class="kd">let</span> <span class="nv">chat</span> <span class="p">=</span> <span class="n">ChatModule</span><span class="p">()</span>

<span class="n">threadWorker</span><span class="p">.</span><span class="n">push</span> <span class="p">{</span>
   <span class="kd">let</span> <span class="nv">modelLib</span> <span class="p">=</span> <span class="s">&quot;model-lib-name&quot;</span>
   <span class="kd">let</span> <span class="nv">modelPath</span> <span class="p">=</span> <span class="s">&quot;/path/to/model/weights&quot;</span>
   <span class="kd">let</span> <span class="nv">input</span> <span class="p">=</span> <span class="s">&quot;What is the capital of Canada?&quot;</span>
   <span class="n">chat</span><span class="p">.</span><span class="n">reload</span><span class="p">(</span><span class="n">modelLib</span><span class="p">,</span> <span class="n">modelPath</span><span class="p">:</span> <span class="n">modelPath</span><span class="p">)</span>

   <span class="n">chat</span><span class="p">.</span><span class="n">prefill</span><span class="p">(</span><span class="n">input</span><span class="p">)</span>
   <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">chat</span><span class="p">.</span><span class="n">stopped</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">displayReply</span><span class="p">(</span><span class="n">chat</span><span class="p">.</span><span class="n">getMessage</span><span class="p">())</span>
      <span class="n">chat</span><span class="p">.</span><span class="n">decode</span><span class="p">()</span>
   <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Because the chat module makes heavy use of GPU and thread-local
resources, it needs to run on a dedicated background thread.
Therefore, <strong>avoid using</strong> <cite>DispatchQueue</cite>, which can cause context switching to
different threads and segfaults due to thread-safety issues.
Use the <cite>ThreadWorker</cite> class to launch all the jobs related
to the chat module. You can check out the source code of
the MLCChat app for a complete example.</p>
</div>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="android.html" class="btn btn-neutral float-right" title="Android App" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="python_engine.html" class="btn btn-neutral float-left" title="Python API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">¬© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
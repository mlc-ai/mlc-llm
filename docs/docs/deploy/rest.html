





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>REST API &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CLI" href="cli.html" />
    <link rel="prev" title="WebLLM and JavaScript API" href="javascript.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/introduction.html">Introduction to MLC LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="javascript.html">WebLLM and JavaScript API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">REST API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#install-mlc-llm-package">Install MLC-LLM Package</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quick-start">Quick start</a></li>
<li class="toctree-l2"><a class="reference internal" href="#launch-the-server">Launch the Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="#api-endpoints">API Endpoints</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_engine.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android App</a></li>
<li class="toctree-l1"><a class="reference internal" href="ide_integration.html">Code Completion IDE Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlc_chat_config.html">Customize MLC Config File in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/convert_weights.html">Convert Weights via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Model Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Prebuilts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>REST API</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/rest.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="rest-api">
<span id="deploy-rest-api"></span><h1>REST API<a class="headerlink" href="#rest-api" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#install-mlc-llm-package" id="id2">Install MLC-LLM Package</a></p></li>
<li><p><a class="reference internal" href="#quick-start" id="id3">Quick start</a></p></li>
<li><p><a class="reference internal" href="#launch-the-server" id="id4">Launch the Server</a></p></li>
<li><p><a class="reference internal" href="#api-endpoints" id="id5">API Endpoints</a></p></li>
</ul>
</nav>
<p>We provide <a class="reference external" href="https://www.ibm.com/topics/rest-apis#:~:text=the%20next%20step-,What%20is%20a%20REST%20API%3F,representational%20state%20transfer%20architectural%20style.">REST API</a>
for a user to interact with MLC-LLM in their own programs.</p>
<section id="install-mlc-llm-package">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Install MLC-LLM Package</a><a class="headerlink" href="#install-mlc-llm-package" title="Permalink to this heading">¶</a></h2>
<p>SERVE is a part of the MLC-LLM package, installation instruction for which can be found <a class="reference internal" href="../install/mlc_llm.html#install-mlc-packages"><span class="std std-ref">here</span></a>. Once you have install the MLC-LLM package, you can run the following command to check if the installation was successful:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlc_llm<span class="w"> </span>serve<span class="w"> </span>--help
</pre></div>
</div>
<p>You should see serve help message if the installation was successful.</p>
</section>
<section id="quick-start">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Quick start</a><a class="headerlink" href="#quick-start" title="Permalink to this heading">¶</a></h2>
<p>This section provides a quick start guide to work with MLC-LLM REST API. To launch a server, run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlc_llm<span class="w"> </span>serve<span class="w"> </span>MODEL<span class="w"> </span><span class="o">[</span>--model-lib-path<span class="w"> </span>MODEL_LIB_PATH<span class="o">]</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">MODEL</span></code> is the model folder after compiling with <a class="reference internal" href="../compilation/compile_models.html#compile-model-libraries"><span class="std std-ref">MLC-LLM build process</span></a>. Information about other arguments can be found under <a class="reference internal" href="#rest-launch-server"><span class="std std-ref">Launch the server</span></a> section.</p>
<p>Once you have launched the Server, you can use the API in your own program to send requests. Below is an example of using the API to interact with MLC-LLM in Python without Streaming (suppose the server is running on <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:8080/</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>import<span class="w"> </span>requests

<span class="c1"># Get a response using a prompt without streaming</span>
<span class="nv">payload</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>
<span class="w">   </span><span class="s2">&quot;model&quot;</span>:<span class="w"> </span><span class="s2">&quot;./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/&quot;</span>,
<span class="w">   </span><span class="s2">&quot;messages&quot;</span>:<span class="w"> </span><span class="o">[</span>
<span class="w">      </span><span class="o">{</span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,<span class="w"> </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;Write a haiku about apples.&quot;</span><span class="o">}</span>,
<span class="w">   </span><span class="o">]</span>,
<span class="w">   </span><span class="s2">&quot;stream&quot;</span>:<span class="w"> </span>False,
<span class="w">   </span><span class="c1"># &quot;n&quot;: 1,</span>
<span class="w">   </span><span class="s2">&quot;max_tokens&quot;</span>:<span class="w"> </span><span class="m">300</span>,
<span class="o">}</span>
<span class="nv">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>requests.post<span class="o">(</span><span class="s2">&quot;http://127.0.0.1:8080/v1/chat/completions&quot;</span>,<span class="w"> </span><span class="nv">json</span><span class="o">=</span>payload<span class="o">)</span>
<span class="nv">choices</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>r.json<span class="o">()[</span><span class="s2">&quot;choices&quot;</span><span class="o">]</span>
<span class="k">for</span><span class="w"> </span>choice<span class="w"> </span><span class="k">in</span><span class="w"> </span>choices:
<span class="w">   </span>print<span class="o">(</span>f<span class="s2">&quot;{choice[&#39;message&#39;][&#39;content&#39;]}\n&quot;</span><span class="o">)</span>
</pre></div>
</div>
<hr class="docutils" />
</section>
<section id="launch-the-server">
<span id="rest-launch-server"></span><h2><a class="toc-backref" href="#id4" role="doc-backlink">Launch the Server</a><a class="headerlink" href="#launch-the-server" title="Permalink to this heading">¶</a></h2>
<p>To launch the MLC Server for MLC-LLM, run the following command in your terminal.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlc_llm<span class="w"> </span>serve<span class="w"> </span>MODEL<span class="w"> </span><span class="o">[</span>--model-lib-path<span class="w"> </span>MODEL_LIB_PATH<span class="o">]</span><span class="w"> </span><span class="o">[</span>--device<span class="w"> </span>DEVICE<span class="o">]</span><span class="w"> </span><span class="o">[</span>--max-batch-size<span class="w"> </span>MAX_BATCH_SIZE<span class="o">]</span><span class="w"> </span><span class="o">[</span>--max-total-seq-length<span class="w"> </span>MAX_TOTAL_SEQ_LENGTH<span class="o">]</span><span class="w"> </span><span class="o">[</span>--prefill-chunk-size<span class="w"> </span>PREFILL_CHUNK_SIZE<span class="o">]</span><span class="w"> </span><span class="o">[</span>--enable-tracing<span class="o">]</span><span class="w"> </span><span class="o">[</span>--host<span class="w"> </span>HOST<span class="o">]</span><span class="w"> </span><span class="o">[</span>--port<span class="w"> </span>PORT<span class="o">]</span><span class="w"> </span><span class="o">[</span>--allow-credentials<span class="o">]</span><span class="w"> </span><span class="o">[</span>--allowed-origins<span class="w"> </span>ALLOWED_ORIGINS<span class="o">]</span><span class="w"> </span><span class="o">[</span>--allowed-methods<span class="w"> </span>ALLOWED_METHODS<span class="o">]</span><span class="w"> </span><span class="o">[</span>--allowed-headers<span class="w"> </span>ALLOWED_HEADERS<span class="o">]</span>
</pre></div>
</div>
<dl class="simple">
<dt>MODEL                  The model folder after compiling with MLC-LLM build process. The parameter</dt><dd><p>can either be the model name with its quantization scheme
(e.g. <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>), or a full path to the model
folder. In the former case, we will use the provided name to search
for the model folder over possible paths.</p>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">--model-lib-path</span></kbd></dt>
<dd><p>A field to specify the full path to the model library file to use (e.g. a <code class="docutils literal notranslate"><span class="pre">.so</span></code> file).</p>
</dd>
<dt><kbd><span class="option">--device</span></kbd></dt>
<dd><p>The description of the device to run on. User should provide a string in the
form of ‘device_name:device_id’ or ‘device_name’, where ‘device_name’ is one of
‘cuda’, ‘metal’, ‘vulkan’, ‘rocm’, ‘opencl’, ‘auto’ (automatically detect the
local device), and ‘device_id’ is the device id to run on. The default value is <code class="docutils literal notranslate"><span class="pre">auto</span></code>,
with the device id set to 0 for default.</p>
</dd>
<dt><kbd><span class="option">--host</span></kbd></dt>
<dd><p>The host at which the server should be started, defaults to <code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code>.</p>
</dd>
<dt><kbd><span class="option">--port</span></kbd></dt>
<dd><p>The port on which the server should be started, defaults to <code class="docutils literal notranslate"><span class="pre">8000</span></code>.</p>
</dd>
<dt><kbd><span class="option">--allow-credentials</span></kbd></dt>
<dd><p>A flag to indicate whether the server should allow credentials. If set, the server will
include the <code class="docutils literal notranslate"><span class="pre">CORS</span></code> header in the response</p>
</dd>
<dt><kbd><span class="option">--allowed-origins</span></kbd></dt>
<dd><p>Specifies the allowed origins. It expects a JSON list of strings, with the default value being <code class="docutils literal notranslate"><span class="pre">[&quot;*&quot;]</span></code>, allowing all origins.</p>
</dd>
<dt><kbd><span class="option">--allowed-methods</span></kbd></dt>
<dd><p>Specifies the allowed methods. It expects a JSON list of strings, with the default value being <code class="docutils literal notranslate"><span class="pre">[&quot;*&quot;]</span></code>, allowing all methods.</p>
</dd>
<dt><kbd><span class="option">--allowed-headers</span></kbd></dt>
<dd><p>Specifies the allowed headers. It expects a JSON list of strings, with the default value being <code class="docutils literal notranslate"><span class="pre">[&quot;*&quot;]</span></code>, allowing all headers.</p>
</dd>
<dt><kbd><span class="option">--max-batch-size</span></kbd></dt>
<dd><p>The maximum batch size for processing.</p>
</dd>
<dt><kbd><span class="option">--max-total-seq-length</span></kbd></dt>
<dd><p>The maximum total number of tokens whose KV data are allowed to exist in the KV cache at any time. Set it to None to enable automatic computation of the max total sequence length.</p>
</dd>
<dt><kbd><span class="option">--prefill-chunk-size</span></kbd></dt>
<dd><p>The maximum total sequence length in a prefill. If not specified, it will be automatically inferred from model config.</p>
</dd>
<dt><kbd><span class="option">--enable-tracing</span></kbd></dt>
<dd><p>A boolean indicating if to enable event logging for requests.</p>
</dd>
</dl>
<p>You can access <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:PORT/docs</span></code> (replace <code class="docutils literal notranslate"><span class="pre">PORT</span></code> with the port number you specified) to see the list of
supported endpoints.</p>
</section>
<section id="api-endpoints">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">API Endpoints</a><a class="headerlink" href="#api-endpoints" title="Permalink to this heading">¶</a></h2>
<p>The REST API provides the following endpoints:</p>
<dl class="http get">
<dt class="sig sig-object http" id="get--v1-models">
<span class="sig-name descname"><span class="pre">GET</span> </span><span class="sig-name descname"><span class="pre">/v1/models</span></span><a class="headerlink" href="#get--v1-models" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<hr class="docutils" />
<blockquote>
<div><p>Get a list of models available for MLC-LLM.</p>
</div></blockquote>
<p><strong>Example</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>import<span class="w"> </span>requests

<span class="nv">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;http://127.0.0.1:8000/v1/models&quot;</span>
<span class="nv">headers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span><span class="s2">&quot;accept&quot;</span>:<span class="w"> </span><span class="s2">&quot;application/json&quot;</span><span class="o">}</span>

<span class="nv">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>requests.get<span class="o">(</span>url,<span class="w"> </span><span class="nv">headers</span><span class="o">=</span>headers<span class="o">)</span>

<span class="k">if</span><span class="w"> </span>response.status_code<span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">200</span>:
<span class="w">   </span>print<span class="o">(</span><span class="s2">&quot;Response:&quot;</span><span class="o">)</span>
<span class="w">   </span>print<span class="o">(</span>response.json<span class="o">())</span>
<span class="k">else</span>:
<span class="w">   </span>print<span class="o">(</span><span class="s2">&quot;Error:&quot;</span>,<span class="w"> </span>response.status_code<span class="o">)</span>
</pre></div>
</div>
<dl class="http post">
<dt class="sig sig-object http" id="post--v1-chat-completions">
<span class="sig-name descname"><span class="pre">POST</span> </span><span class="sig-name descname"><span class="pre">/v1/chat/completions</span></span><a class="headerlink" href="#post--v1-chat-completions" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<hr class="docutils" />
<blockquote>
<div><p>Get a response from MLC-LLM using a prompt, either with or without streaming.</p>
</div></blockquote>
<p><strong>Chat Completion Request Object</strong></p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>messages</strong> (<em>List[ChatCompletionMessage]</em>, required): A sequence of messages that have been exchanged in the conversation so far. Each message in the conversation is represented by a <cite>ChatCompletionMessage</cite> object, which includes the following fields:</dt><dd><ul>
<li><p><strong>content</strong> (<em>Optional[Union[str, List[Dict[str, str]]]]</em>): The text content of the message or structured data in case of tool-generated messages.</p></li>
<li><p><strong>role</strong> (<em>Literal[“system”, “user”, “assistant”, “tool”]</em>): The role of the message sender, indicating whether the message is from the system, user, assistant, or a tool.</p></li>
<li><p><strong>name</strong> (<em>Optional[str]</em>): An optional name for the sender of the message.</p></li>
<li><p><strong>tool_calls</strong> (<em>Optional[List[ChatToolCall]]</em>): A list of calls to external tools or functions made within this message, applicable when the role is <cite>tool</cite>.</p></li>
<li><p><strong>tool_call_id</strong> (<em>Optional[str]</em>): A unique identifier for the tool call, relevant when integrating external tools or services.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><strong>model</strong> (<em>str</em>, required): The model to be used for generating responses.</p></li>
<li><p><strong>frequency_penalty</strong> (<em>float</em>, optional, default=0.0): Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat tokens.</p></li>
<li><p><strong>presence_penalty</strong> (<em>float</em>, optional, default=0.0): Positive values penalize new tokens if they are already present in the text so far, decreasing the model’s likelihood to repeat tokens.</p></li>
<li><p><strong>logprobs</strong> (<em>bool</em>, optional, default=False): Indicates whether to include log probabilities for each token in the response.</p></li>
<li><p><strong>top_logprobs</strong> (<em>int</em>, optional, default=0): An integer ranging from 0 to 5. It determines the number of tokens, most likely to appear at each position, to be returned. Each token is accompanied by a log probability. If this parameter is used, ‘logprobs’ must be set to true.</p></li>
<li><p><strong>logit_bias</strong> (<em>Optional[Dict[int, float]]</em>): Allows specifying biases for or against specific tokens during generation.</p></li>
<li><p><strong>max_tokens</strong> (<em>Optional[int]</em>): The maximum number of tokens to generate in the response(s).</p></li>
<li><p><strong>n</strong> (<em>int</em>, optional, default=1): Number of responses to generate for the given prompt.</p></li>
<li><p><strong>seed</strong> (<em>Optional[int]</em>): A seed for deterministic generation. Using the same seed and inputs will produce the same output.</p></li>
<li><p><strong>stop</strong> (<em>Optional[Union[str, List[str]]]</em>): One or more strings that, if encountered, will cause generation to stop.</p></li>
<li><p><strong>stream</strong> (<em>bool</em>, optional, default=False): If <cite>True</cite>, responses are streamed back as they are generated.</p></li>
<li><p><strong>temperature</strong> (<em>float</em>, optional, default=1.0): Controls the randomness of the generation. Lower values lead to less random completions.</p></li>
<li><p><strong>top_p</strong> (<em>float</em>, optional, default=1.0): Nucleus sampling parameter that controls the diversity of the generated responses.</p></li>
<li><p><strong>tools</strong> (<em>Optional[List[ChatTool]]</em>): Specifies external tools or functions that can be called as part of the chat.</p></li>
<li><p><strong>tool_choice</strong> (<em>Optional[Union[Literal[“none”, “auto”], Dict]]</em>): Controls how tools are selected for use in responses.</p></li>
<li><p><strong>user</strong> (<em>Optional[str]</em>): An optional identifier for the user initiating the request.</p></li>
<li><p><strong>ignore_eos</strong> (<em>bool</em>, optional, default=False): If <cite>True</cite>, the model will ignore the end-of-sequence token for generating responses.</p></li>
<li><p><strong>response_format</strong> (<em>RequestResponseFormat</em>, optional): Specifies the format of the response. Can be either “text” or “json_object”, with optional schema definition for JSON responses.</p></li>
</ul>
<p><strong>Returns</strong></p>
<ul class="simple">
<li><p>If <cite>stream</cite> is <cite>False</cite>, a <cite>ChatCompletionResponse</cite> object containing the generated response(s).</p></li>
<li><p>If <cite>stream</cite> is <cite>True</cite>, a stream of <cite>ChatCompletionStreamResponse</cite> objects, providing a real-time feed of generated responses.</p></li>
</ul>
<p><strong>ChatCompletionResponseChoice</strong></p>
<ul class="simple">
<li><p><strong>finish_reason</strong> (<em>Optional[Literal[“stop”, “length”, “tool_calls”, “error”]]</em>, optional): The reason the completion process was terminated. It can be due to reaching a stop condition, the maximum length, output of tool calls, or an error.</p></li>
<li><p><strong>index</strong> (<em>int</em>, required, default=0): Indicates the position of this choice within the list of choices.</p></li>
<li><p><strong>message</strong> (<em>ChatCompletionMessage</em>, required): The message part of the chat completion, containing the content of the chat response.</p></li>
<li><p><strong>logprobs</strong> (<em>Optional[LogProbs]</em>, optional): Optionally includes log probabilities for each output token</p></li>
</ul>
<p><strong>ChatCompletionStreamResponseChoice</strong></p>
<ul class="simple">
<li><p><strong>finish_reason</strong> (<em>Optional[Literal[“stop”, “length”, “tool_calls”]]</em>, optional): Specifies why the streaming completion process ended. Valid reasons are “stop”, “length”, and “tool_calls”.</p></li>
<li><p><strong>index</strong> (<em>int</em>, required, default=0): Indicates the position of this choice within the list of choices.</p></li>
<li><p><strong>delta</strong> (<em>ChatCompletionMessage</em>, required): Represents the incremental update or addition to the chat completion message in the stream.</p></li>
<li><p><strong>logprobs</strong> (<em>Optional[LogProbs]</em>, optional): Optionally includes log probabilities for each output token</p></li>
</ul>
<p><strong>ChatCompletionResponse</strong></p>
<ul class="simple">
<li><p><strong>id</strong> (<em>str</em>, required): A unique identifier for the chat completion session.</p></li>
<li><p><strong>choices</strong> (<em>List[ChatCompletionResponseChoice]</em>, required): A collection of <cite>ChatCompletionResponseChoice</cite> objects, representing the potential responses generated by the model.</p></li>
<li><p><strong>created</strong> (<em>int</em>, required, default=current time): The UNIX timestamp representing when the response was generated.</p></li>
<li><p><strong>model</strong> (<em>str</em>, required): The name of the model used to generate the chat completions.</p></li>
<li><p><strong>system_fingerprint</strong> (<em>str</em>, required): A system-generated fingerprint that uniquely identifies the computational environment.</p></li>
<li><p><strong>object</strong> (<em>Literal[“chat.completion”]</em>, required, default=”chat.completion”): A string literal indicating the type of object, here always “chat.completion”.</p></li>
<li><p><strong>usage</strong> (<em>UsageInfo</em>, required, default=empty <cite>UsageInfo</cite> object): Contains information about the API usage for this specific request.</p></li>
</ul>
<p><strong>ChatCompletionStreamResponse</strong></p>
<ul class="simple">
<li><p><strong>id</strong> (<em>str</em>, required): A unique identifier for the streaming chat completion session.</p></li>
<li><p><strong>choices</strong> (<em>List[ChatCompletionStreamResponseChoice]</em>, required): A list of <cite>ChatCompletionStreamResponseChoice</cite> objects, each representing a part of the streaming chat response.</p></li>
<li><p><strong>created</strong> (<em>int</em>, required, default=current time): The creation time of the streaming response, represented as a UNIX timestamp.</p></li>
<li><p><strong>model</strong> (<em>str</em>, required): Specifies the model that was used for generating the streaming chat completions.</p></li>
<li><p><strong>system_fingerprint</strong> (<em>str</em>, required): A unique identifier for the system generating the streaming completions.</p></li>
<li><p><strong>object</strong> (<em>Literal[“chat.completion.chunk”]</em>, required, default=”chat.completion.chunk”): A literal indicating that this object represents a chunk of a streaming chat completion.</p></li>
</ul>
<hr class="docutils" />
<p><strong>Example</strong></p>
<p>Below is an example of using the API to interact with MLC-LLM in Python with Streaming.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>import<span class="w"> </span>requests
import<span class="w"> </span>json

<span class="c1"># Get a response using a prompt with streaming</span>
<span class="nv">payload</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>
<span class="w"> </span><span class="s2">&quot;model&quot;</span>:<span class="w"> </span><span class="s2">&quot;./dist/Llama-2-7b-chat-hf-q4f16_1-MLC/&quot;</span>,
<span class="w"> </span><span class="s2">&quot;messages&quot;</span>:<span class="w"> </span><span class="o">[{</span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,<span class="w"> </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;Write a haiku&quot;</span><span class="o">}]</span>,
<span class="w"> </span><span class="s2">&quot;stream&quot;</span>:<span class="w"> </span>True,
<span class="o">}</span>
with<span class="w"> </span>requests.post<span class="o">(</span><span class="s2">&quot;http://127.0.0.1:8080/v1/chat/completions&quot;</span>,<span class="w"> </span><span class="nv">json</span><span class="o">=</span>payload,<span class="w"> </span><span class="nv">stream</span><span class="o">=</span>True<span class="o">)</span><span class="w"> </span>as<span class="w"> </span>r:
<span class="w">   </span><span class="k">for</span><span class="w"> </span>chunk<span class="w"> </span><span class="k">in</span><span class="w"> </span>r.iter_content<span class="o">(</span><span class="nv">chunk_size</span><span class="o">=</span>None<span class="o">)</span>:
<span class="w">      </span><span class="nv">chunk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>chunk.decode<span class="o">(</span><span class="s2">&quot;utf-8&quot;</span><span class="o">)</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="s2">&quot;[DONE]&quot;</span><span class="w"> </span><span class="k">in</span><span class="w"> </span>chunk<span class="o">[</span><span class="m">6</span>:<span class="o">]</span>:
<span class="w">         </span><span class="k">break</span>
<span class="w">      </span><span class="nv">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>json.loads<span class="o">(</span>chunk<span class="o">[</span><span class="m">6</span>:<span class="o">])</span>
<span class="w">      </span><span class="nv">content</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>response<span class="o">[</span><span class="s2">&quot;choices&quot;</span><span class="o">][</span><span class="m">0</span><span class="o">][</span><span class="s2">&quot;delta&quot;</span><span class="o">]</span>.get<span class="o">(</span><span class="s2">&quot;content&quot;</span>,<span class="w"> </span><span class="s2">&quot;&quot;</span><span class="o">)</span>
<span class="w">      </span>print<span class="o">(</span>content,<span class="w"> </span><span class="nv">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span>,<span class="w"> </span><span class="nv">flush</span><span class="o">=</span>True<span class="o">)</span>
print<span class="o">(</span><span class="s2">&quot;\n&quot;</span><span class="o">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>There is also support for function calling similar to OpenAI (<a class="reference external" href="https://platform.openai.com/docs/guides/function-calling">https://platform.openai.com/docs/guides/function-calling</a>). Below is an example on how to use function calling in Python.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>import<span class="w"> </span>requests
import<span class="w"> </span>json

<span class="nv">tools</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span>
<span class="w">   </span><span class="o">{</span>
<span class="w">      </span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;function&quot;</span>,
<span class="w">      </span><span class="s2">&quot;function&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">            </span><span class="s2">&quot;name&quot;</span>:<span class="w"> </span><span class="s2">&quot;get_current_weather&quot;</span>,
<span class="w">            </span><span class="s2">&quot;description&quot;</span>:<span class="w"> </span><span class="s2">&quot;Get the current weather in a given location&quot;</span>,
<span class="w">            </span><span class="s2">&quot;parameters&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">               </span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;object&quot;</span>,
<span class="w">               </span><span class="s2">&quot;properties&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">                  </span><span class="s2">&quot;location&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">                        </span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;string&quot;</span>,
<span class="w">                        </span><span class="s2">&quot;description&quot;</span>:<span class="w"> </span><span class="s2">&quot;The city and state, e.g. San Francisco, CA&quot;</span>,
<span class="w">                  </span><span class="o">}</span>,
<span class="w">                  </span><span class="s2">&quot;unit&quot;</span>:<span class="w"> </span><span class="o">{</span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;string&quot;</span>,<span class="w"> </span><span class="s2">&quot;enum&quot;</span>:<span class="w"> </span><span class="o">[</span><span class="s2">&quot;celsius&quot;</span>,<span class="w"> </span><span class="s2">&quot;fahrenheit&quot;</span><span class="o">]}</span>,
<span class="w">               </span><span class="o">}</span>,
<span class="w">               </span><span class="s2">&quot;required&quot;</span>:<span class="w"> </span><span class="o">[</span><span class="s2">&quot;location&quot;</span><span class="o">]</span>,
<span class="w">            </span><span class="o">}</span>,
<span class="w">      </span><span class="o">}</span>,
<span class="w">   </span><span class="o">}</span>
<span class="o">]</span>

<span class="nv">payload</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>
<span class="w">   </span><span class="s2">&quot;model&quot;</span>:<span class="w"> </span><span class="s2">&quot;./dist/gorilla-openfunctions-v1-q4f16_1-MLC/&quot;</span>,
<span class="w">   </span><span class="s2">&quot;messages&quot;</span>:<span class="w"> </span><span class="o">[</span>
<span class="w">      </span><span class="o">{</span>
<span class="w">            </span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,
<span class="w">            </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;What is the current weather in Pittsburgh, PA in fahrenheit?&quot;</span>,
<span class="w">      </span><span class="o">}</span>
<span class="w">   </span><span class="o">]</span>,
<span class="w">   </span><span class="s2">&quot;stream&quot;</span>:<span class="w"> </span>False,
<span class="w">   </span><span class="s2">&quot;tools&quot;</span>:<span class="w"> </span>tools,
<span class="o">}</span>

<span class="nv">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>requests.post<span class="o">(</span><span class="s2">&quot;http://127.0.0.1:8080/v1/chat/completions&quot;</span>,<span class="w"> </span><span class="nv">json</span><span class="o">=</span>payload<span class="o">)</span>
print<span class="o">(</span>f<span class="s2">&quot;{r.json()[&#39;choices&#39;][0][&#39;message&#39;][&#39;tool_calls&#39;][0][&#39;function&#39;]}\n&quot;</span><span class="o">)</span>

<span class="c1"># Output: {&#39;name&#39;: &#39;get_current_weather&#39;, &#39;arguments&#39;: {&#39;location&#39;: &#39;Pittsburgh, PA&#39;, &#39;unit&#39;: &#39;fahrenheit&#39;}}</span>
</pre></div>
</div>
<hr class="docutils" />
<p>Function Calling with streaming is also supported. Below is an example on how to use function calling with streaming in Python.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>import<span class="w"> </span>requests
import<span class="w"> </span>json

<span class="nv">tools</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span>
<span class="w">   </span><span class="o">{</span>
<span class="w">      </span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;function&quot;</span>,
<span class="w">      </span><span class="s2">&quot;function&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">            </span><span class="s2">&quot;name&quot;</span>:<span class="w"> </span><span class="s2">&quot;get_current_weather&quot;</span>,
<span class="w">            </span><span class="s2">&quot;description&quot;</span>:<span class="w"> </span><span class="s2">&quot;Get the current weather in a given location&quot;</span>,
<span class="w">            </span><span class="s2">&quot;parameters&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">               </span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;object&quot;</span>,
<span class="w">               </span><span class="s2">&quot;properties&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">                  </span><span class="s2">&quot;location&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">                        </span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;string&quot;</span>,
<span class="w">                        </span><span class="s2">&quot;description&quot;</span>:<span class="w"> </span><span class="s2">&quot;The city and state, e.g. San Francisco, CA&quot;</span>,
<span class="w">                  </span><span class="o">}</span>,
<span class="w">                  </span><span class="s2">&quot;unit&quot;</span>:<span class="w"> </span><span class="o">{</span><span class="s2">&quot;type&quot;</span>:<span class="w"> </span><span class="s2">&quot;string&quot;</span>,<span class="w"> </span><span class="s2">&quot;enum&quot;</span>:<span class="w"> </span><span class="o">[</span><span class="s2">&quot;celsius&quot;</span>,<span class="w"> </span><span class="s2">&quot;fahrenheit&quot;</span><span class="o">]}</span>,
<span class="w">               </span><span class="o">}</span>,
<span class="w">               </span><span class="s2">&quot;required&quot;</span>:<span class="w"> </span><span class="o">[</span><span class="s2">&quot;location&quot;</span><span class="o">]</span>,
<span class="w">            </span><span class="o">}</span>,
<span class="w">      </span><span class="o">}</span>,
<span class="w">   </span><span class="o">}</span>
<span class="o">]</span>

<span class="nv">payload</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>
<span class="w">   </span><span class="s2">&quot;model&quot;</span>:<span class="w"> </span><span class="s2">&quot;./dist/gorilla-openfunctions-v1-q4f16_1-MLC/&quot;</span>,
<span class="w">   </span><span class="s2">&quot;messages&quot;</span>:<span class="w"> </span><span class="o">[</span>
<span class="w">      </span><span class="o">{</span>
<span class="w">            </span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;user&quot;</span>,
<span class="w">            </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;What is the current weather in Pittsburgh, PA and Tokyo, JP in fahrenheit?&quot;</span>,
<span class="w">      </span><span class="o">}</span>
<span class="w">   </span><span class="o">]</span>,
<span class="w">   </span><span class="s2">&quot;stream&quot;</span>:<span class="w"> </span>True,
<span class="w">   </span><span class="s2">&quot;tools&quot;</span>:<span class="w"> </span>tools,
<span class="o">}</span>

with<span class="w"> </span>requests.post<span class="o">(</span><span class="s2">&quot;http://127.0.0.1:8080/v1/chat/completions&quot;</span>,<span class="w"> </span><span class="nv">json</span><span class="o">=</span>payload,<span class="w"> </span><span class="nv">stream</span><span class="o">=</span>True<span class="o">)</span><span class="w"> </span>as<span class="w"> </span>r:
<span class="w"> </span><span class="k">for</span><span class="w"> </span>chunk<span class="w"> </span><span class="k">in</span><span class="w"> </span>r.iter_content<span class="o">(</span><span class="nv">chunk_size</span><span class="o">=</span>None<span class="o">)</span>:
<span class="w">     </span><span class="nv">chunk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>chunk.decode<span class="o">(</span><span class="s2">&quot;utf-8&quot;</span><span class="o">)</span>
<span class="w">     </span><span class="k">if</span><span class="w"> </span><span class="s2">&quot;[DONE]&quot;</span><span class="w"> </span><span class="k">in</span><span class="w"> </span>chunk<span class="o">[</span><span class="m">6</span>:<span class="o">]</span>:
<span class="w">         </span><span class="k">break</span>
<span class="w">     </span><span class="nv">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>json.loads<span class="o">(</span>chunk<span class="o">[</span><span class="m">6</span>:<span class="o">])</span>
<span class="w">     </span><span class="nv">content</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>response<span class="o">[</span><span class="s2">&quot;choices&quot;</span><span class="o">][</span><span class="m">0</span><span class="o">][</span><span class="s2">&quot;delta&quot;</span><span class="o">]</span>.get<span class="o">(</span><span class="s2">&quot;content&quot;</span>,<span class="w"> </span><span class="s2">&quot;&quot;</span><span class="o">)</span>
<span class="w">     </span>print<span class="o">(</span>f<span class="s2">&quot;{content}&quot;</span>,<span class="w"> </span><span class="nv">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span>,<span class="w"> </span><span class="nv">flush</span><span class="o">=</span>True<span class="o">)</span>
print<span class="o">(</span><span class="s2">&quot;\n&quot;</span><span class="o">)</span>

<span class="c1"># Output: [&quot;get_current_weather(location=&#39;Pittsburgh,PA&#39;,unit=&#39;fahrenheit&#39;)&quot;, &quot;get_current_weather(location=&#39;Tokyo,JP&#39;,unit=&#39;fahrenheit&#39;)&quot;]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The API is a uniform interface that supports multiple languages. You can also utilize these functionalities in languages other than Python.</p>
</div>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="cli.html" class="btn btn-neutral float-right" title="CLI" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="javascript.html" class="btn btn-neutral float-left" title="WebLLM and JavaScript API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
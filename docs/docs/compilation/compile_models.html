





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Compile Models via MLC &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/tabs.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distribute Compiled Models" href="distribute_compiled_models.html" />
    <link rel="prev" title="Android App" href="../deploy/android.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/project_overview.html">Project Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/mlc_chat_config.html">Configure MLCChat in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deploy/javascript.html">WebLLM and Javascript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/rest.html">Rest API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/cli.html">CLI and C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/python.html">Python API and Gradio Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/android.html">Android App</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Compile Models via MLC</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#install-mlc-llm-package">Install MLC-LLM Package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#work-with-source-code">Work with Source Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#verify-installation">Verify Installation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#get-started">Get Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compile-command-specification">Compile Command Specification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#more-model-compile-commands">More Model Compile Commands</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distribute_compiled_models.html">Distribute Compiled Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="python.html">Python API for Model Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="configure_quantization.html">ðŸš§ Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/customize/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Compile Models via MLC</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/compilation/compile_models.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="compile-models-via-mlc">
<span id="id1"></span><h1>Compile Models via MLC<a class="headerlink" href="#compile-models-via-mlc" title="Permalink to this heading">Â¶</a></h1>
<p>This page describes how to compile a model with MLC LLM. Model compilation takes model inputs, produces quantized model weights,
and optimizes model lib for a given platform. It enables users to bring their own new model weights, try different quantization modes,
and customize the overall model optimization flow.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you proceed, please make sure that you have <a class="reference internal" href="../install/tvm.html#install-tvm-unity"><span class="std std-ref">Install TVM Unity Compiler</span></a> correctly installed on your machine.
TVM-Unity is the necessary foundation for us to compile models with MLC LLM.
If you want to build webgpu, please also complete <a class="reference internal" href="../install/emcc.html#install-web-build"><span class="std std-ref">Install Wasm Build Environment</span></a>.
Please also follow the instructions in <a class="reference internal" href="../deploy/cli.html#deploy-cli"><span class="std std-ref">CLI and C++ API</span></a> to obtain the CLI app that can be used to chat with the compiled model.
Finally, we strongly recommend you read <a class="reference internal" href="../get_started/project_overview.html#project-overview"><span class="std std-ref">Project Overview</span></a> first to get familiarized with the high-level terminologies.</p>
</div>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#install-mlc-llm-package" id="id8">Install MLC-LLM Package</a></p></li>
<li><p><a class="reference internal" href="#get-started" id="id9">Get Started</a></p></li>
<li><p><a class="reference internal" href="#compile-command-specification" id="id10">Compile Command Specification</a></p></li>
<li><p><a class="reference internal" href="#more-model-compile-commands" id="id11">More Model Compile Commands</a></p></li>
</ul>
</nav>
<section id="install-mlc-llm-package">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Install MLC-LLM Package</a><a class="headerlink" href="#install-mlc-llm-package" title="Permalink to this heading">Â¶</a></h2>
<section id="work-with-source-code">
<h3>Work with Source Code<a class="headerlink" href="#work-with-source-code" title="Permalink to this heading">Â¶</a></h3>
<p>The easiest way to use MLC-LLM is to clone the repository, and compile models under the root directory of the repository.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># clone the repository</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mlc-ai/mlc-llm.git<span class="w"> </span>--recursive
<span class="c1"># enter to root directory of the repo</span>
<span class="nb">cd</span><span class="w"> </span>mlc-llm
<span class="c1"># install mlc-llm</span>
pip<span class="w"> </span>install<span class="w"> </span>.
</pre></div>
</div>
</section>
<section id="verify-installation">
<h3>Verify Installation<a class="headerlink" href="#verify-installation" title="Permalink to this heading">Â¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--help
</pre></div>
</div>
<p>You are expected to see the help information of the building script.</p>
</section>
</section>
<section id="get-started">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Get Started</a><a class="headerlink" href="#get-started" title="Permalink to this heading">Â¶</a></h2>
<p>This section provides a step by step instructions to guide you through the compilation process of one specific model.
We take the RedPajama-v1-3B as an example.
You can select the platform where you want to <strong>run</strong> your model from the tabs below and run the corresponding command.
We strongly recommend you <strong>start with Metal/CUDA/Vulkan</strong> as it is easier to validate the compilation result on
your personal computer.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-TWV0YWw=" aria-selected="true" class="sphinx-tabs-tab group-tab" id="tab-0-TWV0YWw=" name="TWV0YWw=" role="tab" tabindex="0">Metal</button><button aria-controls="panel-0-TGludXggLSBDVURB" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-TGludXggLSBDVURB" name="TGludXggLSBDVURB" role="tab" tabindex="-1">Linux - CUDA</button><button aria-controls="panel-0-VnVsa2Fu" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-VnVsa2Fu" name="VnVsa2Fu" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-0-aU9TL2lQYWRPUw==" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-aU9TL2lQYWRPUw==" name="aU9TL2lQYWRPUw==" role="tab" tabindex="-1">iOS/iPadOS</button><button aria-controls="panel-0-QW5kcm9pZA==" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-QW5kcm9pZA==" name="QW5kcm9pZA==" role="tab" tabindex="-1">Android</button><button aria-controls="panel-0-V2ViR1BV" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-V2ViR1BV" name="V2ViR1BV" role="tab" tabindex="-1">WebGPU</button></div><div aria-labelledby="tab-0-TWV0YWw=" class="sphinx-tabs-panel group-tab" id="panel-0-TWV0YWw=" name="TWV0YWw=" role="tabpanel" tabindex="0"><p>On Apple Silicon powered Mac, compile for Apple Silicon Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="w"> </span>togethercomputer/RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>metal<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
<p>On Apple Silicon powered Mac, compile for x86 Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="w"> </span>togethercomputer/RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>metal_x86_64<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-0-TGludXggLSBDVURB" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-TGludXggLSBDVURB" name="TGludXggLSBDVURB" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="w"> </span>togethercomputer/RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>cuda<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-0-VnVsa2Fu" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-VnVsa2Fu" name="VnVsa2Fu" role="tabpanel" tabindex="0"><p>On Linux, compile for Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="w"> </span>togethercomputer/RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
<p>On Linux, compile for Windows: please first install the <a class="reference external" href="https://github.com/mstorsjo/llvm-mingw">LLVM-MinGW</a> toolchain, and substitute the <code class="docutils literal notranslate"><span class="pre">path/to/llvm-mingw</span></code> in the command with your LLVM-MinGW installation path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="w"> </span>togethercomputer/RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--llvm-mingw<span class="w"> </span>path/to/llvm-mingw
</pre></div>
</div>
</div><div aria-labelledby="tab-0-aU9TL2lQYWRPUw==" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-aU9TL2lQYWRPUw==" name="aU9TL2lQYWRPUw==" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="w"> </span>togethercomputer/RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>iphone<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If it runs into error</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Compilation error:
xcrun: error: unable to find utility &quot;metal&quot;, not a developer tool or in PATH
xcrun: error: unable to find utility &quot;metallib&quot;, not a developer tool or in PATH
</pre></div>
</div>
<p>, please check and make sure you have Command Line Tools for Xcode installed correctly.
You can use <code class="docutils literal notranslate"><span class="pre">xcrun</span> <span class="pre">metal</span></code> to validate: when it prints <code class="docutils literal notranslate"><span class="pre">metal:</span> <span class="pre">error:</span> <span class="pre">no</span> <span class="pre">input</span> <span class="pre">files</span></code>, it means the Command Line Tools for Xcode is installed and can be found, and you can proceed with the model compiling.</p>
</div>
</div><div aria-labelledby="tab-0-QW5kcm9pZA==" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-QW5kcm9pZA==" name="QW5kcm9pZA==" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="w"> </span>togethercomputer/RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>android<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-0-V2ViR1BV" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-V2ViR1BV" name="V2ViR1BV" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="w"> </span>togethercomputer/RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>webgpu<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div></div>
<p>By executing the compile command above, we generate the model weights, model lib, and a chat config.
We can check the output with the commands below:</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-1-TWV0YWw=" aria-selected="true" class="sphinx-tabs-tab group-tab" id="tab-1-TWV0YWw=" name="TWV0YWw=" role="tab" tabindex="0">Metal</button><button aria-controls="panel-1-TGludXggLSBDVURB" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-1-TGludXggLSBDVURB" name="TGludXggLSBDVURB" role="tab" tabindex="-1">Linux - CUDA</button><button aria-controls="panel-1-VnVsa2Fu" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-1-VnVsa2Fu" name="VnVsa2Fu" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-1-aU9TL2lQYWRPUw==" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-1-aU9TL2lQYWRPUw==" name="aU9TL2lQYWRPUw==" role="tab" tabindex="-1">iOS/iPadOS</button><button aria-controls="panel-1-QW5kcm9pZA==" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-1-QW5kcm9pZA==" name="QW5kcm9pZA==" role="tab" tabindex="-1">Android</button><button aria-controls="panel-1-V2ViR1BV" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-1-V2ViR1BV" name="V2ViR1BV" role="tab" tabindex="-1">WebGPU</button></div><div aria-labelledby="tab-1-TWV0YWw=" class="sphinx-tabs-panel group-tab" id="panel-1-TWV0YWw=" name="TWV0YWw=" role="tabpanel" tabindex="0"><blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1
<span class="w">  </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1-metal.so<span class="w">     </span><span class="c1"># ===&gt; the model library</span>
<span class="w">  </span>mod_cache_before_build_metal.pkl<span class="w">                 </span><span class="c1"># ===&gt; a cached file for future builds</span>
<span class="w">  </span>params<span class="w">                                           </span><span class="c1"># ===&gt; containing the model weights, tokenizer and chat config</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/params
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>ndarray-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>We now chat with the model using the command line interface (CLI) app.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run CLI</span>
mlc_chat_cli<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1
</pre></div>
</div>
</div></blockquote>
<p>The CLI will use the config file <code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/params/mlc-chat-config.json</span></code>
and model library <code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/RedPajama-INCITE-Chat-3B-v1-q4f16_1-metal.so</span></code>.</p>
</div><div aria-labelledby="tab-1-TGludXggLSBDVURB" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-1-TGludXggLSBDVURB" name="TGludXggLSBDVURB" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1
<span class="w">  </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1-cuda.so<span class="w">      </span><span class="c1"># ===&gt; the model library</span>
<span class="w">  </span>mod_cache_before_build_cuda.pkl<span class="w">                  </span><span class="c1"># ===&gt; a cached file for future builds</span>
<span class="w">  </span>params<span class="w">                                           </span><span class="c1"># ===&gt; containing the model weights, tokenizer and chat config</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/params
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>ndarray-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>We now chat with the model using the command line interface (CLI) app.
Follow the build from the source instruction</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run CLI</span>
mlc_chat_cli<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1
</pre></div>
</div>
<p>The CLI app using config file <code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/params/mlc-chat-config.json</span></code>
and model library <code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/RedPajama-INCITE-Chat-3B-v1-q4f16_1-cuda.so</span></code>.</p>
</div><div aria-labelledby="tab-1-VnVsa2Fu" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-1-VnVsa2Fu" name="VnVsa2Fu" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1
<span class="w">  </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1-vulkan.so<span class="w">    </span><span class="c1"># ===&gt; the model library (will be .dll when built for Windows)</span>
<span class="w">  </span>mod_cache_before_build_vulkan.pkl<span class="w">                </span><span class="c1"># ===&gt; a cached file for future builds</span>
<span class="w">  </span>params<span class="w">                                           </span><span class="c1"># ===&gt; containing the model weights, tokenizer and chat config</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/params
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>ndarray-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>We can further quickly run and validate the model compilation using the command line interface (CLI) app.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run CLI</span>
mlc_chat_cli<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1
</pre></div>
</div>
<p>CLI app will use config file <code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/params/mlc-chat-config.json</span></code>
and model library <code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/RedPajama-INCITE-Chat-3B-v1-q4f16_1-vulkan.so</span></code> (or <code class="docutils literal notranslate"><span class="pre">.dll</span></code>).</p>
</div><div aria-labelledby="tab-1-aU9TL2lQYWRPUw==" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-1-aU9TL2lQYWRPUw==" name="aU9TL2lQYWRPUw==" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1
<span class="w">  </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1-iphone.tar<span class="w">   </span><span class="c1"># ===&gt; the model library</span>
<span class="w">  </span>mod_cache_before_build_iphone.pkl<span class="w">                </span><span class="c1"># ===&gt; a cached file for future builds</span>
<span class="w">  </span>params<span class="w">                                           </span><span class="c1"># ===&gt; containing the model weights, tokenizer and chat config</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/params
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>ndarray-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>The model lib <code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/RedPajama-INCITE-Chat-3B-v1-q4f16_1-iphone.tar</span></code>
will be packaged as a static library into the iOS app. Checkout <a class="reference internal" href="../deploy/ios.html#deploy-ios"><span class="std std-ref">iOS App and Swift API</span></a> for more details.</p>
</div><div aria-labelledby="tab-1-QW5kcm9pZA==" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-1-QW5kcm9pZA==" name="QW5kcm9pZA==" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1
<span class="w">  </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1-android.tar<span class="w">  </span><span class="c1"># ===&gt; the model library</span>
<span class="w">  </span>mod_cache_before_build_android.pkl<span class="w">               </span><span class="c1"># ===&gt; a cached file for future builds</span>
<span class="w">  </span>params<span class="w">                                           </span><span class="c1"># ===&gt; containing the model weights, tokenizer and chat config</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/params
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>ndarray-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>The model lib <code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/RedPajama-INCITE-Chat-3B-v1-q4f16_1-android.tar</span></code>
will be packaged as a static library into the android app. Checkout <a class="reference internal" href="../deploy/android.html#deploy-android"><span class="std std-ref">Android App</span></a> for more details.</p>
</div><div aria-labelledby="tab-1-V2ViR1BV" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-1-V2ViR1BV" name="V2ViR1BV" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1
<span class="w">  </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1-webgpu.wasm<span class="w">  </span><span class="c1"># ===&gt; the model library</span>
<span class="w">  </span>mod_cache_before_build_webgpu.pkl<span class="w">                </span><span class="c1"># ===&gt; a cached file for future builds</span>
<span class="w">  </span>params<span class="w">                                           </span><span class="c1"># ===&gt; containing the model weights, tokenizer and chat config</span>

~/mlc-llm<span class="w"> </span>&gt;<span class="w"> </span>ls<span class="w"> </span>dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/params
<span class="w">  </span>mlc-chat-config.json<span class="w">                             </span><span class="c1"># ===&gt; the chat config</span>
<span class="w">  </span>ndarray-cache.json<span class="w">                               </span><span class="c1"># ===&gt; the model weight info</span>
<span class="w">  </span>params_shard_0.bin<span class="w">                               </span><span class="c1"># ===&gt; the model weights</span>
<span class="w">  </span>params_shard_1.bin
<span class="w">  </span>...
<span class="w">  </span>tokenizer.json<span class="w">                                   </span><span class="c1"># ===&gt; the tokenizer files</span>
<span class="w">  </span>tokenizer_config.json
</pre></div>
</div>
<p>The model lib <code class="docutils literal notranslate"><span class="pre">dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1/RedPajama-INCITE-Chat-3B-v1-q4f16_1-webgpu.wasm</span></code>
can be uploaded to the internet. You can pass a <code class="docutils literal notranslate"><span class="pre">model_lib_map</span></code> field to WebLLM app config to use this library.</p>
</div></div>
<p>Each compilation target produces a specific model library for the given platform. The model weight is shared across
different targets. If you are interested in distributing the model besides local execution, please checkout <a class="reference internal" href="distribute_compiled_models.html#distribute-compiled-models"><span class="std std-ref">Distribute Compiled Models</span></a>.
You are also more than welcome to read the following sections for more details about the compilation.</p>
</section>
<section id="compile-command-specification">
<span id="id2"></span><h2><a class="toc-backref" href="#id10" role="doc-backlink">Compile Command Specification</a><a class="headerlink" href="#compile-command-specification" title="Permalink to this heading">Â¶</a></h2>
<p>This section describes the list of options that can be used during compilation.
Note that the arguments are generated by the dataclass <code class="xref py py-class docutils literal notranslate"><span class="pre">BuildArgs</span></code>, read
more in <a class="reference internal" href="python.html#api-reference-compile-model"><span class="std std-ref">API Reference</span></a>.
Generally, the model compile command is specified by a sequence of arguments and in the following pattern:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>MODEL_NAME_OR_PATH<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--hf-path<span class="w"> </span>HUGGINGFACE_NAME<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--target<span class="w"> </span>TARGET_NAME<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--quantization<span class="w"> </span>QUANTIZATION_MODE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--max-seq-len<span class="w"> </span>MAX_ALLOWED_SEQUENCE_LENGTH<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--reuse-lib<span class="w"> </span>LIB_NAME<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--use-cache<span class="o">=</span><span class="m">0</span><span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--debug-dump<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--use-safetensors<span class="o">]</span>
</pre></div>
</div>
<p>This command first goes with <code class="docutils literal notranslate"><span class="pre">--model</span></code> or <code class="docutils literal notranslate"><span class="pre">--hf-path</span></code>.
<strong>Only one of them needs to be specified</strong>: when the model is publicly available on Hugging Face, you can use <code class="docutils literal notranslate"><span class="pre">--hf-path</span></code> to specify the model.
In other cases you need to specify the model via <code class="docutils literal notranslate"><span class="pre">--model</span></code>.</p>
<dl class="option-list">
<dt><kbd><span class="option">--model <var>MODEL_NAME_OR_PATH</var></span></kbd></dt>
<dd><p>The name or local path of the model to compile.
We will search for the model on your disk in the following two candidates:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dist/models/MODEL_NAME_OR_PATH</span></code> (e.g., <code class="docutils literal notranslate"><span class="pre">--model</span> <span class="pre">Llama-2-7b-chat-hf</span></code>),</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MODEL_NAME_OR_PATH</span></code> (e.g., <code class="docutils literal notranslate"><span class="pre">--model</span> <span class="pre">/my-model/Llama-2-7b-chat-hf</span></code>).</p></li>
</ul>
<p>When running the compile command using <code class="docutils literal notranslate"><span class="pre">--model</span></code>, please make sure you have placed the model to compile under <code class="docutils literal notranslate"><span class="pre">dist/models/</span></code> or another location on the disk.</p>
</dd>
<dt><kbd><span class="option">--hf-path <var>HUGGINGFACE_NAME</var></span></kbd></dt>
<dd><p>The name of the modelâ€™s Hugging Face repository.
We will download the model to <code class="docutils literal notranslate"><span class="pre">dist/models/HUGGINGFACE_NAME</span></code> and load the model from this directory.</p>
<p>For example, by specifying <code class="docutils literal notranslate"><span class="pre">--hf-path</span> <span class="pre">togethercomputer/RedPajama-INCITE-Chat-3B-v1</span></code>, it will download the model from <code class="docutils literal notranslate"><span class="pre">https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1</span></code> to <code class="docutils literal notranslate"><span class="pre">dist/models/</span></code>.</p>
</dd>
</dl>
<p>Another two necessary arguments for the compile command are the target and the quantization mode:</p>
<dl class="option-list">
<dt><kbd><span class="option">--target <var>TARGET_NAME</var></span></kbd></dt>
<dd><p>The target platform to compile the model for.
The default target is <code class="docutils literal notranslate"><span class="pre">auto</span></code>, using which we will detect from <code class="docutils literal notranslate"><span class="pre">cuda</span></code>, <code class="docutils literal notranslate"><span class="pre">metal</span></code>, <code class="docutils literal notranslate"><span class="pre">vulkan</span></code> and <code class="docutils literal notranslate"><span class="pre">opencl</span></code>.
Besides <code class="docutils literal notranslate"><span class="pre">auto</span></code>, other available options are: <code class="docutils literal notranslate"><span class="pre">metal</span></code> (for M1/M2), <code class="docutils literal notranslate"><span class="pre">metal_x86_64</span></code> (for Intel CPU), <code class="docutils literal notranslate"><span class="pre">iphone</span></code>,
<code class="docutils literal notranslate"><span class="pre">vulkan</span></code>, <code class="docutils literal notranslate"><span class="pre">cuda</span></code>, <code class="docutils literal notranslate"><span class="pre">webgpu</span></code>, <code class="docutils literal notranslate"><span class="pre">android</span></code>, and <code class="docutils literal notranslate"><span class="pre">opencl</span></code>.</p>
</dd>
<dt><kbd><span class="option">--quantization <var>QUANTIZATION_MODE</var></span></kbd></dt>
<dd><p>The quantization mode we use to compile.
The format of the code is <code class="docutils literal notranslate"><span class="pre">qAfB(_0)</span></code>, where <code class="docutils literal notranslate"><span class="pre">A</span></code> represents the number of bits for storing weights and <code class="docutils literal notranslate"><span class="pre">B</span></code> represents the number of bits for storing activations.
Available options are: <code class="docutils literal notranslate"><span class="pre">q3f16_0</span></code>, <code class="docutils literal notranslate"><span class="pre">q4f16_1</span></code>, <code class="docutils literal notranslate"><span class="pre">q4f16_2</span></code>, <code class="docutils literal notranslate"><span class="pre">q4f32_0</span></code>, <code class="docutils literal notranslate"><span class="pre">q0f32</span></code>, and <code class="docutils literal notranslate"><span class="pre">q0f16</span></code>.
We encourage you to use 4-bit quantization, as the text generated by 3-bit quantized models may have bad quality depending on the model.</p>
</dd>
</dl>
<p>The following arguments are optional:</p>
<dl class="option-list">
<dt><kbd><span class="option">--max-seq-len <var>MAX_ALLOWED_SEQUENCE_LENGTH</var></span></kbd></dt>
<dd><p>The maximum allowed sequence length for the model.
When it is not specified,
we will use the maximum sequence length from the <code class="docutils literal notranslate"><span class="pre">config.json</span></code> in the model directory.</p>
</dd>
<dt><kbd><span class="option">--reuse-lib <var>LIB_NAME</var></span></kbd></dt>
<dd><p>Specifies the previously generated library to reuse.
This is useful when building the same model architecture with different weights.
You can refer to the <a class="reference internal" href="distribute_compiled_models.html#distribute-model-step3-specify-model-lib"><span class="std std-ref">model distribution</span></a> page for details of this argument.</p>
</dd>
<dt><kbd><span class="option">--use-cache</span></kbd></dt>
<dd><p>When <code class="docutils literal notranslate"><span class="pre">--use-cache=0</span></code> is specified,
the model compilation will not use cached file from previous builds,
and will compile the model from the very start.
Using a cache can help reduce the time needed to compile.</p>
</dd>
<dt><kbd><span class="option">--debug-dump</span></kbd></dt>
<dd><p>Specifies whether to dump debugging files during compilation.</p>
</dd>
<dt><kbd><span class="option">--use-safetensors</span></kbd></dt>
<dd><p>Specifies whether to use <code class="docutils literal notranslate"><span class="pre">.safetensors</span></code> instead of the default <code class="docutils literal notranslate"><span class="pre">.bin</span></code> when loading in model weights.</p>
</dd>
</dl>
</section>
<section id="more-model-compile-commands">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">More Model Compile Commands</a><a class="headerlink" href="#more-model-compile-commands" title="Permalink to this heading">Â¶</a></h2>
<p>This section lists compile commands for more models that you can try out.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-2-2-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-2-2-0" name="2-0" role="tab" tabindex="0">Model: Llama-2-7B</button><button aria-controls="panel-2-2-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-1" name="2-1" role="tab" tabindex="-1">Vicuna-v1-7B</button><button aria-controls="panel-2-2-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-2" name="2-2" role="tab" tabindex="-1">RedPajama-v1-3B</button><button aria-controls="panel-2-2-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-3" name="2-3" role="tab" tabindex="-1">rwkv-raven-1b5/3b/7b</button><button aria-controls="panel-2-2-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-4" name="2-4" role="tab" tabindex="-1">Other models</button></div><div aria-labelledby="tab-2-2-0" class="sphinx-tabs-panel" id="panel-2-2-0" name="2-0" role="tabpanel" tabindex="0"><p>Please <a class="reference external" href="https://huggingface.co/meta-llama">request for access</a> to the Llama-2 weights from Meta first.
After granted access, please create directory <code class="docutils literal notranslate"><span class="pre">dist/models</span></code> and download the model to the directory.
For example, you can run the following code:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>dist/models
<span class="nb">cd</span><span class="w"> </span>dist/models
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
<span class="nb">cd</span><span class="w"> </span>../..
</pre></div>
</div>
<p>After downloading the model, run the following command to compile the model.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-3-3-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-3-3-0" name="3-0" role="tab" tabindex="0">Target: CUDA</button><button aria-controls="panel-3-3-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-1" name="3-1" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-3-3-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-2" name="3-2" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-3-3-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-3" name="3-3" role="tab" tabindex="-1">WebGPU</button><button aria-controls="panel-3-3-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-4" name="3-4" role="tab" tabindex="-1">iPhone/iPad</button><button aria-controls="panel-3-3-5" aria-selected="false" class="sphinx-tabs-tab" id="tab-3-3-5" name="3-5" role="tab" tabindex="-1">Android</button></div><div aria-labelledby="tab-3-3-0" class="sphinx-tabs-panel" id="panel-3-3-0" name="3-0" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>Llama-2-7b-chat-hf<span class="w"> </span>--target<span class="w"> </span>cuda<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-1" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-1" name="3-1" role="tabpanel" tabindex="0"><p>On Apple Silicon powered Mac, compile for Apple Silicon Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>Llama-2-7b-chat-hf<span class="w"> </span>--target<span class="w"> </span>metal<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
<p>On Apple Silicon powered Mac, compile for x86 Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>Llama-2-7b-chat-hf<span class="w"> </span>--target<span class="w"> </span>metal_x86_64<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-2" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-2" name="3-2" role="tabpanel" tabindex="0"><p>On Linux, compile for Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>Llama-2-7b-chat-hf<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
<p>On Linux, compile for Windows: please first install the <a class="reference external" href="https://github.com/mstorsjo/llvm-mingw">LLVM-MinGW</a> toolchain, and substitute the <code class="docutils literal notranslate"><span class="pre">path/to/llvm-mingw</span></code> in the command with your LLVM-MinGW installation path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>Llama-2-7b-chat-hf<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--llvm-mingw<span class="w"> </span>path/to/llvm-mingw
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-3" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-3" name="3-3" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>Llama-2-7b-chat-hf<span class="w"> </span>--target<span class="w"> </span>webgpu<span class="w"> </span>--quantization<span class="w"> </span>q4f32_1
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-4" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-4" name="3-4" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>Llama-2-7b-chat-hf<span class="w"> </span>--target<span class="w"> </span>iphone<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q3f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-3-3-5" class="sphinx-tabs-panel" hidden="true" id="panel-3-3-5" name="3-5" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>Llama-2-7b-chat-hf<span class="w"> </span>--target<span class="w"> </span>android<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div></div>
</div><div aria-labelledby="tab-2-2-1" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-1" name="2-1" role="tabpanel" tabindex="0"><p>Please check this page on <a class="reference internal" href="get-vicuna-weight.html"><span class="doc">how to get the Vicuna model weights</span></a>.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-4-4-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-4-4-0" name="4-0" role="tab" tabindex="0">Target: CUDA</button><button aria-controls="panel-4-4-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-1" name="4-1" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-4-4-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-2" name="4-2" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-4-4-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-3" name="4-3" role="tab" tabindex="-1">WebGPU</button><button aria-controls="panel-4-4-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-4" name="4-4" role="tab" tabindex="-1">iPhone/iPad</button><button aria-controls="panel-4-4-5" aria-selected="false" class="sphinx-tabs-tab" id="tab-4-4-5" name="4-5" role="tab" tabindex="-1">Android</button></div><div aria-labelledby="tab-4-4-0" class="sphinx-tabs-panel" id="panel-4-4-0" name="4-0" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>cuda<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-4-4-1" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-1" name="4-1" role="tabpanel" tabindex="0"><p>On Apple Silicon powered Mac, compile for Apple Silicon Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>metal<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
<p>On Apple Silicon powered Mac, compile for x86 Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>metal_x86_64<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-4-4-2" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-2" name="4-2" role="tabpanel" tabindex="0"><p>On Linux, compile for Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
<p>On Linux, compile for Windows: please first install the <a class="reference external" href="https://github.com/mstorsjo/llvm-mingw">LLVM-MinGW</a> toolchain, and substitute the <code class="docutils literal notranslate"><span class="pre">path/to/llvm-mingw</span></code> in the command with your LLVM-MinGW installation path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--llvm-mingw<span class="w"> </span>path/to/llvm-mingw
</pre></div>
</div>
</div><div aria-labelledby="tab-4-4-3" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-3" name="4-3" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>webgpu<span class="w"> </span>--quantization<span class="w"> </span>q4f32_1
</pre></div>
</div>
</div><div aria-labelledby="tab-4-4-4" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-4" name="4-4" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>iphone<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q3f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-4-4-5" class="sphinx-tabs-panel" hidden="true" id="panel-4-4-5" name="4-5" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>vicuna-v1-7b<span class="w"> </span>--target<span class="w"> </span>android<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div></div>
</div><div aria-labelledby="tab-2-2-2" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-2" name="2-2" role="tabpanel" tabindex="0"><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-5-5-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-5-5-0" name="5-0" role="tab" tabindex="0">Target: CUDA</button><button aria-controls="panel-5-5-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-5-5-1" name="5-1" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-5-5-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-5-5-2" name="5-2" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-5-5-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-5-5-3" name="5-3" role="tab" tabindex="-1">WebGPU</button><button aria-controls="panel-5-5-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-5-5-4" name="5-4" role="tab" tabindex="-1">iPhone/iPad</button><button aria-controls="panel-5-5-5" aria-selected="false" class="sphinx-tabs-tab" id="tab-5-5-5" name="5-5" role="tab" tabindex="-1">Android</button></div><div aria-labelledby="tab-5-5-0" class="sphinx-tabs-panel" id="panel-5-5-0" name="5-0" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>cuda<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-5-5-1" class="sphinx-tabs-panel" hidden="true" id="panel-5-5-1" name="5-1" role="tabpanel" tabindex="0"><p>On Apple Silicon powered Mac, compile for Apple Silicon Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>metal<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
<p>On Apple Silicon powered Mac, compile for x86 Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>metal_x86_64<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-5-5-2" class="sphinx-tabs-panel" hidden="true" id="panel-5-5-2" name="5-2" role="tabpanel" tabindex="0"><p>On Linux, compile for Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
<p>On Linux, compile for Windows: please first install the <a class="reference external" href="https://github.com/mstorsjo/llvm-mingw">LLVM-MinGW</a> toolchain, and substitute the <code class="docutils literal notranslate"><span class="pre">path/to/llvm-mingw</span></code> in the command with your LLVM-MinGW installation path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--llvm-mingw<span class="w"> </span>path/to/llvm-mingw
</pre></div>
</div>
</div><div aria-labelledby="tab-5-5-3" class="sphinx-tabs-panel" hidden="true" id="panel-5-5-3" name="5-3" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>webgpu<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-5-5-4" class="sphinx-tabs-panel" hidden="true" id="panel-5-5-4" name="5-4" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>iphone<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-5-5-5" class="sphinx-tabs-panel" hidden="true" id="panel-5-5-5" name="5-5" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1<span class="w"> </span>--target<span class="w"> </span>android<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div></div>
</div><div aria-labelledby="tab-2-2-3" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-3" name="2-3" role="tabpanel" tabindex="0"><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-6-6-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-6-6-0" name="6-0" role="tab" tabindex="0">Target: CUDA</button><button aria-controls="panel-6-6-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-6-6-1" name="6-1" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-6-6-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-6-6-2" name="6-2" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-6-6-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-6-6-3" name="6-3" role="tab" tabindex="-1">iPhone/iPad</button></div><div aria-labelledby="tab-6-6-0" class="sphinx-tabs-panel" id="panel-6-6-0" name="6-0" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># For 1.5B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-1b5<span class="w"> </span>--target<span class="w"> </span>cuda<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
<span class="c1"># For 3B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-3b<span class="w"> </span>--target<span class="w"> </span>cuda<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
<span class="c1"># For 7B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-7b<span class="w"> </span>--target<span class="w"> </span>cuda<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
</pre></div>
</div>
</div><div aria-labelledby="tab-6-6-1" class="sphinx-tabs-panel" hidden="true" id="panel-6-6-1" name="6-1" role="tabpanel" tabindex="0"><p>On Apple Silicon powered Mac, compile for Apple Silicon Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># For 1.5B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-1b5<span class="w"> </span>--target<span class="w"> </span>metal<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
<span class="c1"># For 3B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-3b<span class="w"> </span>--target<span class="w"> </span>metal<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
<span class="c1"># For 7B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-7b<span class="w"> </span>--target<span class="w"> </span>metal<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
</pre></div>
</div>
<p>On Apple Silicon powered Mac, compile for x86 Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># For 1.5B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-1b5<span class="w"> </span>--target<span class="w"> </span>metal_x86_64<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
<span class="c1"># For 3B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-3b<span class="w"> </span>--target<span class="w"> </span>metal_x86_64<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
<span class="c1"># For 7B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-7b<span class="w"> </span>--target<span class="w"> </span>metal_x86_64<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
</pre></div>
</div>
</div><div aria-labelledby="tab-6-6-2" class="sphinx-tabs-panel" hidden="true" id="panel-6-6-2" name="6-2" role="tabpanel" tabindex="0"><p>On Linux, compile for Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># For 1.5B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-1b5<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
<span class="c1"># For 3B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-3b<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
<span class="c1"># For 7B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-7b<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
</pre></div>
</div>
<p>On Linux, compile for Windows: please first install the <a class="reference external" href="https://github.com/mstorsjo/llvm-mingw">LLVM-MinGW</a> toolchain, and substitute the <code class="docutils literal notranslate"><span class="pre">path/to/llvm-mingw</span></code> in the command with your LLVM-MinGW installation path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># For 1.5B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-1b5<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2<span class="w"> </span>--llvm-mingw<span class="w"> </span>path/to/llvm-mingw
<span class="c1"># For 3B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-3b<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2<span class="w"> </span>--llvm-mingw<span class="w"> </span>path/to/llvm-mingw
<span class="c1"># For 7B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-7b<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2<span class="w"> </span>--llvm-mingw<span class="w"> </span>path/to/llvm-mingw
</pre></div>
</div>
</div><div aria-labelledby="tab-6-6-3" class="sphinx-tabs-panel" hidden="true" id="panel-6-6-3" name="6-3" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># For 1.5B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-1b5<span class="w"> </span>--target<span class="w"> </span>iphone<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
<span class="c1"># For 3B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-3b<span class="w"> </span>--target<span class="w"> </span>iphone<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
<span class="c1"># For 7B model</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--hf-path<span class="o">=</span>RWKV/rwkv-raven-7b<span class="w"> </span>--target<span class="w"> </span>iphone<span class="w"> </span>--quantization<span class="w"> </span>q4f16_2
</pre></div>
</div>
</div></div>
</div><div aria-labelledby="tab-2-2-4" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-4" name="2-4" role="tabpanel" tabindex="0"><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-7-7-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-7-7-0" name="7-0" role="tab" tabindex="0">Target: CUDA</button><button aria-controls="panel-7-7-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-7-7-1" name="7-1" role="tab" tabindex="-1">Metal</button><button aria-controls="panel-7-7-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-7-7-2" name="7-2" role="tab" tabindex="-1">Vulkan</button><button aria-controls="panel-7-7-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-7-7-3" name="7-3" role="tab" tabindex="-1">WebGPU</button><button aria-controls="panel-7-7-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-7-7-4" name="7-4" role="tab" tabindex="-1">iPhone/iPad</button><button aria-controls="panel-7-7-5" aria-selected="false" class="sphinx-tabs-tab" id="tab-7-7-5" name="7-5" role="tab" tabindex="-1">Android</button></div><div aria-labelledby="tab-7-7-0" class="sphinx-tabs-panel" id="panel-7-7-0" name="7-0" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>cuda<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-7-7-1" class="sphinx-tabs-panel" hidden="true" id="panel-7-7-1" name="7-1" role="tabpanel" tabindex="0"><p>On Apple Silicon powered Mac, compile for Apple Silicon Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>metal<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
<p>On Apple Silicon powered Mac, compile for x86 Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>metal_x86_64<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-7-7-2" class="sphinx-tabs-panel" hidden="true" id="panel-7-7-2" name="7-2" role="tabpanel" tabindex="0"><p>On Linux, compile for Linux:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
<p>On Linux, compile for Windows: please first install the <a class="reference external" href="https://github.com/mstorsjo/llvm-mingw">LLVM-MinGW</a> toolchain, and substitute the <code class="docutils literal notranslate"><span class="pre">path/to/llvm-mingw</span></code> in the command with your LLVM-MinGW installation path.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>vulkan<span class="w"> </span>--quantization<span class="w"> </span>q4f16_1<span class="w"> </span>--llvm-mingw<span class="w"> </span>path/to/llvm-mingw
</pre></div>
</div>
</div><div aria-labelledby="tab-7-7-3" class="sphinx-tabs-panel" hidden="true" id="panel-7-7-3" name="7-3" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>webgpu<span class="w"> </span>--quantization<span class="w"> </span>q4f32_0
</pre></div>
</div>
</div><div aria-labelledby="tab-7-7-4" class="sphinx-tabs-panel" hidden="true" id="panel-7-7-4" name="7-4" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>iphone<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div><div aria-labelledby="tab-7-7-5" class="sphinx-tabs-panel" hidden="true" id="panel-7-7-5" name="7-5" role="tabpanel" tabindex="0"><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and put the model to `dist/models/MODEL_NAME`, and then run</span>
python3<span class="w"> </span>-m<span class="w"> </span>mlc_llm.build<span class="w"> </span>--model<span class="w"> </span>MODEL_NAME<span class="w"> </span>--target<span class="w"> </span>android<span class="w"> </span>--max-seq-len<span class="w"> </span><span class="m">768</span><span class="w"> </span>--quantization<span class="w"> </span>q4f16_1
</pre></div>
</div>
</div></div>
</div></div>
<p>For each model and each backend, the above only provides the most recommended build command (which is the most optimized). You can also try with different argument values (e.g., different quantization modes), whose build results may not run as fast and robustly as the provided one when running the model.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Uing 3-bit quantization usually can be overly aggressive and only works for limited settings.
If you encounter issues where the compiled model does not perform as expected,
consider utilizing a higher number of bits for quantization (e.g., 4-bit quantization).</p>
</div>
<p>If you are interested in distributing the model besides local execution, please checkout <a class="reference internal" href="distribute_compiled_models.html#distribute-compiled-models"><span class="std std-ref">Distribute Compiled Models</span></a>.</p>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="distribute_compiled_models.html" class="btn btn-neutral float-right" title="Distribute Compiled Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../deploy/android.html" class="btn btn-neutral float-left" title="Android App" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">Â© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
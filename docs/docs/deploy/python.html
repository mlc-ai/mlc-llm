





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Python API and Gradio Frontend &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/tabs.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="iOS App and Swift API" href="ios.html" />
    <link rel="prev" title="CLI and C++ API" href="cli.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/project_overview.html">Project Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/mlc_chat_config.html">Configure MLCChat in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="javascript.html">WebLLM and Javascript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">Rest API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI and C++ API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Python API and Gradio Frontend</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#python-api">Python API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#verify-installation">Verify Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-started">Get Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tutorial-with-python-notebooks">Tutorial with Python Notebooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configure-mlcchat-in-python">Configure MLCChat in Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="#raw-text-generation-in-python">Raw Text Generation in Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stream-iterator-in-python">Stream Iterator in Python</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#api-reference">API Reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mlc_chat.ChatModule"><code class="docutils literal notranslate"><span class="pre">ChatModule</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.__init__"><code class="docutils literal notranslate"><span class="pre">ChatModule.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.benchmark_generate"><code class="docutils literal notranslate"><span class="pre">ChatModule.benchmark_generate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.embed_text"><code class="docutils literal notranslate"><span class="pre">ChatModule.embed_text()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.generate"><code class="docutils literal notranslate"><span class="pre">ChatModule.generate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.reset_chat"><code class="docutils literal notranslate"><span class="pre">ChatModule.reset_chat()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.stats"><code class="docutils literal notranslate"><span class="pre">ChatModule.stats()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#mlc_chat.ChatConfig"><code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#mlc_chat.ConvConfig"><code class="docutils literal notranslate"><span class="pre">ConvConfig</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#mlc_chat.GenerationConfig"><code class="docutils literal notranslate"><span class="pre">GenerationConfig</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gradio-frontend">Gradio Frontend</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android App</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Models via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/distribute_compiled_models.html">Distribute Compiled Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/python.html">Python API for Model Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/configure_quantization.html">ðŸš§ Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/customize/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Python API and Gradio Frontend</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/python.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="python-api-and-gradio-frontend">
<h1>Python API and Gradio Frontend<a class="headerlink" href="#python-api-and-gradio-frontend" title="Permalink to this heading">Â¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#python-api" id="id1">Python API</a></p>
<ul>
<li><p><a class="reference internal" href="#verify-installation" id="id2">Verify Installation</a></p></li>
<li><p><a class="reference internal" href="#get-started" id="id3">Get Started</a></p></li>
<li><p><a class="reference internal" href="#tutorial-with-python-notebooks" id="id4">Tutorial with Python Notebooks</a></p></li>
<li><p><a class="reference internal" href="#configure-mlcchat-in-python" id="id5">Configure MLCChat in Python</a></p></li>
<li><p><a class="reference internal" href="#raw-text-generation-in-python" id="id6">Raw Text Generation in Python</a></p></li>
<li><p><a class="reference internal" href="#stream-iterator-in-python" id="id7">Stream Iterator in Python</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#api-reference" id="id8">API Reference</a></p></li>
<li><p><a class="reference internal" href="#gradio-frontend" id="id9">Gradio Frontend</a></p></li>
</ul>
</nav>
<p>We expose Python API for the MLC-Chat for easy integration into other Python projects.
We also provide a web demo based on <a class="reference external" href="https://gradio.app/">gradio</a> as an example of using Python API to interact with MLC-Chat.</p>
<section id="python-api">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Python API</a><a class="headerlink" href="#python-api" title="Permalink to this heading">Â¶</a></h2>
<p>The Python API is a part of the MLC-Chat package, which we have prepared pre-built pip wheels via the <a class="reference internal" href="../install/mlc_llm.html"><span class="doc">installation page</span></a>.</p>
<section id="verify-installation">
<h3><a class="toc-backref" href="#id2" role="doc-backlink">Verify Installation</a><a class="headerlink" href="#verify-installation" title="Permalink to this heading">Â¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from mlc_chat import ChatModule; print(ChatModule)&quot;</span>
</pre></div>
</div>
<p>You are expected to see the information about the <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code></a> class.</p>
<p>If the prebuilt is unavailable on your platform, or you would like to build a runtime
that supports other GPU runtime than the prebuilt version. Please refer our <a class="reference internal" href="rest.html#mlcchat-package-build-from-source"><span class="std std-ref">Build MLC-Chat Package From Source</span></a> tutorial.</p>
</section>
<section id="get-started">
<h3><a class="toc-backref" href="#id3" role="doc-backlink">Get Started</a><a class="headerlink" href="#get-started" title="Permalink to this heading">Â¶</a></h3>
<p>After confirming that the package <code class="docutils literal notranslate"><span class="pre">mlc_chat</span></code> is installed, we can follow the steps
below to chat with a MLC-compiled model in Python.</p>
<p>First, let us make sure that the MLC-compiled <code class="docutils literal notranslate"><span class="pre">model</span></code> we want to chat with already exists.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">model</span></code> has the format <code class="docutils literal notranslate"><span class="pre">f&quot;{model_name}-{quantize_mode}&quot;</span></code>. For instance, if
you used <code class="docutils literal notranslate"><span class="pre">q4f16_1</span></code> as the <code class="docutils literal notranslate"><span class="pre">quantize_mode</span></code> to compile <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf</span></code>, you
would have <code class="docutils literal notranslate"><span class="pre">model</span></code> being <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>.</p>
</div>
<p>If you do not have the MLC-compiled <code class="docutils literal notranslate"><span class="pre">model</span></code> ready:</p>
<ul class="simple">
<li><p>Checkout <a class="reference internal" href="../index.html#get-started"><span class="std std-ref">Try out MLC Chat</span></a> to download prebuilt models for simplicity, or</p></li>
<li><p>Checkout <a class="reference internal" href="../compilation/compile_models.html#compile-models-via-mlc"><span class="std std-ref">Compile Models via MLC</span></a> to compile models with <code class="docutils literal notranslate"><span class="pre">mlc_llm</span></code> (another package) yourself</p></li>
</ul>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Check prebuilt models</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Check compiled models</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><p>If you downloaded prebuilt models from MLC LLM, by default:</p>
<ul class="simple">
<li><p>Model lib should be placed at <code class="docutils literal notranslate"><span class="pre">./dist/prebuilt/lib/$(model)-$(arch).$(suffix)</span></code>.</p></li>
<li><p>Model weights and chat config are located under <code class="docutils literal notranslate"><span class="pre">./dist/prebuilt/mlc-chat-$(model)/</span></code>.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please make sure that you have the same directory structure as above, because Python API
relies on it to automatically search for model lib and weights. If you would like to directly
provide a full model lib path to override the auto-search, you can specify <code class="docutils literal notranslate"><span class="pre">ChatModule.model_lib_path</span></code></p>
</div>
<details class="summary-example">
<summary>Example</summary><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/prebuilt/lib
Llama-2-7b-chat-hf-q4f16_1-metal.so<span class="w">  </span><span class="c1"># Format: $(model)-$(arch).$(suffix)</span>
Llama-2-7b-chat-hf-q4f16_1-vulkan.so
...
&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1<span class="w">  </span><span class="c1"># Format: ./dist/prebuilt/mlc-chat-$(model)/</span>
<span class="c1"># chat config:</span>
mlc-chat-config.json
<span class="c1"># model weights:</span>
ndarray-cache.json
params_shard_*.bin
...
</pre></div>
</div>
</details></div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><p>If you have compiled models using MLC LLM, by default:</p>
<ul class="simple">
<li><p>Model libraries should be placed at <code class="docutils literal notranslate"><span class="pre">./dist/$(model)/$(model)-$(arch).$(suffix)</span></code>.</p></li>
<li><p>Model weights and chat config are located under <code class="docutils literal notranslate"><span class="pre">./dist/$(model)/params/</span></code>.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please make sure that you have the same directory structure as above, because Python API
relies on it to automatically search for model lib and weights. If you would like to directly
provide a full model lib path to override the auto-search, you can specify <code class="docutils literal notranslate"><span class="pre">ChatModule.model_lib_path</span></code></p>
</div>
<details class="summary-example">
<summary>Example</summary><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1/<span class="w"> </span><span class="c1"># Format: ./dist/$(model)/</span>
Llama-2-7b-chat-hf-q4f16_1-metal.so<span class="w">  </span><span class="c1"># Format: $(model)-$(arch).$(suffix)</span>
...
&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1/params<span class="w">  </span><span class="c1"># Format: ``./dist/$(model)/params/``</span>
<span class="c1"># chat config:</span>
mlc-chat-config.json
<span class="c1"># model weights:</span>
ndarray-cache.json
params_shard_*.bin
...
</pre></div>
</div>
</details></div></div>
<p>After making sure that the files exist, using the conda environment you used
to install <code class="docutils literal notranslate"><span class="pre">mlc_chat</span></code>, from the <code class="docutils literal notranslate"><span class="pre">mlc-llm</span></code> directory, you can create a Python
file <code class="docutils literal notranslate"><span class="pre">sample_mlc_chat.py</span></code> and paste the following lines:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># From the mlc-llm directory, run</span>
<span class="c1"># $ python sample_mlc_chat.py</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>
<span class="c1"># You can change to other models that you downloaded, for example,</span>
<span class="c1"># cm = ChatModule(model=&quot;Llama-2-13b-chat-hf-q4f16_1&quot;)  # Llama2 13b model</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Print prefill and decode performance statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;How many points did you list out?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Reset the chat module by</span>
<span class="c1"># cm.reset_chat()</span>
</pre></div>
</div>
<p>Now run the Python file to start the chat</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>sample_mlc_chat.py
</pre></div>
</div>
<p>You can also checkout the <a class="reference internal" href="../prebuilt_models.html"><span class="doc">Model Prebuilts</span></a> page to run other models.</p>
<details class="summary-see-output">
<summary>See output</summary><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Using model folder: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
Using mlc chat config: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1/mlc-chat-config.json
Using library model: ./dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so

Thank you for your question! The meaning of life is a complex and subjective topic that has been debated by philosophers, theologians, scientists, and many others for centuries. There is no one definitive answer to this question, as it can vary depending on a person&#39;s beliefs, values, experiences, and perspectives.

However, here are some possible ways to approach the question:

1. Religious or spiritual beliefs: Many people believe that the meaning of life is to fulfill a divine or spiritual purpose, whether that be to follow a set of moral guidelines, to achieve spiritual enlightenment, or to fulfill a particular destiny.
2. Personal growth and development: Some people believe that the meaning of life is to learn, grow, and evolve as individuals, to develop one&#39;s talents and abilities, and to become the best version of oneself.
3. Relationships and connections: Others believe that the meaning of life is to form meaningful connections and relationships with others, to love and be loved, and to build a supportive and fulfilling social network.
4. Contribution and impact: Some people believe that the meaning of life is to make a positive impact on the world, to contribute to society in a meaningful way, and to leave a lasting legacy.
5. Simple pleasures and enjoyment: Finally, some people believe that the meaning of life is to simply enjoy the present moment, to find pleasure and happiness in the simple things in life, and to appreciate the beauty and wonder of the world around us.

Ultimately, the meaning of life is a deeply personal and subjective question, and each person must find their own answer based on their own beliefs, values, and experiences.

Statistics: prefill: 3477.5 tok/s, decode: 153.6 tok/s

I listed out 5 possible ways to approach the question of the meaning of life.
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You could also specify the address of <code class="docutils literal notranslate"><span class="pre">model</span></code> and <code class="docutils literal notranslate"><span class="pre">model_lib_path</span></code> explicitly. If
you only specify <code class="docutils literal notranslate"><span class="pre">model</span></code> as <code class="docutils literal notranslate"><span class="pre">model_name</span></code> and <code class="docutils literal notranslate"><span class="pre">quantize_mode</span></code>, we will
do a search for you. See more in the documentation of <a class="reference internal" href="#mlc_chat.ChatModule.__init__" title="mlc_chat.ChatModule.__init__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mlc_chat.ChatModule.__init__()</span></code></a>.</p>
</div>
</section>
<section id="tutorial-with-python-notebooks">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Tutorial with Python Notebooks</a><a class="headerlink" href="#tutorial-with-python-notebooks" title="Permalink to this heading">Â¶</a></h3>
<p>Now that you have tried out how to chat with the model in Python, we would
recommend you to checkout the following tutorials in Python notebook (all runnable in Colab):</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb">Getting Started with MLC-LLM</a>:
how to quickly download prebuilt models and chat with it</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_compile_llama2_with_mlc_llm.ipynb">Compiling Llama-2 with MLC-LLM</a>:
how to use Python APIs to compile models with the MLC-LLM workflow</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_extensions_to_more_model_variants.ipynb">Extensions to More Model Variants</a>:
how to use Python APIs to compile and chat with any model variant youâ€™d like</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_raw_text_generation.ipynb">Raw Text Generation with MLC-LLM</a>:
how to perform raw text generation with MLC-LLM in Python</p></li>
</ul>
</section>
<section id="configure-mlcchat-in-python">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Configure MLCChat in Python</a><a class="headerlink" href="#configure-mlcchat-in-python" title="Permalink to this heading">Â¶</a></h3>
<p>If you have checked out <a class="reference internal" href="../get_started/mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Configure MLCChat in JSON</span></a>, you would know
that you could configure MLCChat through various fields such as <code class="docutils literal notranslate"><span class="pre">temperature</span></code>. We provide the
option of overriding any field youâ€™d like in Python, so that you do not need to manually edit
<code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p>
<p>Since there are two concepts â€“ <cite>MLCChat Configuration</cite> and <cite>Conversation Configuration</cite> â€“ we correspondingly
provide two dataclasses <a class="reference internal" href="#mlc_chat.ChatConfig" title="mlc_chat.ChatConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatConfig</span></code></a> and <a class="reference internal" href="#mlc_chat.ConvConfig" title="mlc_chat.ConvConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ConvConfig</span></code></a>.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span><span class="p">,</span> <span class="n">ChatConfig</span><span class="p">,</span> <span class="n">ConvConfig</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Using a `ConvConfig`, we modify `system`, a field in the conversation template</span>
<span class="c1"># `system` refers to the prompt encoded before starting the chat</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">system</span><span class="o">=</span><span class="s1">&#39;Please show as much happiness as you can when talking to me.&#39;</span><span class="p">)</span>

<span class="c1"># We then include the `ConvConfig` instance in `ChatConfig` while overriding `max_gen_len`</span>
<span class="c1"># Note that `conv_config` is an optional subfield of `chat_config`</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">max_gen_len</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">)</span>

<span class="c1"># Using the `chat_config` we created, instantiate a `ChatModule`</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">mlc_chat</span><span class="o">.</span><span class="n">ChatModule</span><span class="p">(</span><span class="s1">&#39;Llama-2-7b-chat-hf-q4f16_1&#39;</span><span class="p">,</span> <span class="n">chat_config</span><span class="o">=</span><span class="n">chat_config</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is one plus one?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># You could also pass in a `ConvConfig` instance to `reset_chat()`</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">system</span><span class="o">=</span><span class="s1">&#39;Please show as much sadness as you can when talking to me.&#39;</span><span class="p">)</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">max_gen_len</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">)</span>
<span class="n">cm</span><span class="o">.</span><span class="n">reset_chat</span><span class="p">(</span><span class="n">chat_config</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is one plus one?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<details class="summary-see-output">
<summary>See output</summary><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Using model folder: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
Using mlc chat config: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1/mlc-chat-config.json
Using library model: ./dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so

Oh, wow, *excitedly* one plus one? *grinning* Well, let me see... *counting on fingers* One plus one is... *eureka* Two!
...

*Sobs* Oh, the tragedy of it all... *sobs* One plus one... *chokes back tears* It&#39;s... *gulps* it&#39;s... *breaks down in tears* TWO!
...
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You do not need to specify the entire <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> or <code class="docutils literal notranslate"><span class="pre">ConvConfig</span></code>. Instead, we will first
load all the fields defined in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, a file required when instantiating
a <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code></a>. Then, we will load in the optional <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> you provide, overriding the
fields specified.</p>
<p>It is also worth noting that <code class="docutils literal notranslate"><span class="pre">ConvConfig</span></code> itself is overriding the original conversation template
specified by the field <code class="docutils literal notranslate"><span class="pre">conv_template</span></code> in chat configuration. Learn more about it in
<a class="reference internal" href="../get_started/mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Configure MLCChat in JSON</span></a>.</p>
</div>
</section>
<section id="raw-text-generation-in-python">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Raw Text Generation in Python</a><a class="headerlink" href="#raw-text-generation-in-python" title="Permalink to this heading">Â¶</a></h3>
<p>Raw text generation allows the user to have more flexibility over his prompts,
without being forced to create a new conversational template, making prompt customization easier.
This serves other demands for APIs to handle LLM generation without the usual system prompts and other items.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span><span class="p">,</span> <span class="n">ChatConfig</span><span class="p">,</span> <span class="n">ConvConfig</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Use a `ConvConfig` to define the generation settings</span>
<span class="c1"># Since the &quot;LM&quot; template only supports raw text generation,</span>
<span class="c1"># system prompts will not be executed even if provided</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">stop_tokens</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,],</span> <span class="n">add_bos</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stop_str</span><span class="o">=</span><span class="s2">&quot;[INST]&quot;</span><span class="p">)</span>

<span class="c1"># Note that `conv_config` is an optional subfield of `chat_config`</span>
<span class="c1"># The &quot;LM&quot; template serves the basic purposes of raw text generation</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">,</span> <span class="n">conv_template</span><span class="o">=</span><span class="s2">&quot;LM&quot;</span><span class="p">)</span>

<span class="c1"># Using the `chat_config` we created, instantiate a `ChatModule`</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="s1">&#39;Llama-2-7b-chat-hf-q4f16_1&#39;</span><span class="p">,</span> <span class="n">chat_config</span><span class="o">=</span><span class="n">chat_config</span><span class="p">)</span>

<span class="c1"># To make the model follow conversations a chat structure should be provided</span>
<span class="c1"># This allows users to build their own prompts without building a new template</span>
<span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&lt;&lt;SYS&gt;&gt;</span><span class="se">\n</span><span class="s2">You are a helpful, respectful and honest assistant.</span><span class="se">\n</span><span class="s2">&lt;&lt;/SYS&gt;&gt;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
<span class="n">inst_prompt</span> <span class="o">=</span> <span class="s2">&quot;What is mother nature?&quot;</span>

<span class="c1"># Concatenate system and instruction prompts, and add instruction tags</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;[INST] </span><span class="si">{</span><span class="n">system_prompt</span><span class="o">+</span><span class="n">inst_prompt</span><span class="si">}</span><span class="s2"> [/INST]&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># The LM template has no memory, so it will be reset every single generation</span>
<span class="c1"># In this case the model will just follow normal text completion</span>
<span class="c1"># because there isn&#39;t a chat structure</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Life is a quality that distinguishes&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">LM</span></code> is a template without memory, which means that every execution will be cleared.
Additionally, system prompts will not be run when instantiating a <cite>mlc_chat.ChatModule</cite>,
unless explicitly given inside the prompt.</p>
</div>
</section>
<section id="stream-iterator-in-python">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Stream Iterator in Python</a><a class="headerlink" href="#stream-iterator-in-python" title="Permalink to this heading">Â¶</a></h3>
<p>Stream Iterator gives users an option to stream generated text to the function that the API is called from,
instead of streaming to stdout, which could be a necessity when building services on top of MLC Chat.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamIterator</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>

<span class="c1"># Stream to an Iterator</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Thread</span>

<span class="n">stream</span> <span class="o">=</span> <span class="n">StreamIterator</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">generation_thread</span> <span class="o">=</span> <span class="n">Thread</span><span class="p">(</span>
   <span class="n">target</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">,</span>
   <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span> <span class="s2">&quot;progress_callback&quot;</span><span class="p">:</span> <span class="n">stream</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">generation_thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="n">output</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="k">for</span> <span class="n">delta_message</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
   <span class="n">output</span> <span class="o">+=</span> <span class="n">delta_message</span>

<span class="n">generation_thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="api-reference">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">API Reference</a><a class="headerlink" href="#api-reference" title="Permalink to this heading">Â¶</a></h2>
<p>User can initiate a chat module by creating <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code></a> class, which is a wrapper of the MLC-Chat model.
The <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code></a> class provides the following methods:</p>
<dl class="py class">
<dt class="sig sig-object py" id="mlc_chat.ChatModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mlc_chat.</span></span><span class="sig-name descname"><span class="pre">ChatModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chat_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#mlc_chat.ChatConfig" title="mlc_chat.chat_module.ChatConfig"><span class="pre">ChatConfig</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_lib_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>The ChatModule for MLC LLM.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>

<span class="c1"># Generate a response for a given prompt</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span>
    <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Print prefill and decode performance statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;How many points did you list out?&quot;</span><span class="p">,</span>
    <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>str</em>) â€“ The model folder after compiling with MLC-LLM build process. The parameter
can either be the model name with its quantization scheme
(e.g. <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>), or a full path to the model
folder. In the former case, we will use the provided name to search
for the model folder over possible paths.</p></li>
<li><p><strong>device</strong> (<em>str</em>) â€“ The description of the device to run on. User should provide a string in the
form of â€˜device_name:device_idâ€™ or â€˜device_nameâ€™, where â€˜device_nameâ€™ is one of
â€˜cudaâ€™, â€˜metalâ€™, â€˜vulkanâ€™, â€˜rocmâ€™, â€˜openclâ€™, â€˜autoâ€™ (automatically detect the
local device), and â€˜device_idâ€™ is the device id to run on. If no â€˜device_idâ€™
is provided, it will be set to 0 by default.</p></li>
<li><p><strong>chat_config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#mlc_chat.ChatConfig" title="mlc_chat.ChatConfig"><em>ChatConfig</em></a><em>]</em>) â€“ A <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> instance partially filled. Will be used to override the
<code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p></li>
<li><p><strong>model_lib_path</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ The full path to the model library file to use (e.g. a <code class="docutils literal notranslate"><span class="pre">.so</span></code> file).
If unspecified, we will use the provided <code class="docutils literal notranslate"><span class="pre">model</span></code> to search over
possible paths.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chat_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#mlc_chat.ChatConfig" title="mlc_chat.chat_module.ChatConfig"><span class="pre">ChatConfig</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_lib_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule.__init__" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.benchmark_generate">
<span class="sig-name descname"><span class="pre">benchmark_generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#mlc_chat.ChatModule.benchmark_generate" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Controlled generation with input prompt and fixed number of
generated tokens, ignoring system prompt. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">benchmark_generate</span><span class="p">(</span><span class="s2">&quot;What&#39;s the meaning of life?&quot;</span><span class="p">,</span> <span class="n">generate_length</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated text:</span><span class="se">\n</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>will generate 256 tokens in total based on prompt â€œWhatâ€™s the meaning
of life?â€. After generation, you can use <cite>cm.stats()</cite> to print the
generation speed.</p>
<p class="rubric">Notes</p>
<p>1. This function is typically used in controlled benchmarks. It generates
text without system prompt (i.e., it is pure text generation with no chat
style) and ignores the token stop model(s).
2. To make the benchmark as accurate as possible, we first do a round of
warmup prefill and decode before text generation.
3. This function resets the previous performance statistics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prompt</strong> (<em>str</em>) â€“ The prompt of the text generation.</p></li>
<li><p><strong>generate_length</strong> (<em>int</em>) â€“ The target length of generation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>output</strong> â€“ The generated text output.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.embed_text">
<span class="sig-name descname"><span class="pre">embed_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule.embed_text" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Given a text input, returns its embedding in the LLM.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>str</em>) â€“ The user input string.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>embedding</strong> â€“ The embedding of the text.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tvm.runtime.NDArray</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a high-level method and is only used for retrieving text embeddings. Users are
not supposed to call <a class="reference internal" href="#mlc_chat.ChatModule.generate" title="mlc_chat.ChatModule.generate"><code class="xref py py-func docutils literal notranslate"><span class="pre">generate()</span></code></a> after calling this method in the same chat session,
since the input to this method is not prefilled and will cause error. If user needs to
call <a class="reference internal" href="#mlc_chat.ChatModule.generate" title="mlc_chat.ChatModule.generate"><code class="xref py py-func docutils literal notranslate"><span class="pre">generate()</span></code></a> later, please call <a class="reference internal" href="#mlc_chat.ChatModule.reset_chat" title="mlc_chat.ChatModule.reset_chat"><code class="xref py py-func docutils literal notranslate"><span class="pre">reset_chat()</span></code></a> first.
For a more fine-grained embedding API, see <code class="xref py py-func docutils literal notranslate"><span class="pre">_embed()</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generation_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#mlc_chat.GenerationConfig" title="mlc_chat.chat_module.GenerationConfig"><span class="pre">GenerationConfig</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">progress_callback</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#mlc_chat.ChatModule.generate" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A high-level method that returns the full response from the chat module given a user prompt.
User can optionally specify which callback method to use upon receiving the response. By default,
no callback will be applied.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prompt</strong> (<em>str</em>) â€“ The user input prompt, i.e. a question to ask the chat module.</p></li>
<li><p><strong>generation_config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#mlc_chat.GenerationConfig" title="mlc_chat.GenerationConfig"><em>GenerationConfig</em></a><em>]</em>) â€“ The generation config object to override the ChatConfig generation settings.</p></li>
<li><p><strong>progress_callback</strong> (<em>object</em>) â€“ The optional callback method used upon receiving a newly generated message from the chat module.
See <cite>mlc_chat/callback.py</cite> for a full list of available callback classes. Currently, only
streaming to stdout callback method is supported, see <cite>Examples</cite> for more detailed usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>output</strong> â€“ The generated full output from the chat module.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Suppose we would like to stream the response of the chat module to stdout</span>
<span class="c1"># with a refresh interval of 2. Upon calling generate(), We will see the response of</span>
<span class="c1"># the chat module streaming to stdout piece by piece, and in the end we receive the</span>
<span class="c1"># full response as a single string `output`.</span>

<span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span><span class="p">,</span> <span class="n">GenerationConfig</span><span class="p">,</span> <span class="n">callback</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">xxx</span><span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;what&#39;s the color of banana?&quot;</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
  <span class="n">prompt</span><span class="p">,</span> <span class="n">GenerationConfig</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">),</span> <span class="n">callback</span><span class="o">.</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.reset_chat">
<span class="sig-name descname"><span class="pre">reset_chat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">chat_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#mlc_chat.ChatConfig" title="mlc_chat.chat_module.ChatConfig"><span class="pre">ChatConfig</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule.reset_chat" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Reset the chat session, clear all chat history, and potentially
override the original <cite>mlc-chat-config.json</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>chat_config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#mlc_chat.ChatConfig" title="mlc_chat.ChatConfig"><em>ChatConfig</em></a><em>]</em>) â€“ A <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> instance partially filled. If specified, the chat
module will reload the <cite>mlc-chat-config.json</cite>, and override it with
<code class="docutils literal notranslate"><span class="pre">chat_config</span></code>, just like in initialization.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The model remains the same after <a class="reference internal" href="#mlc_chat.ChatModule.reset_chat" title="mlc_chat.ChatModule.reset_chat"><code class="xref py py-func docutils literal notranslate"><span class="pre">reset_chat()</span></code></a>.
To reload module, please either re-initialize a <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChatModule</span></code></a> instance
or use <code class="xref py py-func docutils literal notranslate"><span class="pre">_reload()</span></code> instead.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.stats">
<span class="sig-name descname"><span class="pre">stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#mlc_chat.ChatModule.stats" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Get the runtime stats of the encoding step, decoding step, (and embedding step if exists)
of the chat module in text form.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>stats</strong> â€“ The runtime stats text.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mlc_chat.ChatConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mlc_chat.</span></span><span class="sig-name descname"><span class="pre">ChatConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_lib</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_template</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repetition_penalty</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean_gen_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_gen_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shift_fill_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer_files</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#mlc_chat.ConvConfig" title="mlc_chat.chat_module.ConvConfig"><span class="pre">ConvConfig</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_category</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_shards</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_window_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatConfig" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A dataclass that represents user-defined partial configuration for the
chat config file.</p>
<p>An instance of <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> can be passed in to the instantiation of a
<a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code></a> instance to override the default setting in
<code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> under the model folder.</p>
<p>Since the configuraiton is partial, everything will be <code class="docutils literal notranslate"><span class="pre">Optional</span></code>.</p>
<p>Note that we will exploit this class to also represent <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>
during intermediate processing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_lib</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ The necessary model library to launch this model architecture. We recommend
reuse model library when possible. For example, all LLaMA-7B models can
use <code class="docutils literal notranslate"><span class="pre">vicuna-v1-7b-{matching</span> <span class="pre">quantization</span> <span class="pre">scheme}</span></code>. So you can distribute
LLaMA-7B weight variants and still use them in prebuilt MLC chat apps.</p></li>
<li><p><strong>local_id</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ Uniquely identifying the model in application. This is also used by
command line interface app to specify which model to run.</p></li>
<li><p><strong>conv_template</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ The name of the conversation template that this chat uses.</p></li>
<li><p><strong>temperature</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) â€“ The temperature applied to logits before sampling. The default value is
<code class="docutils literal notranslate"><span class="pre">0.7</span></code>. A higher temperature encourages more diverse outputs, while a
lower temperature produces more deterministic outputs.</p></li>
<li><p><strong>repetition_penalty</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) â€“ <p>The repetition penalty controls the likelihood of the model generating
repeated texts. The default value is set to <code class="docutils literal notranslate"><span class="pre">1.0</span></code>, indicating that no
repetition penalty is applied. Increasing the value reduces the
likelihood of repeat text generation. However, setting a high
<code class="docutils literal notranslate"><span class="pre">repetition_penalty</span></code> may result in the model generating meaningless
texts. The ideal choice of repetition penalty may vary among models.</p>
<p>For more details on how repetition penalty controls text generation, please
check out the CTRL paper (<a class="reference external" href="https://arxiv.org/pdf/1909.05858.pdf">https://arxiv.org/pdf/1909.05858.pdf</a>).</p>
</p></li>
<li><p><strong>top_p</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) â€“ <p>This parameter determines the set of tokens from which we sample during
decoding. The default value is set to <code class="docutils literal notranslate"><span class="pre">0.95</span></code>. At each step, we select
tokens from the minimal set that has a cumulative probability exceeding
the <code class="docutils literal notranslate"><span class="pre">top_p</span></code> parameter.</p>
<p>For additional information on top-p sampling, please refer to this blog
post: <a class="reference external" href="https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling">https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling</a>.</p>
</p></li>
<li><p><strong>mean_gen_len</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) â€“ </p></li>
<li><p><strong>max_gen_len</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) â€“ </p></li>
<li><p><strong>shift_fill_factor</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) â€“ </p></li>
<li><p><strong>tokenizer_files</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) â€“ List of tokenizer files of the model.</p></li>
<li><p><strong>conv_config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#mlc_chat.ConvConfig" title="mlc_chat.ConvConfig"><em>ConvConfig</em></a><em>]</em>) â€“ The partial overriding configuration for conversation template. Will first
load the predefined template with the name specified in <code class="docutils literal notranslate"><span class="pre">conv_template</span></code>
and then override some of the configuraitons specified in <code class="docutils literal notranslate"><span class="pre">conv_config</span></code>.</p></li>
<li><p><strong>model_category</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ The category of the modelâ€™s architecture (e.g. <code class="docutils literal notranslate"><span class="pre">llama</span></code>, <code class="docutils literal notranslate"><span class="pre">gpt_neox</span></code>, <code class="docutils literal notranslate"><span class="pre">rwkv</span></code>).</p></li>
<li><p><strong>model_name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ Name of the model (e.g. <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf</span></code>).</p></li>
<li><p><strong>num_shards</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ Tensor parallel degree.</p></li>
<li><p><strong>max_window_size</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ Maximum kv cache window size.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mlc_chat.ConvConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mlc_chat.</span></span><span class="sig-name descname"><span class="pre">ConvConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">system</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">roles</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">messages</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">separator_style</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">role_msg_sep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">role_empty_sep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_str</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_bos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ConvConfig" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A dataclass that represents user-defined partial configuration for conversation template.</p>
<p>This is an attribute of <a class="reference internal" href="#mlc_chat.ChatConfig" title="mlc_chat.ChatConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatConfig</span></code></a>, which can then be passed in to the
instantiation of a <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code></a> instance to override the default
setting in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> under the model folder. Note that we will
first load the predefined template with the name specified in <code class="docutils literal notranslate"><span class="pre">conv_template</span></code>.</p>
<p>Since the configuration is partial, everything will be <code class="docutils literal notranslate"><span class="pre">Optional</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ Name of the conversation.</p></li>
<li><p><strong>system</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ The prompt encoded before starting the chat.</p></li>
<li><p><strong>roles</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) â€“ An array that describes the role names of the user and the model. These
names are specific to the model being used.</p></li>
<li><p><strong>messages</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) â€“ The chat history represented as an array of string pairs in the following
format: <code class="docutils literal notranslate"><span class="pre">[[role_0,</span> <span class="pre">msg_0],</span> <span class="pre">[role_1,</span> <span class="pre">msg_1],</span> <span class="pre">...]</span></code>.</p></li>
<li><p><strong>offset</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ The offset used to begin the chat from the chat history. When offset
is not <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">messages[0:offset-1]</span></code> will be encoded.</p></li>
<li><p><strong>separator_style</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) â€“ Specifies whether we are in chat-bot mode (<code class="docutils literal notranslate"><span class="pre">0</span></code>) or pure LM prompt mode (<code class="docutils literal notranslate"><span class="pre">1</span></code>).</p></li>
<li><p><strong>seps</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) â€“ An array of strings indicating the separators to be used after a user
message and a model message respectively.</p></li>
<li><p><strong>role_msg_sep</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ A string indicating the separator between a role and a message.</p></li>
<li><p><strong>role_empty_sep</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ A string indicating the separator to append to a role when there is no message yet.</p></li>
<li><p><strong>stop_str</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) â€“ When the <code class="docutils literal notranslate"><span class="pre">stop_str</span></code> is encountered, the model will stop generating output.</p></li>
<li><p><strong>stop_tokens</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>int</em><em>]</em><em>]</em>) â€“ A list of token IDs that act as stop tokens.</p></li>
<li><p><strong>add_bos</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) â€“ Determines whether a beginning-of-string (bos) token should be added
before the input tokens.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mlc_chat.GenerationConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mlc_chat.</span></span><span class="sig-name descname"><span class="pre">GenerationConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repetition_penalty</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean_gen_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_gen_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.GenerationConfig" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A dataclass that represents user-defined generation configuration.</p>
<p>An instance of <code class="docutils literal notranslate"><span class="pre">GenerationConfig</span></code> can be passed in to the generate function
of a <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code></a> instance to override the default generation
setting in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> and <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> under the model folder.</p>
<p>Once the generation ends, <code class="docutils literal notranslate"><span class="pre">GenerationConfig</span></code> is discarded, since the values
will only override the <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> generation settings during one generation,
unless it is recurrently passed to generate function. This allows changing generation
settings over time, without overriding <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> permanently.</p>
<p>Since the configuraiton is partial, everything will be <code class="docutils literal notranslate"><span class="pre">Optional</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>temperature</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) â€“ The temperature applied to logits before sampling. The default value is
<code class="docutils literal notranslate"><span class="pre">0.7</span></code>. A higher temperature encourages more diverse outputs, while a
lower temperature produces more deterministic outputs.</p></li>
<li><p><strong>repetition_penalty</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) â€“ <p>The repetition penalty controls the likelihood of the model generating
repeated texts. The default value is set to <code class="docutils literal notranslate"><span class="pre">1.0</span></code>, indicating that no
repetition penalty is applied. Increasing the value reduces the
likelihood of repeat text generation. However, setting a high
<code class="docutils literal notranslate"><span class="pre">repetition_penalty</span></code> may result in the model generating meaningless
texts. The ideal choice of repetition penalty may vary among models.</p>
<p>For more details on how repetition penalty controls text generation, please
check out the CTRL paper (<a class="reference external" href="https://arxiv.org/pdf/1909.05858.pdf">https://arxiv.org/pdf/1909.05858.pdf</a>).</p>
</p></li>
<li><p><strong>top_p</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) â€“ <p>This parameter determines the set of tokens from which we sample during
decoding. The default value is set to <code class="docutils literal notranslate"><span class="pre">0.95</span></code>. At each step, we select
tokens from the minimal set that has a cumulative probability exceeding
the <code class="docutils literal notranslate"><span class="pre">top_p</span></code> parameter.</p>
<p>For additional information on top-p sampling, please refer to this blog
post: <a class="reference external" href="https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling">https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling</a>.</p>
</p></li>
<li><p><strong>mean_gen_len</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) â€“ </p></li>
<li><p><strong>max_gen_len</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) â€“ </p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="gradio-frontend">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Gradio Frontend</a><a class="headerlink" href="#gradio-frontend" title="Permalink to this heading">Â¶</a></h2>
<p>The gradio frontend provides a web interface for the MLC-Chat model, which allows user to interact with the model in a more user-friendly way and switch between different models to compare performance.
To use gradio frontend, you need to install gradio first:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>gradio
</pre></div>
</div>
<p>Then you can run the following code to start the interface:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>mlc_chat.gradio<span class="w"> </span>--artifact-path<span class="w"> </span>ARTIFACT_PATH<span class="w"> </span><span class="o">[</span>--device<span class="w"> </span>DEVICE<span class="o">]</span><span class="w"> </span><span class="o">[</span>--port<span class="w"> </span>PORT_NUMBER<span class="o">]</span><span class="w"> </span><span class="o">[</span>--share<span class="o">]</span>
</pre></div>
</div>
<dl class="option-list">
<dt><kbd><span class="option">--artifact-path</span></kbd></dt>
<dd><p>Please provide a path containing all the model folders you wish to use. The default value is <code class="docutils literal notranslate"><span class="pre">dist</span></code>.</p>
</dd>
<dt><kbd><span class="option">--device</span></kbd></dt>
<dd><p>The description of the device to run on. User should provide a string in the form of â€˜device_name:device_idâ€™ or â€˜device_nameâ€™, where â€˜device_nameâ€™ is one of â€˜cudaâ€™, â€˜metalâ€™, â€˜vulkanâ€™, â€˜rocmâ€™, â€˜openclâ€™, â€˜autoâ€™ (automatically detect the local device), and â€˜device_idâ€™ is the device id to run on. If no â€˜device_idâ€™ is provided, it will be set to 0. The default value is <code class="docutils literal notranslate"><span class="pre">auto</span></code>.</p>
</dd>
<dt><kbd><span class="option">--port</span></kbd></dt>
<dd><p>The port number to run gradio. The default value is <code class="docutils literal notranslate"><span class="pre">7860</span></code>.</p>
</dd>
<dt><kbd><span class="option">--share</span></kbd></dt>
<dd><p>Whether to create a publicly shareable link for the interface.</p>
</dd>
</dl>
<p>After setting up properly, you are expected to see the following interface in your browser:</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/gradio-interface.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/gradio-interface.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/gradio-interface.png" style="width: 100%;" /></a>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ios.html" class="btn btn-neutral float-right" title="iOS App and Swift API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cli.html" class="btn btn-neutral float-left" title="CLI and C++ API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">Â© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>






<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Try out MLC Chat &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/tabs.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Project Overview" href="project_overview.html" />
    <link rel="prev" title="ðŸ‘‹ Welcome to MLC LLM" href="../index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Try out MLC Chat</a></li>
<li class="toctree-l1"><a class="reference internal" href="project_overview.html">Project Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlc_chat_config.html">Configure MLCChat in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deploy/javascript.html">WebLLM and Javascript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/rest.html">Rest API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/cli.html">CLI and C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/python.html">Python API and Gradio Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/android.html">Android App</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Models via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/distribute_compiled_models.html">Distribute Compiled Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/python.html">Python API for Model Compilation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/customize/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Try out MLC Chat</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/get_started/try_out.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="try-out-mlc-chat">
<span id="get-started"></span><h1>Try out MLC Chat<a class="headerlink" href="#try-out-mlc-chat" title="Permalink to this heading">Â¶</a></h1>
<p>Welcome to MLC LLM. To get started, we have prepared prebuilt packages
for you to try out MLC Chat app built with MLC LLM,
and you can try out prebuilt models on the following platforms:</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Python</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">CLI (Linux/MacOS/Windows)</button><button aria-controls="panel-0-0-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-2" name="0-2" role="tab" tabindex="-1">Web Browser</button><button aria-controls="panel-0-0-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-3" name="0-3" role="tab" tabindex="-1">iOS</button><button aria-controls="panel-0-0-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-4" name="0-4" role="tab" tabindex="-1">Android</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><p><strong>MLC LLM supports 7B/13B/70B Llama-2.</strong>
We provide a <a class="reference external" href="https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb">Jupyter notebook</a>
for you to try MLC Chat Python API in Colab.
You can also follow the instructions below and try out the Python API in you native environment.</p>
<p>To run LLMs using MLC LLM in Python, please visit <a class="reference external" href="https://mlc.ai/package/">https://mlc.ai/package/</a> to install
the chat package using pip. With the Python package installed, run the following
for preparation.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Verify the installation of the Python package.</span>
<span class="c1"># You are expected to see &quot;&lt;class &#39;mlc_chat.chat_module.ChatModule&#39;&gt;&quot; printed out.</span>
python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from mlc_chat import ChatModule; print(ChatModule)&quot;</span>

<span class="c1"># Install Git and Git-LFS if you haven&#39;t already. Then run</span>
git<span class="w"> </span>lfs<span class="w"> </span>install

<span class="c1"># Create a directory, download the model weights from HuggingFace, and download the binary libraries</span>
<span class="c1"># from GitHub.</span>
mkdir<span class="w"> </span>-p<span class="w"> </span>dist/prebuilt
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mlc-ai/binary-mlc-llm-libs.git<span class="w"> </span>dist/prebuilt/lib

<span class="c1"># Download prebuilt weights of Llama-2-7B, Llama-2-13B or Llama-2-70B</span>
<span class="nb">cd</span><span class="w"> </span>dist/prebuilt
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
<span class="c1"># or the 13B model</span>
<span class="c1"># git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-13b-chat-hf-q4f16_1</span>
<span class="c1"># or the 70B model (require at least 50GB VRAM on Apple Silicon Mac to run.)</span>
<span class="c1"># git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-70b-chat-hf-q4f16_1</span>
<span class="nb">cd</span><span class="w"> </span>../..
</pre></div>
</div>
<p>Then create a Python file <code class="docutils literal notranslate"><span class="pre">sample_mlc_chat.py</span></code> paste the following lines:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># From the mlc-llm directory, run</span>
<span class="c1"># $ python sample_mlc_chat.py</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>
<span class="c1"># You can change to other models that you downloaded, for example,</span>
<span class="c1"># cm = ChatModule(model=&quot;Llama-2-13b-chat-hf-q4f16_1&quot;)  # Llama2 13b model</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span>
    <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Print prefill and decode performance statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;How many points did you list out?&quot;</span><span class="p">,</span>
    <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Reset the chat module by</span>
<span class="c1"># cm.reset_chat()</span>
</pre></div>
</div>
<p>Now run the Python file to start the chat</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>sample_mlc_chat.py
</pre></div>
</div>
<p>You can also checkout the <a class="reference internal" href="../prebuilt_models.html"><span class="doc">Model Prebuilts</span></a> page to run other models.</p>
<p>To use Python API interactively, you are welcome to check out the
<a class="reference external" href="https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb">Jupyter notebook</a>
and run it in Colab.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/python-api.jpg"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/python-api.jpg" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/python-api.jpg" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-text">MLC LLM Python API</span><a class="headerlink" href="#id2" title="Permalink to this image">Â¶</a></p>
</figcaption>
</figure>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><p><strong>MLC LLM now supports 7B/13B/70B Llama-2.</strong> <em>Follow the instructions below and try out the CLI!</em></p>
<p>To run the models on your PC, you can try out the CLI version of MLC LLM.</p>
<p>We have prepared Conda packages for MLC Chat CLI. If you havenâ€™t installed Conda yet,
please refer to <a class="reference internal" href="../install/conda.html"><span class="doc">this tutorial</span></a> to install Conda.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are using Windows or Linux. Make sure you have the latest Vulkan driver installed.
Please follow the instructions in <a class="reference internal" href="../install/gpu.html"><span class="doc">GPU Drivers and SDKs</span></a> tutorial to prepare the environment.</p>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a new conda environment, install CLI app, and activate the environment.</span>
conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>mlc-chat-venv<span class="w"> </span>-c<span class="w"> </span>mlc-ai<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>mlc-chat-cli-nightly
conda<span class="w"> </span>activate<span class="w"> </span>mlc-chat-venv

<span class="c1"># Install Git and Git-LFS if you haven&#39;t already.</span>
<span class="c1"># They are used for downloading the model weights from HuggingFace.</span>
conda<span class="w"> </span>install<span class="w"> </span>git<span class="w"> </span>git-lfs
git<span class="w"> </span>lfs<span class="w"> </span>install

<span class="c1"># Create a directory, download the model weights from HuggingFace, and download the binary libraries</span>
<span class="c1"># from GitHub.</span>
mkdir<span class="w"> </span>-p<span class="w"> </span>dist/prebuilt
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mlc-ai/binary-mlc-llm-libs.git<span class="w"> </span>dist/prebuilt/lib

<span class="c1"># Download prebuilt weights of Llama-2-7B, Llama-2-13B or Llama-2-70B</span>
<span class="nb">cd</span><span class="w"> </span>dist/prebuilt
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
<span class="c1"># or the 13B model</span>
<span class="c1"># git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-13b-chat-hf-q4f16_1</span>
<span class="c1"># or the 70B model (require at least 50GB VRAM on Apple Silicon Mac to run.)</span>
<span class="c1"># git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-70b-chat-hf-q4f16_1</span>
<span class="nb">cd</span><span class="w"> </span>../..
mlc_chat_cli<span class="w"> </span>--model<span class="w"> </span>Llama-2-7b-chat-hf-q4f16_1
<span class="c1"># or the 13B model</span>
<span class="c1"># mlc_chat_cli --model Llama-2-13b-chat-hf-q4f16_1</span>
<span class="c1"># or the 70B model (require at least 50GB VRAM on Apple Silicon Mac to run.)</span>
<span class="c1"># mlc_chat_cli --model Llama-2-70b-chat-hf-q4f16_1</span>

<span class="c1"># You can try more models, for example:</span>
<span class="c1"># download prebuilt weights of RedPajama-3B</span>
<span class="nb">cd</span><span class="w"> </span>dist/prebuilt
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_1
<span class="nb">cd</span><span class="w"> </span>../..
mlc_chat_cli<span class="w"> </span>--model<span class="w"> </span>RedPajama-INCITE-Chat-3B-v1-q4f16_1
</pre></div>
</div>
<p>You can also checkout the <a class="reference internal" href="../prebuilt_models.html"><span class="doc">Model Prebuilts</span></a> page to run other models.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/Llama2-macOS.gif"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/Llama2-macOS.gif" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/Llama2-macOS.gif" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-text">MLC LLM on CLI</span><a class="headerlink" href="#id3" title="Permalink to this image">Â¶</a></p>
</figcaption>
</figure>
</div><div aria-labelledby="tab-0-0-2" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-2" name="0-2" role="tabpanel" tabindex="0"><p>With the advancements of WebGPU, we can now run LLM completely in the web browser environment.
You can try out the web version of MLC LLM in <a class="reference external" href="https://webllm.mlc.ai/#chat-demo">WebLLM</a>.</p>
<p>In WebLLM, once the model weights are fetched and stored in the local cache in the first run, you can start to interact with the model without Internet connection.</p>
<p>A WebGPU-compatible browser and a local GPU are needed to run WebLLM. You can download the latest Google Chrome and use <a class="reference external" href="https://webgpureport.org/">WebGPU Report</a> to verify the functionality of WebGPU on your browser.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://blog.mlc.ai/img/redpajama/web.gif"><img alt="https://blog.mlc.ai/img/redpajama/web.gif" src="https://blog.mlc.ai/img/redpajama/web.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">MLC LLM on Web</span><a class="headerlink" href="#id4" title="Permalink to this image">Â¶</a></p>
</figcaption>
</figure>
</div><div aria-labelledby="tab-0-0-3" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-3" name="0-3" role="tabpanel" tabindex="0"><p>The MLC Chat app is now available in App Store at no cost. You can download and explore it by simply clicking the button below:</p>
<a class="reference external image-reference" href="https://apps.apple.com/us/app/mlc-chat/id6448482937"><img alt="https://linkmaker.itunes.apple.com/assets/shared/badges/en-us/appstore-lrg.svg" src="https://linkmaker.itunes.apple.com/assets/shared/badges/en-us/appstore-lrg.svg" width="135" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>MLC LLM now supports Llama-2 via the test link below</strong> *</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can also try out the beta version of MLC-Chat on
<a class="reference external" href="https://testflight.apple.com/join/57zd7oxa">TestFlight</a>.</p>
</div>
<p>Once the app is installed, you can download the models and then engage in chat with the model without requiring an internet connection.</p>
<p>Memory requirements vary across different models. The Llama2-7B model necessitates an iPhone device with a minimum of 6GB RAM, whereas the RedPajama-3B model can run on an iPhone with at least 4GB RAM.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://blog.mlc.ai/img/redpajama/ios.gif"><img alt="https://blog.mlc.ai/img/redpajama/ios.gif" src="https://blog.mlc.ai/img/redpajama/ios.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">MLC Chat on iOS</span><a class="headerlink" href="#id5" title="Permalink to this image">Â¶</a></p>
</figcaption>
</figure>
</div><div aria-labelledby="tab-0-0-4" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-4" name="0-4" role="tabpanel" tabindex="0"><p>The MLC Chat Android app is free and available for download, and you can try out by simply clicking the button below:</p>
<a class="reference external image-reference" href="https://github.com/mlc-ai/binary-mlc-llm-libs/raw/main/mlc-chat.apk"><img alt="https://seeklogo.com/images/D/download-android-apk-badge-logo-D074C6882B-seeklogo.com.png" src="https://seeklogo.com/images/D/download-android-apk-badge-logo-D074C6882B-seeklogo.com.png" style="width: 135px;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Once the app is installed, you can engage in a chat with the model without the need for an internet connection:</p>
<p>Memory requirements vary across different models. The Vicuna-7B model necessitates an Android device with a minimum of 6GB RAM, whereas the RedPajama-3B model can run on an Android device with at least 4GB RAM.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://blog.mlc.ai/img/android/android-recording.gif"><img alt="https://blog.mlc.ai/img/android/android-recording.gif" src="https://blog.mlc.ai/img/android/android-recording.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">MLC LLM on Android</span><a class="headerlink" href="#id6" title="Permalink to this image">Â¶</a></p>
</figcaption>
</figure>
</div></div>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="project_overview.html" class="btn btn-neutral float-right" title="Project Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../index.html" class="btn btn-neutral float-left" title="ðŸ‘‹ Welcome to MLC LLM" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">Â© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>